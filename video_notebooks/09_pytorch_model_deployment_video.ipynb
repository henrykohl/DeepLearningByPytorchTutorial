{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_pytorch_model_deployment_video.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cb5027e48c7849d2a66300789b5b6db6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5a0592da3a94970b8bc47b10282054b",
              "IPY_MODEL_9d2e2f76d33d4d7cb85fab5848a8b495",
              "IPY_MODEL_d212279b331648d2bce193c9cae0e25b"
            ],
            "layout": "IPY_MODEL_c5060376c93048f79f70ee65cd08f3df"
          }
        },
        "f5a0592da3a94970b8bc47b10282054b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fabf0282728d4dce9bb0949219d0a329",
            "placeholder": "​",
            "style": "IPY_MODEL_511211588f62455a961d966390cff822",
            "value": "100%"
          }
        },
        "9d2e2f76d33d4d7cb85fab5848a8b495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e416f7063a2c413fb808910463499be4",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ede6ba36dbaa4a1687dad9052ee1e908",
            "value": 10
          }
        },
        "d212279b331648d2bce193c9cae0e25b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_739d5b7498274710b8a336584cc4ef65",
            "placeholder": "​",
            "style": "IPY_MODEL_2b5c1a40a25240a586179f54d14579df",
            "value": " 10/10 [00:54&lt;00:00,  5.47s/it]"
          }
        },
        "c5060376c93048f79f70ee65cd08f3df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fabf0282728d4dce9bb0949219d0a329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "511211588f62455a961d966390cff822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e416f7063a2c413fb808910463499be4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ede6ba36dbaa4a1687dad9052ee1e908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "739d5b7498274710b8a336584cc4ef65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b5c1a40a25240a586179f54d14579df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d0b2fbe5b904839b575c60f007c9977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5b08b956f794e99bf76fc5cc9679a2d",
              "IPY_MODEL_144570d550b64d11958b2e497696b7cd",
              "IPY_MODEL_a8b2cee8020a448a8b568a471148d981"
            ],
            "layout": "IPY_MODEL_96017667844f4a67a6e670c66c2141ab"
          }
        },
        "a5b08b956f794e99bf76fc5cc9679a2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a906e7dd2714589bd586e08565ff95b",
            "placeholder": "​",
            "style": "IPY_MODEL_8df61ce127634d848c0054b2e0b72974",
            "value": "100%"
          }
        },
        "144570d550b64d11958b2e497696b7cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35ba32b372f14557bdec85747f349318",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4bd9761438f4734b674ebcc5e71fa08",
            "value": 150
          }
        },
        "a8b2cee8020a448a8b568a471148d981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d134831ffa9e446b85d36ec9ea0b2105",
            "placeholder": "​",
            "style": "IPY_MODEL_cf83b2528f8146b0a1b5b34e4f2f53f3",
            "value": " 150/150 [00:18&lt;00:00,  8.26it/s]"
          }
        },
        "96017667844f4a67a6e670c66c2141ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a906e7dd2714589bd586e08565ff95b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8df61ce127634d848c0054b2e0b72974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35ba32b372f14557bdec85747f349318": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4bd9761438f4734b674ebcc5e71fa08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d134831ffa9e446b85d36ec9ea0b2105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf83b2528f8146b0a1b5b34e4f2f53f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dae44612c46644c8b496a7f9bfa6c3c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48486a66fd844a9290ec49866c134b3c",
              "IPY_MODEL_3983c92304d04553a213755aac582367",
              "IPY_MODEL_4b95cf3fb5f34ef98a83ef1fe14d9db6"
            ],
            "layout": "IPY_MODEL_65db50a782564d31b1d0f81224adfd56"
          }
        },
        "48486a66fd844a9290ec49866c134b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa5ad051ad4b4d1299e417bbfd15581b",
            "placeholder": "​",
            "style": "IPY_MODEL_e5023f3772b14e1ca6f31a683cc9849e",
            "value": "100%"
          }
        },
        "3983c92304d04553a213755aac582367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55b52e781a074d27b054eefe594e06c8",
            "max": 150,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa902acee91d41219628f85319b4902e",
            "value": 150
          }
        },
        "4b95cf3fb5f34ef98a83ef1fe14d9db6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95bfc6e17e634d2c831d79aef7ec545d",
            "placeholder": "​",
            "style": "IPY_MODEL_b3ba57b373e140988c34219c07bed764",
            "value": " 150/150 [01:23&lt;00:00,  1.63it/s]"
          }
        },
        "65db50a782564d31b1d0f81224adfd56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa5ad051ad4b4d1299e417bbfd15581b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5023f3772b14e1ca6f31a683cc9849e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55b52e781a074d27b054eefe594e06c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa902acee91d41219628f85319b4902e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95bfc6e17e634d2c831d79aef7ec545d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3ba57b373e140988c34219c07bed764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a41e92879364ffdbe4a4b84b2a815b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7267b534abd43ab84c70c74b644a06b",
              "IPY_MODEL_f11aeabb239a4e5aa8c1847d7437b60f",
              "IPY_MODEL_572281cc888f48c7afd2461426b0321e"
            ],
            "layout": "IPY_MODEL_245ab0553fb445ec8d7961a218c97a39"
          }
        },
        "f7267b534abd43ab84c70c74b644a06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b5471c1519b461aa31736b9d24d43d2",
            "placeholder": "​",
            "style": "IPY_MODEL_c98453092a49405780dec8a65b79e6fb",
            "value": "100%"
          }
        },
        "f11aeabb239a4e5aa8c1847d7437b60f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb8a6f3b1d7443b28be1571e43755493",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc9109d69c8544e18a9dac3fa8f4da22",
            "value": 5
          }
        },
        "572281cc888f48c7afd2461426b0321e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0aabb802874547beaa385aa0ee024fbd",
            "placeholder": "​",
            "style": "IPY_MODEL_bf88ce31ca9041e9910921e1c6fe84cf",
            "value": " 5/5 [18:10&lt;00:00, 210.93s/it]"
          }
        },
        "245ab0553fb445ec8d7961a218c97a39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b5471c1519b461aa31736b9d24d43d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c98453092a49405780dec8a65b79e6fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb8a6f3b1d7443b28be1571e43755493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc9109d69c8544e18a9dac3fa8f4da22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0aabb802874547beaa385aa0ee024fbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf88ce31ca9041e9910921e1c6fe84cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef81f5f84f8348a487edeac0f5c57cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54726694432c418892bc631b7298be28",
              "IPY_MODEL_0f8fa951f7404abd94c7fec7ac76d8b3",
              "IPY_MODEL_345212b216b441ed98972310fbe38fd3"
            ],
            "layout": "IPY_MODEL_cffcdcf745454ded9117279fd0a6648b"
          }
        },
        "54726694432c418892bc631b7298be28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26920567db2249e5a57b02d61002ad91",
            "placeholder": "​",
            "style": "IPY_MODEL_13d5c7c455f14751b9d98989aa037b2f",
            "value": "100%"
          }
        },
        "0f8fa951f7404abd94c7fec7ac76d8b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f48ff9ff53c44f989448a9319a0c2e9",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4eb53b977e96429cbb7d8c3258656d8b",
            "value": 10
          }
        },
        "345212b216b441ed98972310fbe38fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c92843d455b46bca62bb4d33e49f5de",
            "placeholder": "​",
            "style": "IPY_MODEL_fd4b13ee40424cf395b32a2efd8cffb8",
            "value": " 10/10 [14:50&lt;00:00, 87.88s/it]"
          }
        },
        "cffcdcf745454ded9117279fd0a6648b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26920567db2249e5a57b02d61002ad91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13d5c7c455f14751b9d98989aa037b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f48ff9ff53c44f989448a9319a0c2e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb53b977e96429cbb7d8c3258656d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c92843d455b46bca62bb4d33e49f5de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd4b13ee40424cf395b32a2efd8cffb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrykohl/DeepLearningByPytorchTutorial/blob/master/video_notebooks/09_pytorch_model_deployment_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"red\">09. PyTorch Model Deployment</font>\n",
        "\n",
        "What is model deployment?\n",
        "\n",
        "Machine learning model deployment is the act of making your machine learning model(s) available to someone or something else.\n",
        "\n",
        "## Resources: \n",
        "\n",
        "* Book version of notebook: https://www.learnpytorch.io/09_pytorch_model_deployment/ \n",
        "* Slides: https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/09_pytorch_model_deployment.pdf "
      ],
      "metadata": {
        "id": "fztGSYtzaZw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Welcome to Milestone Project 3: PyTorch Model Deployment!\n",
        "\n",
        "We've come a long way with our FoodVision Mini project.</font>\n",
        "\n",
        "<font color=\"purple\">But so far our PyTorch models have only been accessible to us.\n",
        "\n",
        "How about we bring FoodVision Mini to life and make it publically accessible?</font>\n",
        "\n",
        "<font color=\"purple\">In other words, **we're going to deploy our FoodVision Mini model to the internet as a usable app!**</font>\n",
        "\n",
        "<img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/09-model-deployment-what-were-doing-demo-trimmed-cropped-small.gif\" alt=\"demo of foodvision mini computer vision model being used on a mobile device to predict on an image of sushi and getting it right\" width=900/>\n",
        "\n",
        "<font color=\"purple\">*Trying out the [deployed version of FoodVision Mini](https://huggingface.co/spaces/mrdbourke/foodvision_mini) (what we're going to build) on my lunch. The model got it right too 🍣!*</font>"
      ],
      "metadata": {
        "id": "zqNjCX8rV_Gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"purple\">What is machine learning model deployment?.</font> \n",
        "\n",
        "<font color=\"purple\">**Machine learning model deployment** is the process of making your machine learning model accessible to someone or something else.\n",
        "\n",
        "Someone else being a person who can interact with your model in some way.</font> \n",
        "\n",
        "<font color=\"purple\">For example, someone taking a photo on their smartphone of food and then having our FoodVision Mini model classify it into pizza, steak or sushi.\n",
        "\n",
        "Something else might be another program, app or even another model that interacts with your machine learning model(s).</font> \n",
        "\n",
        "<font color=\"purple\">For example, a banking database might rely on a machine learning model making predictions as to whether a transaction is fraudulent or not before transferring funds.\n",
        "\n",
        "Or an operating system may lower its resource consumption based on a machine learning model making predictions on how much power someone generally uses at specific times of day.</font>\n",
        "\n",
        "<font color=\"purple\">These use cases can be mixed and matched as well.\n",
        "\n",
        "For example, a Tesla car's computer vision system will interact with the car's route planning program (something else) and then the route planning program will get inputs and feedback from the driver (someone else).</font>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-what-is-model-deployment-someone-or-something-else.png\" width=900 alt=\"two use cases for model deployment, making your model available to someone else, for example, someone using it in an app, or making it available to something else such as another program or model\"/> \n",
        "\n",
        "<font color=\"purple\">*Machine learning model deployment involves making your model available to someone or something else. For example, someone might use your model as part of a food recognition app (such as FoodVision Mini or [Nutrify](https://nutrify.app)). And something else might be another model or program using your model such as a banking system using a machine learning model to detect if a transaction is fraud or not.*</font> "
      ],
      "metadata": {
        "id": "zX8oq1DFbFTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"purple\">Why deploy a machine learning model?\n",
        "\n",
        "One of the most important philosophical questions in machine learning is: </font>\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-does-it-exist.jpeg\" alt=\"curious dinosaur often referred to as philosoraptor asking the question if a machine learning model never leaves a notebook, does it exist?\" width=300/>\n",
        "</div>\n",
        "\n",
        "<font color=\"purple\">Deploying a model is as important as training one.\n",
        "\n",
        "Because although you can get a pretty good idea of how your model's going to function by evaluting it on a well crafted test set or visualizing its results, you never really know how it's going to perform until you release it to the wild.</font>\n",
        "\n",
        "<font color=\"purple\">Having people who've never used your model interact with it will often reveal edge cases you never thought of during training.\n",
        "\n",
        "For example, what happens if someone was to upload a photo that *wasn't* of food to our FoodVision Mini model?</font>\n",
        "\n",
        "<font color=\"purple\">One solution would be to create another model that first classifies images as \"food\" or \"not food\" and passing the target image through that model first (this is what [Nutrify](https://nutrify.app) does).\n",
        "\n",
        "Then if the image is of \"food\" it goes to our FoodVision Mini model and gets classified into pizza, steak or sushi.</font>\n",
        "\n",
        "<font color=\"purple\">And if it's \"not food\", a message is displayed.\n",
        "\n",
        "But what if these predictions were wrong?</font>\n",
        "\n",
        "<font color=\"purple\">What happens then?\n",
        "\n",
        "You can see how these questions could keep going.</font>\n",
        "\n",
        "<font color=\"purple\">Thus this highlights the importance of model deployment: it helps you figure out errors in your model that aren't obvious during training/testing.</font>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-pytorch-workflow-with-deployment.png\" alt=\"A PyTorch workflow with added model deployment and monitoring step\" width=900/>\n",
        "\n",
        "<font color=\"purple\">*We covered a PyTorch workflow back in [01. PyTorch Workflow](https://www.learnpytorch.io/01_pytorch_workflow/). But once you've got a good model, deployment is a good next step. Monitoring involves seeing how your model goes on the most important data split: data from the real world. For more resources on deployment and monitoring see [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering).*</font>"
      ],
      "metadata": {
        "id": "O_gf4-ovmZlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"purple\">Different types of machine learning model deployment\n",
        "\n",
        "Whole books could be written on the different types of machine learning model deployment (and many good ones are listed in [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering)).</font>\n",
        "\n",
        "<font color=\"purple\">And the field is still developing in terms of best practices.\n",
        "\n",
        "But I like to start with the question:</font>\n",
        "\n",
        "> <font color=\"purple\">\"What is the most ideal scenario for my machine learning model to be used?\"\n",
        "\n",
        "And then work backwards from there.</font>\n",
        "\n",
        "<font color=\"purple\">Of course, you may not know this ahead of time. But you're smart enough to imagine such things.\n",
        "\n",
        "In the case of FoodVision Mini, our ideal scenario might be:</font>\n",
        "\n",
        "* <font color=\"purple\">Someone takes a photo on a mobile device (through an app or web broswer).\n",
        "* The prediction comes back fast.</font>\n",
        "\n",
        "<font color=\"purple\">Easy.\n",
        "\n",
        "So we've got two main criteria:</font>\n",
        "\n",
        "1. <font color=\"purple\">The model should work on a mobile device (this means there will be some compute constraints). \n",
        "2. The model should make predictions *fast* (because a slow app is a boring app).</font>\n",
        "\n",
        "<font color=\"purple\">And of course, depending on your use case, your requirements may vary.\n",
        "\n",
        "You may notice the above two points break down into another two questions:</font>\n",
        "\n",
        "1. <font color=\"purple\">**Where's it going to go?** - As in, where is it going to be stored?\n",
        "2. **How's it going to function?** - As in, does it return predictions immediately? Or do they come later?</font>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-deployment-questions-to-ask.png\" alt=\"some questions to ask when starting to deploy machine learning models, what's the model ideal use case, then work backwards and ask where's my model going to go and how's my model going to function\" width=900/>\n",
        "\n",
        "<font color=\"purple\">*When starting to deploy machine learning models, it's helpful to start by asking what's the most ideal use case and then work backwards from there, asking where the model's going to go and then how it's going to function.*"
      ],
      "metadata": {
        "id": "McPe9X4ut_97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"purple\">Where's it going to go? <font color=\"red\">模型放置位置：本機上？雲端上？</font>\n",
        "When you deploy your machine learning model, where does it live?</font>\n",
        "\n",
        "<font color=\"purple\">The main debate here is usually on-device (also called edge/in the browser) or on the cloud (a computer/server that isn't the *actual* device someone/something calls the model from). \n",
        "\n",
        "<font color=\"purple\">Both have their pros and cons.\n",
        "\n",
        "| **Deployment location** | **Pros** | **Cons** | \n",
        "| ----- | ----- | ----- |\n",
        "| **On-device (edge/in the browser)** | Can be very fast (since no data leaves the device) | Limited compute power (larger models take longer to run) | \n",
        "| | Privacy preserving (again no data has to leave the device) | Limited storage space (smaller model size required) | \n",
        "| | No internet connection required (sometimes) | Device-specific skills often required | \n",
        "| | | | \n",
        "| **On cloud** | Near unlimited compute power (can scale up when needed) | Costs can get out of hand (if proper scaling limits aren't enforced) |\n",
        "| | Can deploy one model and use everywhere (via API) | Predictions can be slower due to data having to leave device and predictions having to come back (network latency) |\n",
        "| | Links into existing cloud ecosystem | Data has to leave device (this may cause privacy concerns) |\n",
        "\n",
        "<font color=\"purple\">There are more details to these but I've left resources in the [extra-curriculum](https://www.learnpytorch.io/09_pytorch_model_deployment/#extra-curriculum) to learn more. \n",
        "\n",
        "Let's give an example.</font>\n",
        "\n",
        "<font color=\"purple\">If we're deploying FoodVision Mini as an app, we want it to perform well and fast.\n",
        "\n",
        "So which model would we prefer?</font> \n",
        "\n",
        "1. <font color=\"purple\">A model on-device that performs at 95% accuracy with an inference time (latency) of one second per prediction.\n",
        "2. A model on the cloud that performs at 98% accuracy with an inference time of 10 seconds per per prediction (bigger, better model but takes longer to compute).\n",
        "\n",
        "<font color=\"purple\">I've made these numbers up but they showcase a potential difference between on-device and on the cloud.\n",
        "\n",
        "Option 1 could potentially be a smaller less performant model that runs fast because its able to fit on a mobile device.</font>\n",
        "\n",
        "<font color=\"purple\">Option 2 could potentially a larger more performant model that requires more compute and storage but it takes a bit longer to run because we have to send data off the device and get it back (so even though the actual prediction might be fast, the network time and data transfer has to factored in).\n",
        "\n",
        "For FoodVision Mini, we'd likely prefer option 1, because the small hit in performance is far outweighed by the faster inference speed.</font>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-on-device-vs-cloud.png\" width=900 alt=\"tesla computer vision system on device vs on the cloud\"/>\n",
        "\n",
        "<font color=\"purple\">*In the case of a Tesla car's computer vision system, which would be better? A smaller model that performs well on device (model is on the car) or a larger model that performs better that's on the cloud? In this case, you'd much prefer the model being on the car. The extra network time it would take for data to go from the car to the cloud and then back to the car just wouldn't be worth it (or potentially even possible with poor signal areas).*\n",
        "\n",
        "<font color=\"purple\">![](http://)![](http://)> **Note:** For a full example of seeing what it's like to deploy a PyTorch model to an edge device, see the [PyTorch tutorial on achieving real-time inference (30fps+)](https://pytorch.org/tutorials/intermediate/realtime_rpi.html) with a computer vision model on a Raspberry Pi."
      ],
      "metadata": {
        "id": "KWmSTDVE5g9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"purple\">How's it going to function? <font color=\"red\">模型如何運作：online? offline?</font>\n",
        "\n",
        "<font color=\"purple\">Back to the ideal use case, when you deploy your machine learning model, how should it work?\n",
        "\n",
        "As in, would you like predictions returned immediately?</font>\n",
        "\n",
        "<font color=\"purple\">Or is it okay for them to happen later?\n",
        "\n",
        "These two scenarios are generally referred to as:</font>\n",
        "\n",
        "* <font color=\"purple\">**Online (real-time)** - Predicitions/inference happen **immediately**. For example, someone uploads an image, the image gets transformed and predictions are returned or someone makes a purchase and the transaction is verified to be non-fradulent by a model so the purchase can go through.\n",
        "* **Offline (batch)** - Predictions/inference happen **periodically**. For example, a photos application sorts your images into different categories (such as beach, mealtime, family, friends) whilst your mobile device is plugged into charge.</font>\n",
        "\n",
        "> <font color=\"purple\">**Note:** \"Batch\" refers to inference being performed on multiple samples at a time. However, to add a little confusion, batch processing can happen immediately/online (multiple images being classified at once) and/or offline (mutliple images being predicted/trained on at once).</font>  \n",
        "\n",
        "<font color=\"purple\">The main difference between each being: predictions being made immediately or periodically.\n",
        "\n",
        "Periodically can have a varying timescale too, from every few seconds to every few hours or days.</font>\n",
        "\n",
        "<font color=\"purple\">And you can mix and match the two.\n",
        "\n",
        "In the case of FoodVision Mini, we'd want our inference pipeline to happen online (real-time), so when someone uploads an image of pizza, steak or sushi, the prediction results are returned immediately (any slower than real-time would make a boring experience).</font>\n",
        "\n",
        "<font color=\"purple\">But for our training pipeline, it's okay for it to happen in a batch (offline) fashion, which is what we've been doing throughout the previous chapters.</font>"
      ],
      "metadata": {
        "id": "ZqXQPZmoGL_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"purple\">Ways to deploy a machine learning model <font color=\"red\">模型放置位置：本機上？雲端上？</font>\n",
        "\n",
        "<font color=\"purple\">We've discussed a couple of options for deploying machine learning models (on-device and cloud).\n",
        "\n",
        "And each of these will have their specific requirements:</font>\n",
        "\n",
        "| **Tool/resource** | **Deployment type** | \n",
        "| ----- | ----- |\n",
        "| [Google's ML Kit](https://developers.google.com/ml-kit) | On-device (Android and iOS) | \n",
        "| [Apple's Core ML](https://developer.apple.com/documentation/coreml) and [`coremltools` Python package](https://coremltools.readme.io/docs) | On-device (all Apple devices) | \n",
        "| [Amazon Web Service's (AWS) Sagemaker](https://aws.amazon.com/sagemaker/) | Cloud | \n",
        "| [Google Cloud's Vertex AI](https://cloud.google.com/vertex-ai) | Cloud |\n",
        "| [Microsoft's Azure Machine Learning](https://azure.microsoft.com/en-au/services/machine-learning/) | Cloud |\n",
        "| [Hugging Face Spaces](https://huggingface.co/spaces) | Cloud |\n",
        "| API with [FastAPI](https://fastapi.tiangolo.com) | Cloud/self-hosted server |\n",
        "| API with [TorchServe](https://pytorch.org/serve/) | Cloud/self-hosted server | \n",
        "| [ONNX (Open Neural Network Exchange)](https://onnx.ai/index.html) | Many/general |\n",
        "| Many more... |\n",
        "\n",
        "> <font color=\"purple\">**Note:** An [application programming interface (API)](https://en.wikipedia.org/wiki/API) is a way for two (or more) computer programs to interact with each other. For example, if your model was deployed as API, you would be able to write a program that could send data to it and then receive predictions back.</font>\n",
        "\n",
        "<font color=\"purple\">Which option you choose will be highly dependent on what you're building/who you're working with.\n",
        "\n",
        "But with so many options, it can be very intimidating.</font>\n",
        "\n",
        "<font color=\"purple\">So best to start small and keep it simple.\n",
        "\n",
        "And one of the best ways to do so is by turning your machine learning model into a demo app with [Gradio](https://gradio.app) and then deploying it on Hugging Face Spaces.</font>\n",
        "\n",
        "<font color=\"purple\">We'll be doing just that with FoodVision Mini later on.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-tools-and-places-to-deploy-ml-models.png\" alt=\"tools and places to deploy machine learning models\" width=900/>\n",
        "\n",
        "<font color=\"purple\">*A handful of places and tools to host and deploy machine learning models. There are plenty I've missed so if you'd like to add more, please leave a [discussion on GitHub](https://github.com/mrdbourke/pytorch-deep-learning/discussions).*"
      ],
      "metadata": {
        "id": "eaAVOwL6ISko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"purple\">What we're going to cover \n",
        "\n",
        "<font color=\"purple\">Enough talking about deploying a machine learning model.\n",
        "\n",
        "Let's become machine learning engineers and actually deploy one.</font>\n",
        "\n",
        "<font color=\"purple\">Our goal is to deploy our FoodVision Model via a demo Gradio app with the following metrics:\n",
        "1. **Performance:** 95%+ accuracy.\n",
        "2. **Speed:** real-time inference of 30FPS+ (each prediction has a latency of lower than ~0.03s).\n",
        "\n",
        "We'll start by running an experiment to compare our best two models so far: EffNetB2 and ViT feature extractors.</font>\n",
        "\n",
        "<font color=\"purple\">Then we'll deploy the one which performs closest to our goal metrics.\n",
        "\n",
        "Finally, we'll finish with a (BIG) surprise bonus.</font>\n",
        "\n",
        "| **Topic** | **Contents** | \n",
        "| ----- | ----- | \n",
        "| **0. Getting setup** | We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. | \n",
        "| **1. Get data** | Let's download the [`pizza_steak_sushi_20_percent.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip) dataset so we can train our previously best performing models on the same dataset. |\n",
        "| **2. FoodVision Mini model deployment experiment outline** | Even on the third milestone project, we're still going to be running multiple experiments to see which model (EffNetB2 or ViT) achieves closest to our goal metrics. |\n",
        "| **3. Creating an EffNetB2 feature extractor** | An EfficientNetB2 feature extractor performed the best on our pizza, steak, sushi dataset in [07. PyTorch Experiment Tracking](https://www.learnpytorch.io/07_pytorch_experiment_tracking/), let's recreate it as a candidate for deployment. |\n",
        "| **4. Creating a ViT feature extractor** | A ViT feature extractor has been the best performing model yet on our pizza, steak, sushi dataset in [08. PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/), let's recreate it as a candidate for deployment alongside EffNetB2. |\n",
        "| **5. Making predictions with our trained models and timing them** | We've built two of the best performing models yet, let's make predictions with them and track their results. |\n",
        "| **6. Comparing model results, prediction times and size** | Let's compare our models to see which performs best with our goals. | \n",
        "| **7. Bringing FoodVision Mini to life by creating a Gradio demo** | One of our models performs better than the other (in terms of our goals), so let's turn it into a working app demo! |\n",
        "| **8. Turning our FoodVision Mini Gradio demo into a deployable app** | Our Gradio app demo works locally, let's prepare it for deployment! |\n",
        "| **9. Deploying our Gradio demo to** <font color=\"coral\">HuggingFace</font> **Spaces** | Let's take FoodVision Mini to the web and make it pubically accessible for all! |\n",
        "| **10. Creating a BIG surprise** | We've built FoodVision Mini, time to step things up a notch. |\n",
        "| **11. Deploying our BIG surprise** | Deploying one app was fun, how about we make it two? |"
      ],
      "metadata": {
        "id": "8Q0weKYXJ58C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"purple\">Where can you get help?\n",
        "\n",
        "All of the materials for this course [are available on GitHub](https://github.com/mrdbourke/pytorch-deep-learning).</font>\n",
        "\n",
        "<font color=\"purple\">If you run into trouble, you can ask a question on the course [GitHub Discussions page](https://github.com/mrdbourke/pytorch-deep-learning/discussions).\n",
        "\n",
        "And of course, there's the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) and [PyTorch developer forums](https://discuss.pytorch.org/), a very helpful place for all things PyTorch.</font>"
      ],
      "metadata": {
        "id": "v-z_m20DJ-Gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Get setup"
      ],
      "metadata": {
        "id": "b6EmBsFjausC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">As we've done previously, let's make sure we've got all of the modules we'll need for this section.\n",
        "\n",
        "We'll import the Python scripts (such as `data_setup.py` and `engine.py`) we created in [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/).</font>\n",
        "\n",
        "<font color=\"purple\">To do so, we'll download [`going_modular`](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/going_modular) directory from the [`pytorch-deep-learning` repository](https://github.com/mrdbourke/pytorch-deep-learning) (if we don't already have it).\n",
        "\n",
        "We'll also get the [`torchinfo`](https://github.com/TylerYep/torchinfo) package if it's not available.</font> \n",
        "\n",
        "<font color=\"purple\">`torchinfo` will help later on to give us a visual representation of our model.\n",
        "\n",
        "And since later on we'll be using `torchvision` v0.13 package (available as of July 2022), we'll make sure we've got the latest versions.</font>\n",
        "\n",
        "> <font color=\"purple\">**Note:** If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`."
      ],
      "metadata": {
        "id": "tqrN76R8b6AV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">匯入\n",
        "* torch \n",
        "* torchvision \n",
        "* matplotlib.pyplot.plt\n",
        "* torch.nn\n",
        "* torchvision.transforms </font>\n",
        "\n",
        "<font color=\"red\">安裝\n",
        "* <font color=\"red\">torchinfo\n",
        "\n",
        "<font color=\"red\">下載\n",
        "* <font color=\"red\">going_modular.going_modular\n",
        "  > <font color=\"red\">匯入\n",
        "  * <font color=\"red\">data_setup\n",
        "  * <font color=\"red\">engine\n",
        "* helper_functions\n",
        "  > <font color=\"red\">匯入\n",
        "  * <font color=\"red\">download_data\n",
        "  * <font color=\"red\">set_seeds\n",
        "  * <font color=\"red\">plot_loss_curves\n",
        "\n",
        "\n",
        "torch version: 1.12.1+cu113\n",
        "\n",
        "torchvision version: 0.13.1+cu113"
      ],
      "metadata": {
        "id": "UCH3JLFAc6XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "  import torch\n",
        "  import torchvision\n",
        "  assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "  assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "  print(f\"torch version: {torch.__version__}\")\n",
        "  print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "  print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "  !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "  import torch\n",
        "  import torchvision\n",
        "  print(f\"torch version: {torch.__version__}\")\n",
        "  print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YpSiECUbEe0",
        "outputId": "83d88219-1e6e-42dc-d38e-7f3997f8dbeb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "torch version: 2.0.1+cu118\n",
            "torchvision version: 0.15.2+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> <font color=\"purple\">**Note:** If you're using Google Colab and the cell above starts to install various software packages, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of `torch` and `torchvision`.\n",
        "\n",
        "Now we'll continue with the regular imports, setting up device agnostic code and this time we'll also get the [`helper_functions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py) script from GitHub.</font>\n",
        "\n",
        "<font color=\"purple\">The `helper_functions.py` script contains several functions we created in previous sections:\n",
        "* `set_seeds()` to set the random seeds (created in [07. PyTorch Experiment Tracking section 0](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#create-a-helper-function-to-set-seeds)).\n",
        "* `download_data()` to download a data source given a link (created in [07. PyTorch Experiment Tracking section 1](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data)).\n",
        "* `plot_loss_curves()` to inspect our model's training results (created in [04. PyTorch Custom Datasets section 7.8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#78-plot-the-loss-curves-of-model-0))</font>\n",
        "\n",
        "> <font color=\"purple\">**Note:** It may be a better idea for many of the functions in the `helper_functions.py` script to be merged into `going_modular/going_modular/utils.py`, perhaps that's an extension you'd like to try."
      ],
      "metadata": {
        "id": "8VU7lDFQoldg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "  !pip install -q torchinfo\n",
        "  from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "  from going_modular.going_modular import data_setup, engine\n",
        "  from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "  # Get the going_modular scripts\n",
        "  print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "  !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "  !mv pytorch-deep-learning/going_modular .\n",
        "  !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "  !rm -rf pytorch-deep-learning\n",
        "  from going_modular.going_modular import data_setup, engine\n",
        "  from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "id": "ivpKKpO6bOsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d8ed8a-cc47-49eb-842f-ff586d7c7f8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 3812, done.\u001b[K\n",
            "remote: Counting objects: 100% (455/455), done.\u001b[K\n",
            "remote: Compressing objects: 100% (252/252), done.\u001b[K\n",
            "remote: Total 3812 (delta 241), reused 400 (delta 196), pack-reused 3357\u001b[K\n",
            "Receiving objects: 100% (3812/3812), 650.41 MiB | 42.63 MiB/s, done.\n",
            "Resolving deltas: 100% (2197/2197), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls going_modular/going_modular"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RffrxihTbg7q",
        "outputId": "2a114638-3a31-489a-93a7-eea6b05f807e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_setup.py  model_builder.py  __pycache__  train.py\n",
            "engine.py      predictions.py\t README.md    utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YYDOXbsVbp0r",
        "outputId": "04b91e69-4831-4ebc-82db-fa24f67dc6df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting Data\n",
        "\n",
        "The dataset we're going to use for deploying a FoodVision Mini model is...\n",
        "\n",
        "Pizza, steak, sushi 20% dataset (pizza, steak, sushi classes from Food101, random 20% of samples)\n",
        "\n",
        "We can get data with code from: https://www.learnpytorch.io/09_pytorch_model_deployment/#1-getting-data"
      ],
      "metadata": {
        "id": "Hr_pgSE2a2Bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We left off in [08. PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/#106-save-feature-extractor-vit-model-and-check-file-size) comparing our own <font color=\"red\">Vision Transformer (ViT)</font> feature extractor model to the <font color=\"red\">EfficientNetB2 (EffNetB2)</font> feature extractor model we created in [07. PyTorch Experiment Tracking](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#9-load-in-the-best-model-and-make-predictions-with-it).\n",
        "\n",
        "And we found that there was a slight difference in the comparison.</font>\n",
        "\n",
        "<font color=\"purple\">The <font color=\"red\">EffNetB2</font> model was trained on 20% of the pizza, steak and sushi data from Food101 where as the <font color=\"red\">ViT</font> model was trained on 10%.\n",
        "\n",
        "Since our goal is to deploy the best model for our FoodVision Mini problem, let's start by downloading the [20% pizza, steak and sushi dataset](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip) and train an <font color=\"red\">EffNetB2</font> feature extractor and <font color=\"red\">ViT</font> feature extractor on it and then compare the two models.</font>\n",
        "\n",
        "<font color=\"purple\">This way we'll be comparing apples to apples (one model trained on a dataset to another model trained on the same dataset).</font>\n",
        "\n",
        "> <font color=\"purple\">**Note:** The dataset we're downloading is a sample of the entire [Food101 dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html#food101) (101 food classes with 1,000 images each). More specifically, 20% refers to 20% of images from the pizza, steak and sushi classes selected at random. You can see how this dataset was created in [`extras/04_custom_data_creation.ipynb`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) and more details in [04. PyTorch Custom Datasets section 1](https://www.learnpytorch.io/04_pytorch_custom_datasets/#1-get-data).\n",
        "\n",
        "We can download the data using the `download_data()` function we created in [07. PyTorch Experiment Tracking section 1](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#1-get-data) from [`helper_functions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py).</font> "
      ],
      "metadata": {
        "id": "Wkk4QGRcuCCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上傳檔案大小大於25MB到GitHub的方法\n",
        "\n",
        "https://bytesbin.com/upload-files-larger-than-25mb-to-github/"
      ],
      "metadata": {
        "id": "0cYpKgGf_Jnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "\"\"\"下載pizza_steak_sushi_20_percent.zip\"\"\"\n",
        "\n",
        "# 原data在mrdbourke帳號下\n",
        "# data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
        "#                                      destination=\"pizza_steak_sushi_20_percent\")\n",
        "\n",
        "# 原data已置放在自己帳號下\n",
        "data_20_percent_path = download_data(source=\"https://github.com/henrykohl/DeepLearningByPytorchTutorial/raw/master/data/pizza_steak_sushi_20_percent.zip\",\n",
        "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
        "\n",
        "data_20_percent_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egGp5nNTbD7M",
        "outputId": "962e0bc3-3d74-42c7-ad39-4f98d6f2e30a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Did not find data/pizza_steak_sushi_20_percent directory, creating one...\n",
            "[INFO] Downloading pizza_steak_sushi_20_percent.zip from https://github.com/henrykohl/DeepLearningByPytorchTutorial/raw/master/data/pizza_steak_sushi_20_percent.zip...\n",
            "[INFO] Unzipping pizza_steak_sushi_20_percent.zip data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('data/pizza_steak_sushi_20_percent')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Wonderful!\n",
        "\n",
        "Now we've got a dataset, let's creat training and test paths.</font>"
      ],
      "metadata": {
        "id": "jml8EJtQ3m2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training and test paths\n",
        "\"\"\"設定 訓練路徑 測試路徑\"\"\" \n",
        "train_dir = data_20_percent_path / \"train\"\n",
        "test_dir = data_20_percent_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjaa2VP3cpaO",
        "outputId": "6a1441fb-23d8-40bc-de62-0b38280a20b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi_20_percent/train'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. FoodVision Mini model deployment experiment outline\n",
        "\n",
        "### 3 questions\n",
        "1. What is my most ideal machine learning model deployment scenario?\n",
        "2. Where is my model going to go?\n",
        "3. How is my model going to function?\n",
        "\n",
        "**FoodVision Mini ideal use case:** A model that performs well and fast.\n",
        "\n",
        "1. Performs well: 95%+ accuracy\n",
        "2. Fast: as close to real-time (or faster) as possible (30FPS+ or 30ms latency) \n",
        "  * Latency = time for prediction to take place \n",
        "\n",
        "To try and achieve these goals, we're going to build two model experiments:\n",
        "\n",
        "1. EffNetB2 feature extractor (just like in 07. PyTorch Experiment Tracking)\n",
        "2. ViT feature extractor (just like in 08. PyTorch Paper Replicating)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3Da8tLPjczI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">The ideal deployed model FoodVision Mini performs well and fast. \n",
        "\n",
        "We'd like our model to perform as close to real-time as possible.</font>\n",
        "\n",
        "<font color=\"purple\">Real-time in this case being ~30FPS (frames per second) because that's [about how fast the human eye can see](https://www.healthline.com/health/human-eye-fps) (there is debate on this but let's just use ~30FPS as our benchmark).\n",
        "\n",
        "And for classifying three different classes (pizza, steak and sushi), we'd like a model that performs at 95%+ accuracy.</font>\n",
        "\n",
        "<font color=\"purple\">Of course, higher accuracy would be nice but this might sacrifice speed.\n",
        "\n",
        "So our goals are:</font>\n",
        "\n",
        "1. <font color=\"purple\">**Performance** - A model that performs at 95%+ accuracy.\n",
        "2. **Speed** - A model that can classify an image at ~30FPS (0.03 seconds inference time per image, also known as latency).</font>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployments-speed-vs-inference.png\" alt=\"foodvision mini goals in terms of performance and inference time.\" width=750/>\n",
        "\n",
        "<font color=\"purple\">*FoodVision Mini deployment goals. We'd like a fast predicting well-performing model (because a slow app is boring).*\n",
        "\n",
        "We'll put an emphasis on speed, meaning, we'd prefer a model performing at 90%+ accuracy at ~30FPS than a model performing 95%+ accuracy at 10FPS.</font>\n",
        "\n",
        "<font color=\"purple\">To try and achieve these results, let's bring in our best performing models from the previous sections: \n",
        "\n",
        "1. **EffNetB2 feature extractor** (EffNetB2 for short) - originally created in [07. PyTorch Experiment Tracking section 7.5](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models) using [`torchvision.models.efficientnet_b2()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#efficientnet-b2) with adjusted `classifier` layers.\n",
        "2. **ViT-B/16 feature extractor** (ViT for short) - originally created in [08. PyTorch Paper Replicating section 10](https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset) using [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#vit-b-16) with adjusted `head` layers.\n",
        "    * <font color=\"purple\">**Note** ViT-B/16 stands for \"Vision Transformer Base, patch size 16\".\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-two-experiments.png\" alt=\"modelling experiments for foodvision mini deployments, one effnetb2 feature extractor model and a vision transformer feature extractor model\" width=750 />\n",
        "\n",
        "> <font color=\"purple\">**Note:** A \"feature extractor model\" often starts with a model that has been pretrained on a dataset similar to your own problem. The pretrained model's base layers are often left frozen (the pretrained patterns/weights stay the same) whilst some of the top (or classifier/classification head) layers get customized to your own problem by training on your own data. We covered the concept of a feature extractor model in [06. PyTorch Transfer Learning section 3.4](https://www.learnpytorch.io/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs).</font>"
      ],
      "metadata": {
        "id": "QPEQVak03zsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Creating an EffNetB2 feature extractor\n",
        "\n",
        "Feautre extractor = a term for a transfer learning model that has its base layers frozen and output layers (or head layers) customized to a certain problem.\n",
        "\n",
        "EffNetB2 pretrained model in PyTorch - https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights "
      ],
      "metadata": {
        "id": "mmG_WB-Fd71H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We first created an <font color=\"red\">**EffNetB2** feature extractor model</font> in [07. PyTorch Experiment Tracking section 7.5](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#75-create-feature-extractor-models).\n",
        "\n",
        "And by the end of that section we saw it performed very well.</font>\n",
        "\n",
        "<font color=\"purple\">So let's now recreate it here so we can compare its results to a <font color=\"red\">**ViT** feature extractor</font> trained on the same data.</font>\n",
        "\n",
        "<font color=\"purple\">To do so we can:\n",
        "1. Setup the pretrained weights as [`weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT`](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.EfficientNet_B2_Weights), where \"`DEFAULT`\" means \"best currently available\" (or could use `weights=\"DEFAULT\"`). <font color=\"red\">設定預訓練權重</font>\n",
        "2. Get the pretrained model image transforms from the weights with the `transforms()` method (we need these so we can convert our images into the same format as the pretrained <font color=\"red\">**EffNetB2**</font> was trained on). <font color=\"red\">獲得預訓練transform</font>\n",
        "3. Create a pretrained model instance by passing the weights to an instance of [`torchvision.models.efficientnet_b2`](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#efficientnet-b2). <font color=\"red\">建立預訓練模型實例</font>\n",
        "4. Freeze the base layers in the model.<font color=\"red\">凍結預訓練模型內層</font>\n",
        "5. Update the classifier head to suit our own data.<font color=\"red\">更新預訓練classifier head</font></font>"
      ],
      "metadata": {
        "id": "M1RFlMzdDMJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "# Check out the effnetb2 heads layer (來自4.0)\n",
        "\"\"\"新增\"\"\"\n",
        "effnetb2 = torchvision.models.efficientnet_b2()\n",
        "effnetb2.classifier\n",
        "```\n",
        "\n",
        "\n",
        "Sequential(\n",
        "\n",
        "  (0): Dropout(p=0.3, inplace=True)\n",
        "\n",
        "  (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
        "  \n",
        ")"
      ],
      "metadata": {
        "id": "0USNRO7_EvCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "# 此cell為示範，可以不需要執行\n",
        "import torchvision\n",
        "\n",
        "\"\"\"步驟2 步驟3 順序可交換\"\"\"\n",
        "\n",
        "# 1. Setup pretrained EffNetB2 weights\n",
        "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" is equivalent to saying \"best available\"\n",
        "\n",
        "# 2. Get EffNetB2 transforms\n",
        "effnetb2_transforms = effnetb2_weights.transforms()\n",
        "\n",
        "# 3. Setup pretrained model instance\n",
        "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n",
        "\n",
        "# 4. Freeze the base layers in the model (this will stop all layers from training)\n",
        "for param in effnetb2.parameters():\n",
        "  param.requires_grad = False\n",
        "```\n",
        "\n",
        "\n",
        "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-bcdf34b7.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-bcdf34b7.pth\n",
        "100%|██████████| 35.2M/35.2M [00:00<00:00, 89.8MB/s]"
      ],
      "metadata": {
        "id": "Zk7fTJ53HrW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# # Print EffNetB2 model summary (uncomment for full output) \n",
        "# summary(effnetb2, \n",
        "#         input_size=(1, 3, 224, 224),\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "eoLgDG7ff1VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Now to change the *classifier head*, let's first inspect it using the `classifier` attribute of our model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RCB_uAyWF7Fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "# Check out EffNetB2 classifier head\n",
        "effnetb2.classifier\n",
        "```\n",
        "\n",
        "\n",
        "Sequential(\n",
        "\n",
        "  (0): Dropout(p=0.3, inplace=True)\n",
        "\n",
        "  (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "AJyUwW4eVErt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Excellent! To change the *classifier head* to suit our own problem, let's replace the `out_features` variable with the same number of classes we have (in our case, `out_features=3`, one for pizza, steak, sushi).\n",
        "\n",
        "> **Note:** This process of changing the output *layers/classifier head* will be dependent on the problem you're working on. For example, if you wanted a different *number* of outputs or a different *kind* of ouput, you would have to change the output layers accordingly.</font> "
      ],
      "metadata": {
        "id": "vHm2NMhzHbmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set seeds for reproducibility\n",
        "# set_seeds()\n",
        "\n",
        "# # 5. Update the classifier head\n",
        "# effnetb2.classifier = nn.Sequential(\n",
        "#   nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n",
        "#   nn.Linear(in_features=1408,    # keep in_features same \n",
        "#        out_features=3, bias=True)) # change out_features to suit our number of classes"
      ],
      "metadata": {
        "id": "0Twn8RLehBZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Beautiful!</font>"
      ],
      "metadata": {
        "id": "fksBI_39L9tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchinfo import summary\n",
        "\n",
        "# # Print EffNetB2 model summary (uncomment for full output) \n",
        "# summary(effnetb2, \n",
        "#         input_size=(1, 3, 224, 224),\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "4gDqFBFkhQb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Creating a function to make an EffNetB2 feature extractor\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uEl8EQeYhS2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Looks like our <font color=\"red\">**EffNetB2** feature extractor</font> is ready to go, however, since there's quite a few steps involved here, how about we turn the code above into a function we can re-use later?</font>\n",
        "\n",
        "<font color=\"purple\">We'll call it <font color=\"PEAR\">**`create_effnetb2_model()`**</font> and it'll take a customizable number of classes and a random seed parameter for reproducibility.\n",
        "\n",
        "Ideally, it will return an <font color=\"red\">**EffNetB2** feature extractor</font> along with its assosciated transforms.</font>"
      ],
      "metadata": {
        "id": "4hra6mQMMLUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_effnetb2_model(num_classes:int=3, # default output classes = 3 (pizza, steak, sushi)\n",
        "              seed:int=42):\n",
        "  \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "  Args:\n",
        "    num_classes (int, optional): number of classes in the classifier head. \n",
        "        Defaults to 3.\n",
        "    seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "  Returns:\n",
        "    model (torch.nn.Module): EffNetB2 feature extractor model. \n",
        "    transforms (torchvision.transforms): EffNetB2 image transforms.\n",
        "  \"\"\"\n",
        "  # 1, 2, 3 Create EffNetB2 pretrained weights, transforms and model\n",
        "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "  # 4. Freeze all layers in the base model\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # 5. Change classifier head with random seed for reproducibility\n",
        "  torch.manual_seed(seed)\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=0.3, inplace=True),\n",
        "      nn.Linear(in_features=1408, out_features=num_classes)\n",
        "  )\n",
        "\n",
        "  return model, transforms"
      ],
      "metadata": {
        "id": "xRUzPCQZh_4A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Woohoo! That's a nice looking function, let's try it out.</font>"
      ],
      "metadata": {
        "id": "7hvtpxwGd6pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"調用create_effnetb2_model建立 effnetb2模型 與 effnetb2_transforms變換\"\"\"\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n",
        "                              seed=42)"
      ],
      "metadata": {
        "id": "1XuwSyTFi2GG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ea9841-d112-486b-e0f9-944aec27f92a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-bcdf34b7.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-bcdf34b7.pth\n",
            "100%|██████████| 35.2M/35.2M [00:00<00:00, 144MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">No errors, nice, now to really try it out, let's get a summary with torchinfo.summary().</font>"
      ],
      "metadata": {
        "id": "MgEgc8ujeGFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_transforms"
      ],
      "metadata": {
        "id": "-tIhgmP4Rsuk",
        "outputId": "2dfcb703-6771-44df-d94a-51b2536439dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[288]\n",
              "    resize_size=[288]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BICUBIC\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchinfo import summary\n",
        "\n",
        "# # Print EffNetB2 model summary (uncomment for full output) \n",
        "# summary(effnetb2, \n",
        "#         input_size=(1, 3, 288, 288),\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "ZmNK6oYmi_Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-effnetb2-feature-extractor.png\" alt=\"effnetb2 feature extractor model summary\" width=900/>\n",
        "\n",
        "<font color=\"purple\">Base layers frozen, top layers trainable and customized!</font>"
      ],
      "metadata": {
        "id": "spR1ekkIfaM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Creating DataLoaders for EffNetB2"
      ],
      "metadata": {
        "id": "DHLMR5LsjAvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Our <font color=\"red\">**EffNetB2** feature extractor</font> is ready, time to create some `DataLoader`s.\n",
        "\n",
        "We can do this by using the [`data_setup.create_dataloaders()`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/data_setup.py) function we created in [05. PyTorch Going Modular section 2](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy).</font>\n",
        "\n",
        "<font color=\"purple\">We'll use a `batch_size` of 32 and transform our images using the `effnetb2_transforms` so they're in the same format that our `effnetb2` model was trained on."
      ],
      "metadata": {
        "id": "RbhNW9NLhe96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup \n",
        "\"\"\"設定-數據加載器\"\"\"\n",
        "from going_modular.going_modular import data_setup\n",
        "\n",
        "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                      test_dir=test_dir,\n",
        "                                                      transform=effnetb2_transforms,\n",
        "                                                      batch_size=32)"
      ],
      "metadata": {
        "id": "5lUtcYVKjuY-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader_effnetb2), len(test_dataloader_effnetb2), class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKpZ2LdEj--i",
        "outputId": "f07ea097-0bc6-485c-8788-78ef63638887"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 5, ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Training EffNetB2 feature extractor "
      ],
      "metadata": {
        "id": "ZaFplq8ikEPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Model ready, `DataLoader`s ready, let's train!\n",
        "\n",
        "Just like in [07. PyTorch Experiment Tracking section 7.6](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#76-create-experiments-and-set-up-training-code), ten epochs should be enough to get good results.</font>\n",
        "\n",
        "<font color=\"purple\">We can do so by creating an optimizer (we'll use [`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam) with a learning rate of `1e-3`), a loss function (we'll use [`torch.nn.CrossEntropyLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) for multi-class classification) and then passing these as well as our `DataLoader`s to the [`engine.train()`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py) function we created in [05. PyTorch Going Modular section 4](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them)."
      ],
      "metadata": {
        "id": "C372uWFjiKqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(params=effnetb2.parameters(),\n",
        "                   lr=1e-3)\n",
        "\n",
        "# Training function (engine.py)\n",
        "set_seeds()\n",
        "effnetb2_results = engine.train(model=effnetb2,\n",
        "                  train_dataloader=train_dataloader_effnetb2,\n",
        "                  test_dataloader=test_dataloader_effnetb2,\n",
        "                  epochs=10,\n",
        "                  optimizer=optimizer,\n",
        "                  loss_fn=loss_fn, \n",
        "                  device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "ef81f5f84f8348a487edeac0f5c57cbf",
            "54726694432c418892bc631b7298be28",
            "0f8fa951f7404abd94c7fec7ac76d8b3",
            "345212b216b441ed98972310fbe38fd3",
            "cffcdcf745454ded9117279fd0a6648b",
            "26920567db2249e5a57b02d61002ad91",
            "13d5c7c455f14751b9d98989aa037b2f",
            "9f48ff9ff53c44f989448a9319a0c2e9",
            "4eb53b977e96429cbb7d8c3258656d8b",
            "5c92843d455b46bca62bb4d33e49f5de",
            "fd4b13ee40424cf395b32a2efd8cffb8"
          ]
        },
        "id": "NCcK0I8SkP30",
        "outputId": "d3964244-f6ba-4630-ad0c-a1628ae694d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef81f5f84f8348a487edeac0f5c57cbf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.9804 | train_acc: 0.5646 | test_loss: 0.7385 | test_acc: 0.9284\n",
            "Epoch: 2 | train_loss: 0.6843 | train_acc: 0.8729 | test_loss: 0.5931 | test_acc: 0.9347\n",
            "Epoch: 3 | train_loss: 0.5644 | train_acc: 0.8979 | test_loss: 0.4820 | test_acc: 0.9347\n",
            "Epoch: 4 | train_loss: 0.4720 | train_acc: 0.8667 | test_loss: 0.4185 | test_acc: 0.9443\n",
            "Epoch: 5 | train_loss: 0.4309 | train_acc: 0.8729 | test_loss: 0.3725 | test_acc: 0.9568\n",
            "Epoch: 6 | train_loss: 0.3748 | train_acc: 0.8938 | test_loss: 0.3456 | test_acc: 0.9443\n",
            "Epoch: 7 | train_loss: 0.3367 | train_acc: 0.9083 | test_loss: 0.3123 | test_acc: 0.9506\n",
            "Epoch: 8 | train_loss: 0.3237 | train_acc: 0.9250 | test_loss: 0.3014 | test_acc: 0.9688\n",
            "Epoch: 9 | train_loss: 0.3657 | train_acc: 0.8625 | test_loss: 0.2753 | test_acc: 0.9381\n",
            "Epoch: 10 | train_loss: 0.2551 | train_acc: 0.9437 | test_loss: 0.2639 | test_acc: 0.9597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Inspecting EffNetB2 loss curves"
      ],
      "metadata": {
        "id": "xs6U1XTLlrbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Nice!\n",
        "\n",
        "As we saw in 07. PyTorch Experiment Tracking, the <font color=\"red\">**EffNetB2** feature extractor model</font> works quite well on our data.</font>\n",
        "\n",
        "<font color=\"purple\">Let's turn its results into loss curves to inspect them further.\n",
        "\n",
        "Note: Loss curves are one of the best ways to visualize how your model's performing. For more on loss curves, check out 04. PyTorch Custom Datasets section 8: What should an ideal loss curve look like?</font>"
      ],
      "metadata": {
        "id": "5pM4lmGBnA0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(effnetb2_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "H5JEnNIImAgg",
        "outputId": "9a7a10cb-5f74-48fe-952f-debbea9c0c03"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See here for what an ideal loss curve should look like: https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like "
      ],
      "metadata": {
        "id": "MOxZSC94meBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Woah!\n",
        "\n",
        "Those are some nice looking loss curves.</font> \n",
        "\n",
        "<font color=\"purple\">It looks like our model is performing quite well and perhaps would benefit from a little longer training and potentially some [data augmentation](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation) (to help prevent potential overfitting occurring from longer training).</font>"
      ],
      "metadata": {
        "id": "2xWA-SbtnRMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Saving EffNetB2 feature extractor"
      ],
      "metadata": {
        "id": "z2NoT7L_mV_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Now we've got a well-performing trained model, let's save it to file so we can import and use it later.\n",
        "\n",
        "To save our model we can use the [`utils.save_model()`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/utils.py) function we created in [05. PyTorch Going Modular section 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy).</font>\n",
        "\n",
        "<font color=\"purple\">We'll set the `target_dir` to `\"models\"` and the `model_name` to `\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"` (a little comprehensive but at least we know what's going on).</font>"
      ],
      "metadata": {
        "id": "1afC76VOngTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import utils\n",
        "\n",
        "# Save the model\n",
        "utils.save_model(model=effnetb2,\n",
        "         target_dir=\"models\",\n",
        "         model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTME7_YSo5CY",
        "outputId": "c8901f3d-322a-4ef8-8d7a-8fee5317d78f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving model to: models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Inspecting the size of our EffNetB2 feature extractor\n",
        "\n",
        "Why would it be important to consider the size of a saved model?\n",
        "\n",
        "If we're deploying our model to be used on a mobile app/website, there may be limited compute resources.\n",
        "\n",
        "So if our model file is too large, we may not be able to store/run it on our target device."
      ],
      "metadata": {
        "id": "huZdf9QFpJ7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Since one of our criteria for deploying a model to power FoodVision Mini is **speed** (~30FPS or better), let's check the size of our model.\n",
        "\n",
        "Why check the size?</font>\n",
        "\n",
        "<font color=\"purple\">Well, while not always the case, the size of a model can influence its inference speed.\n",
        "\n",
        "As in, if a model has more parameters, it generally performs more operations and each one of these operations requires some computing power.</font>\n",
        "\n",
        "<font color=\"purple\">And because we'd like our model to work on devices with limited computing power (e.g. on a mobile device or in a web browser), generally, the smaller the size the better (as long as it still performs well in terms of accuracy).\n",
        "\n",
        "To check our model's size in bytes, we can use Python's [<font color=\"PEAR\">`pathlib.Path.stat(\"path_to_model\").st_size`</font>](https://docs.python.org/3/library/pathlib.html#pathlib.Path.stat) and then we can convert it (roughly) to megabytes by dividing it by `(1024*1024)`.</font> "
      ],
      "metadata": {
        "id": "6PbVfVnZnzUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes and convert to megabytes\n",
        "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size / (1024 * 1024)\n",
        "print(f\"Pretrained EffNetB2 feature extractor model size: {round(pretrained_effnetb2_model_size, 2)} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPo2jgrBpZFw",
        "outputId": "cf56a441-e396-4cca-8b97-f018e2365c4d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained EffNetB2 feature extractor model size: 29.86 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Collecting EffNetB2 feature extractor stats"
      ],
      "metadata": {
        "id": "4p7eY0Z-qg3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've got a few statistics about our <font color=\"red\">**EffNetB2** feature extractor model</font> such as test loss, test accuracy and model size, how about we collect them all in a dictionary so we can compare them to the upcoming <font color=\"red\">**ViT** feature extractor</font>.\n",
        "\n",
        "<font color=\"purple\">And we'll calculate an extra one for fun, total number of parameters.</font>\n",
        "\n",
        "<font color=\"purple\">We can do so by counting the number of elements (or patterns/weights) in `effnetb2.parameters()`. We'll access the number of elements in each parameter using the [`torch.numel()`](https://pytorch.org/docs/stable/generated/torch.numel.html) (short for \"number of elements\") method. "
      ],
      "metadata": {
        "id": "gQfQGT3aqyUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of parameters in EffNetB2\n",
        "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
        "effnetb2_total_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDfHVCMWrUIf",
        "outputId": "790dd172-262c-4792-b884-e54fcb42f08b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7705221"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Excellent!\n",
        "\n",
        "Now let's put everything in a dictionary so we can make comparisons later on.</font>"
      ],
      "metadata": {
        "id": "9Ucg89UQrHG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary with EffNetB2 statistics\n",
        "\"\"\"建立EffNetB2的統計dictionary: test_loss, test_acc, number_of_parameters, model_size\"\"\"\n",
        "effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1], # 10個\"test_loss\"結果中的最後一個\n",
        "          \"test_acc\": effnetb2_results[\"test_acc\"][-1],   # 10個\"test_acc\"結果中的最後一個\n",
        "          \"number_of_parameters\": effnetb2_total_params,\n",
        "          \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
        "\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMQ_axUzrup3",
        "outputId": "8f1cd25a-835d-4571-95ec-a39f2175694c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.26394864320755007,\n",
              " 'test_acc': 0.959659090909091,\n",
              " 'number_of_parameters': 7705221,\n",
              " 'model_size (MB)': 29.856457710266113}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Epic! \n",
        "\n",
        "<font color=\"purple\">Looks like our <font color=\"red\">**EffNetB2** model</font> is performing at over 95% accuracy! \n",
        "\n",
        "Criteria number 1: perform at 95%+ accuracy, tick!</font>"
      ],
      "metadata": {
        "id": "-LgGfBr5rTln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Creating a ViT feature extractor\n",
        "\n",
        "We're up to our second modelling experiment, repeating the steps for EffNetB2 but this time with a ViT feature extractor, see here for ideas: https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset"
      ],
      "metadata": {
        "id": "0lckiZT6sPQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Time to continue with our FoodVision Mini modelling experiments.\n",
        "\n",
        "This time we're going to create a <font color=\"red\">**ViT** feature extractor.</font></font>\n",
        "\n",
        "<font color=\"purple\">And we'll do it in much the same way as the <font color=\"red\">**EffNetB2** feature extractor</font> except this time with [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16) instead of `torchvision.models.efficientnet_b2()`.\n",
        "\n",
        "We'll start by creating a function called `create_vit_model()` which will be very similar to <font color=\"PEAR\">**`create_effnetb2_model()`**</font> except of course returning a <font color=\"red\">**ViT** feature extractor model</font> and transforms rather than <font color=\"red\">**EffNetB2**</font>.</font>\n",
        "\n",
        "<font color=\"purple\">Another slight difference is that `torchvision.models.vit_b_16()`'s output layer is called `heads` rather than `classifier`."
      ],
      "metadata": {
        "id": "WS7CZcwlrgfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the ViT heads layer \n",
        "vit = torchvision.models.vit_b_16()\n",
        "vit.heads"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww-6Q_tisrFH",
        "outputId": "841c12a5-30e1-4bc2-c73e-1072eb2beda0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Knowing this, we've got all the pieces of the puzzle we need.</font>"
      ],
      "metadata": {
        "id": "yEqjDL0lr0EI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# 新增(來自3.0) - 此cell為示範，可以不需要執行\n",
        "import torchvision\n",
        "\n",
        "\"\"\"步驟2 步驟3 順序可交換\"\"\"\n",
        "\n",
        "# 1. Setup pretrained ViT_B_16 weights\n",
        "vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # \"DEFAULT\" is equivalent to saying \"best available\"\n",
        "\n",
        "# 2. Get ViT_B_16 transforms\n",
        "vit_transforms = vit_weights.transforms()\n",
        "\n",
        "# 3. Setup pretrained model instance\n",
        "vit = torchvision.models.vit_b_16(weights=vit_weights) # could also use weights=\"DEFAULT\"\n",
        "\n",
        "# 4. Freeze the base layers in the model (this will stop all layers from training)\n",
        "for param in vit.parameters():\n",
        "  param.requires_grad = False\n",
        "```\n",
        "\n",
        "\n",
        "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
        "100%|██████████| 330M/330M [00:02<00:00, 123MB/s]"
      ],
      "metadata": {
        "id": "VK6MtBEiruEJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "# 新增Check out Vit classifier head (來自3.0)\n",
        "vit.heads\n",
        "```\n",
        "Sequential(\n",
        "\n",
        "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
        "\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "CWtR6B4bsD5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "# 新增(來自3.0)\n",
        "# Set seeds for reproducibility\n",
        "set_seeds()\n",
        "\n",
        "# 5. Update the classifier head\n",
        "vit.heads = nn.Sequential(\n",
        "  nn.Linear(in_features=768,    # keep in_features same \n",
        "       out_features=3, bias=True)) # change out_features to suit our number of classes\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Lx8rja_csUUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Creating a function to make an ViT feature extractor(新)\n",
        "\n"
      ],
      "metadata": {
        "id": "4Nt2l-86Ulyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (新4.1)\n",
        "def create_vit_model(num_classes:int=3,\n",
        "            seed:int=42):\n",
        "  \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "      num_classes (int, optional): number of target classes. Defaults to 3.\n",
        "      seed (int, optional): random seed value for output layer. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "      model (torch.nn.Module): ViT-B/16 feature extractor model. \n",
        "      transforms (torchvision.transforms): ViT-B/16 image transforms.\n",
        "  \"\"\"\n",
        "  # Create ViT_B_16 pretrained weights, transforms and model\n",
        "  weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.vit_b_16(weights=weights)\n",
        "\n",
        "  # Freeze all of the base layers\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # Change classifier head to suit our needs (this will be trainable)\n",
        "  torch.manual_seed(seed)\n",
        "  model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n",
        "                       out_features=num_classes)) # update to reflect target number of classes\n",
        "  \n",
        "  return model, transforms"
      ],
      "metadata": {
        "id": "HB2FxBD5Pn6d"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">ViT feature extraction model creation function ready!\n",
        "\n",
        "Let's test it out.</font>"
      ],
      "metadata": {
        "id": "zA0e6bXIwmnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ViT model and transforms\n",
        "vit, vit_transforms = create_vit_model()\n",
        "vit_transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UkRL23GPpwN",
        "outputId": "6247739b-f475-4a21-a05c-4115ae0bfbe6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:05<00:00, 67.3MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[256]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BILINEAR\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">No errors, lovely to see! \n",
        "\n",
        "Now let's get a nice-looking summary of our ViT model using `torchinfo.summary()`.</font>"
      ],
      "metadata": {
        "id": "d-f_ZFyrw6_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchinfo import summary\n",
        "\n",
        "# # Print ViT model summary (uncomment for full output) \n",
        "# summary(vit, \n",
        "#         input_size=(1, 3, 224, 224),\n",
        "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "#         col_width=20,\n",
        "#         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "zi7PJ9ARQjRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-vit-feature-extractor-3-classes.png\" width=900 alt=\"vit feature extractor with 3 output classes\"/>\n",
        "\n",
        "<font color=\"purple\">Just like our <font color=\"red\">**EffNetB2** feature extractor model</font>, our <font color=\"red\">**ViT** model's</font> base layers are frozen and the output layer is customized to our needs! \n",
        "\n",
        "<font color=\"purple\">Do you notice the big difference though?\n",
        "\n",
        "Our <font color=\"red\">**ViT** model</font> has *far* more parameters than our <font color=\"red\">**EffNetB2** model</font>. Perhaps this will come into play when we compare are our models across speed and performance later on.</font>"
      ],
      "metadata": {
        "id": "TLUSzkrNxH-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Create DataLoaders for ViT feature extractor"
      ],
      "metadata": {
        "id": "HCC68E6RQsO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've got our <font color=\"red\">**ViT** model</font> ready, now let's create some `DataLoader`s for it.\n",
        "\n",
        "We'll do this in the same way we did for <font color=\"red\">**EffNetB2**</font> except we'll use <font color=\"PEAR\">**`vit_transforms`**</font> to transform our images into the same format the <font color=\"red\">**ViT** model</font> was trained on.</font> "
      ],
      "metadata": {
        "id": "G2ZWsvpKxU-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup ViT DataLoaders \n",
        "\"\"\"設定-數據加載器\"\"\"\n",
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                 test_dir=test_dir,\n",
        "                                                 transform=vit_transforms,\n",
        "                                                 batch_size=32)\n",
        "len(train_dataloader_vit), len(test_dataloader_vit), class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6O5f5EjQ6Py",
        "outputId": "b32ecd47-acf9-4064-9dd2-0a4eed53c32f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 5, ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Training ViT Feature Extractor\n",
        "\n",
        "We're up to model experiment number two: a ViT feature extractor."
      ],
      "metadata": {
        "id": "VGZilSlTRj9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">You know what time it is...\n",
        "\n",
        "<font color=\"purple\">...it's traininggggggg time (sung in the same tune as the song [Closing Time](https://youtu.be/xGytDsqkQY8)). \n",
        "\n",
        "<font color=\"purple\">Let's train our <font color=\"red\">**ViT** feature extractor model</font> for 10 epochs using our `engine.train()` function with `torch.optim.Adam()` and a learning rate of `1e-3` as our optimizer and `torch.nn.CrossEntropyLoss()` as our loss function.\n",
        "\n",
        "We'll use our `set_seeds()` function before training to try and make our results as reproducible as possible.</font> "
      ],
      "metadata": {
        "id": "a-Qg8FxgxuvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer \n",
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "\n",
        "# Setup loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train ViT feature extractor with seeds set for reproducibility\n",
        "set_seeds()\n",
        "vit_results = engine.train(model=vit,\n",
        "               train_dataloader=train_dataloader_vit,\n",
        "               test_dataloader=test_dataloader_vit,\n",
        "               epochs=10,\n",
        "               optimizer=optimizer,\n",
        "               loss_fn=loss_fn,\n",
        "               device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227,
          "referenced_widgets": [
            "cb5027e48c7849d2a66300789b5b6db6",
            "f5a0592da3a94970b8bc47b10282054b",
            "9d2e2f76d33d4d7cb85fab5848a8b495",
            "d212279b331648d2bce193c9cae0e25b",
            "c5060376c93048f79f70ee65cd08f3df",
            "fabf0282728d4dce9bb0949219d0a329",
            "511211588f62455a961d966390cff822",
            "e416f7063a2c413fb808910463499be4",
            "ede6ba36dbaa4a1687dad9052ee1e908",
            "739d5b7498274710b8a336584cc4ef65",
            "2b5c1a40a25240a586179f54d14579df"
          ]
        },
        "id": "UsO1FtiTRtzg",
        "outputId": "9f5b5539-a646-445b-ede8-b2f054f81988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb5027e48c7849d2a66300789b5b6db6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.7023 | train_acc: 0.7500 | test_loss: 0.2714 | test_acc: 0.9290\n",
            "Epoch: 2 | train_loss: 0.2531 | train_acc: 0.9104 | test_loss: 0.1669 | test_acc: 0.9602\n",
            "Epoch: 3 | train_loss: 0.1766 | train_acc: 0.9542 | test_loss: 0.1270 | test_acc: 0.9693\n",
            "Epoch: 4 | train_loss: 0.1277 | train_acc: 0.9625 | test_loss: 0.1072 | test_acc: 0.9722\n",
            "Epoch: 5 | train_loss: 0.1163 | train_acc: 0.9646 | test_loss: 0.0950 | test_acc: 0.9784\n",
            "Epoch: 6 | train_loss: 0.1270 | train_acc: 0.9375 | test_loss: 0.0830 | test_acc: 0.9722\n",
            "Epoch: 7 | train_loss: 0.0899 | train_acc: 0.9771 | test_loss: 0.0844 | test_acc: 0.9784\n",
            "Epoch: 8 | train_loss: 0.0928 | train_acc: 0.9812 | test_loss: 0.0759 | test_acc: 0.9722\n",
            "Epoch: 9 | train_loss: 0.0933 | train_acc: 0.9792 | test_loss: 0.0729 | test_acc: 0.9784\n",
            "Epoch: 10 | train_loss: 0.0662 | train_acc: 0.9833 | test_loss: 0.0642 | test_acc: 0.9847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Plot loss curves of ViT feature extractor"
      ],
      "metadata": {
        "id": "EuPMeMMlS3NE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Alright, alright, alright, ViT model trained, let's get visual and see some loss curves.\n",
        "\n",
        "> **Note:** Don't forget you can see what an ideal set of loss curves should look like in [04. PyTorch Custom Datasets section 8](https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like).</font>"
      ],
      "metadata": {
        "id": "y2nlFIlwyqNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(vit_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "LPpkHiuaS9lj",
        "outputId": "abce7102-e6b4-430a-f3d5-a558f5d90bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more on what an ideal loss curves should look like see here: https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like"
      ],
      "metadata": {
        "id": "9zqiyaBWTBqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Ohh yeah! \n",
        "\n",
        "<font color=\"purple\">Those are some nice looking loss curves. Just like our <font color=\"red\">**EffNetB2** feature extractor model</font>, it looks our <font color=\"red\">**ViT** model</font> might benefit from a little longer training time and perhaps some [data augmentation](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation) (to help prevent overfitting).</font>"
      ],
      "metadata": {
        "id": "FjYfUR4F0bAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Saving ViT feature extractor"
      ],
      "metadata": {
        "id": "1C8p7jY5TQ1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Our <font color=\"red\">**ViT** model</font> is performing outstanding! \n",
        "\n",
        "<font color=\"purple\">So let's save it to file so we can import it and use it later if we wish.\n",
        "\n",
        "<font color=\"purple\">We can do so using the `utils.save_model()` function we created in [05. PyTorch Going Modular section 5](https://www.learnpytorch.io/05_pytorch_going_modular/#5-creating-a-function-to-save-the-model-utilspy). "
      ],
      "metadata": {
        "id": "jgniD87A0q3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "from going_modular.going_modular import utils\n",
        "\n",
        "utils.save_model(model=vit,\n",
        "         target_dir=\"models\",\n",
        "         model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5QAFc1bTopE",
        "outputId": "b3d2edab-fd75-42fb-a647-f09c9f4fcaf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving model to: models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 Checking the size of ViT feature extractor"
      ],
      "metadata": {
        "id": "mu0mdgcTT2yi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">And since we want to compare our <font color=\"red\">**EffNetB2** model</font> to our <font color=\"red\">**ViT** model</font> across a number of characteristics, let's find out its size.\n",
        "\n",
        "To check our model's size in bytes, we can use Python's <font color=\"PEAR\">**`pathlib.Path.stat(\"path_to_model\").st_size`**</font> and then we can convert it (roughly) to megabytes by dividing it by `(1024*1024)`.</font> "
      ],
      "metadata": {
        "id": "-MfTVwX23eog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size / (1024*1024)\n",
        "print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjmHwt-iT9-5",
        "outputId": "5d1dd2df-3322-43a2-fb1a-69504d4e1f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained ViT feature extractor model size: 327.36128330230713 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Hmm, how does the <font color=\"red\">**ViT** feature extractor model</font> size compare to our <font color=\"red\">**EffNetB2** model</font> size?\n",
        "\n",
        "<font color=\"purple\">We'll find this out shortly when we compare all of our model's characteristics."
      ],
      "metadata": {
        "id": "HvCt29Vf35cD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.7 Collecting ViT feature extractor stats"
      ],
      "metadata": {
        "id": "AbHS2zyXUL87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Let's put together all of our <font color=\"red\">**ViT** feature extractor model</font> statistics.\n",
        "\n",
        "<font color=\"purple\">We saw it in the summary output above but we'll calculate its total number of parameters."
      ],
      "metadata": {
        "id": "CTBR1n8s4Eme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of parameters in ViT \n",
        "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
        "vit_total_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7S3eAIXUV6U",
        "outputId": "593fef39-b6a9-429b-898f-0b097713541d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85800963"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Woah, that looks like a fair bit more than our <font color=\"red\">**EffNetB2**</font>!</font>\n",
        "\n",
        "> <font color=\"purple\">**Note:** A larger number of parameters (or weights/patterns) generally means a model has a higher *capacity* to learn, whether it actually uses this extra capacity is another story. In light of this, our <font color=\"red\">**EffNetB2** model</font> has 7,705,221 parameters where as our <font color=\"red\">**ViT** model</font> has 85,800,963 (11.1x more) so we could assume that our <font color=\"red\">**ViT** model</font> has more of a capacity to learn, if given more data (more opportunities to learn). However, this larger capacity to learn often comes with an  increased model filesize and a longer time to perform inference.\n",
        "\n",
        "Now let's create a dictionary with some important characteristics of our ViT model.</font>"
      ],
      "metadata": {
        "id": "tO9H53is4P44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ViT statistics dictionary\n",
        "\"\"\"ViT的統計dictionary: test_loss, test_acc, number_of_parameters, model_size\"\"\"\n",
        "vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n",
        "        \"test_acc\": vit_results[\"test_acc\"][-1],\n",
        "        \"number_of_parameters\": vit_total_params,\n",
        "        \"model_size (MB)\": pretrained_vit_model_size}"
      ],
      "metadata": {
        "id": "AT-D4Q6zVN8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec3mJ3s0V1Yi",
        "outputId": "4212a029-960d-4d53-97dd-7f2f0eae6e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.06418212698772549,\n",
              " 'test_acc': 0.984659090909091,\n",
              " 'number_of_parameters': 85800963,\n",
              " 'model_size (MB)': 327.36128330230713}"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Nice! Looks like our <font color=\"red\">**ViT** model</font> achieves over 95% accuracy too."
      ],
      "metadata": {
        "id": "XcawOlvP4hsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Making predictions with our trained models and timing them\n",
        "\n",
        "Our goal:\n",
        "1. Performs well (95%+ test accuracy)\n",
        "2. Fast (30+FPS)\n",
        "\n",
        "To test criteria two:\n",
        "1. Loop through test images\n",
        "2. Time how long each model takes to make a prediction on the image \n",
        "\n",
        "Let's work towards making a function called `pred_and_store()` to do so. \n",
        "\n",
        "First we'll need a list of test image paths. "
      ],
      "metadata": {
        "id": "0Ml5ZoB2V32u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've got a couple of trained models, both performing pretty well.\n",
        "\n",
        "Now how about we test them out doing what we'd like them to do?</font>\n",
        "\n",
        "<font color=\"purple\">As in, let's see how they go making predictions (performing inference).\n",
        "\n",
        "We know both of our models are performing at over 95% accuracy on the test dataset, but how fast are they?</font>\n",
        "\n",
        "<font color=\"purple\">Ideally, if we're deploying our FoodVision Mini model to a mobile device so people can take photos of their food and identify it, we'd like the predictions to happen at real-time (~30 frames per second).\n",
        "\n",
        "That's why our second criteria is: a fast model.</font>\n",
        "\n",
        "<font color=\"purple\">To find out how long each of our models take to performance inference, let's create a function called <font color=\"PEAR\">**`pred_and_store()`**</font> to iterate over each of the test dataset images one by one and perform a prediction. \n",
        "\n",
        "We'll time each of the predictions as well as store the results in a common prediction format: a list of dictionaries (where each element in the list is a single prediction and each sinlge prediction is a dictionary).</font> \n",
        "\n",
        "> <font color=\"purple\">**Note:** We time the predictions one by one rather than by batch because when our model is deployed, it will likely only be making a prediction on one image at a time. As in, someone takes a photo and our model predicts on that single image.\n",
        "\n",
        "Since we'd like to make predictions across all the images in the test set, let's first get a list of all of the test image paths so we can iterate over them.</font> \n",
        "\n",
        "<font color=\"purple\">To do so, we'll use Python's [<font color=\"PEAR\">**`pathlib.Path(\"target_dir\").glob(\"*/*.jpg\"))`**</font>](https://docs.python.org/3/library/pathlib.html#basic-use) to find all of the filepaths in a target directory with the extension `.jpg` (all of our test images).</font>"
      ],
      "metadata": {
        "id": "z_Cf0aHa4q_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get all test data paths\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_data_paths[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_MNGQPsWGDe",
        "outputId": "3be1f1e2-0f5a-4ee7-96c9-e5b9508ad7f1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('data/pizza_steak_sushi_20_percent/test/sushi/2903621.jpg'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test/sushi/301603.jpg'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test/sushi/3834718.jpg'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test/sushi/2743100.jpg'),\n",
              " PosixPath('data/pizza_steak_sushi_20_percent/test/sushi/1844723.jpg')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Creating a function to make across the test dataset\n",
        "\n",
        "Steps to create `pred_and_store()`:\n",
        "\n",
        "1. Create a function that takes a list of paths and a trained PyTorch and a series of transforms a list of target class names and a target device.\n",
        "2. Create an empty list (can return a full list of all predictions later).\n",
        "3. Loop through the target input paths (the rest of the steps will take place inside the loop).\n",
        "4. Create an empty dictionary for each sample (prediction statistics will go in here).\n",
        "5. Get the sample path and ground truth class from the filepath.\n",
        "6. Start the prediction timer. \n",
        "7. Open the image using `PIL.Image.open(path)`.\n",
        "8. Transform the image to be usable with a given model.\n",
        "9. Prepare the model model for inference by sending to the target device and turning on `eval()` mode.\n",
        "10. Turn on `torch.inference_mode()` and pass the target transformed image to the model and perform forward pass + calculate pred prob + pred class.\n",
        "11. Add the pred prob + pred class to empty dictionary from step 4. \n",
        "12. End the prediction timer started in step 6 and add the time to the prediction dictionary.\n",
        "13. See if the predicted class matches the ground truth class.\n",
        "14. Append the updated prediction dictionary to the empty list of predictions we created in step 2.\n",
        "15. Return the list of prediction dictionaries."
      ],
      "metadata": {
        "id": "t0JHPvLWkudO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Now we've got a list of our test image paths, let's get to work on our `pred_and_store()` function:\n",
        "\n",
        "1. Create a function that takes a <ins>list of *paths*</ins>, a <ins>trained PyTorch *model*</ins>, a series of <ins>transforms</ins> (to prepare images), a <ins>list of target *class names*</ins> and a <ins>target *device*</ins>.\n",
        "2. Create an <ins>empty *list*</ins> to store prediction dictionaries (we want the function to return a <ins>list of dictionaries</ins>, one for each prediction).\n",
        "3. Loop through the target input *paths* (steps 4-14 will happen inside the loop).\n",
        "4. Create an <ins>empty dictionary</ins> for each iteration in the loop to <ins>store prediction values</ins> per sample.\n",
        "5. Get the sample <ins>path</ins> and ground truth <ins>class name</ins> (we can do this by infering the class from the path).\n",
        "6. Start the prediction <ins>timer</ins> using Python's [`timeit.default_timer()`](https://docs.python.org/3/library/timeit.html#timeit.default_timer).\n",
        "7. <ins>Open the image</ins> using [`PIL.Image.open(path)`](https://pillow.readthedocs.io/en/stable/reference/Image.html#functions).\n",
        "8. <ins>Transform the image</ins> so it's capable of being using with the target model as well as <ins>add a batch dimension</ins> and <ins>send the image to the target device</ins>.\n",
        "9. Prepare the model for inference by <ins>sending it to the target *device*</ins> and <ins>turning on **`eval()`** mode</ins>.\n",
        "10. <ins>Turn on [`torch.inference_mode()`](https://pytorch.org/docs/stable/generated/torch.inference_mode.html)</ins> and <ins>pass the target transformed image to the model</ins> and <ins>calculate the prediction probability</ins> using `torch.softmax()` and the target label using `torch.argmax()`.\n",
        "11. Add the *prediction probability* and *prediction class* to the prediction dictionary created in step 4. Also make sure the prediction probability is on the CPU so it can be used with non-GPU libraries such as NumPy and pandas for later inspection.\n",
        "12. End the prediction <ins>timer</ins> started in step 6 and <ins>add the time</ins> to the prediction dictionary created in step 4.\n",
        "13. See if the *predicted class* matches the *ground truth class* from step 5 and <ins>add the result</ins> to the prediction dictionary created in step 4.\n",
        "14. <ins>Append the updated prediction dictionary</ins> to the empty list of predictions created in step 2.\n",
        "15. Return the <ins>list of prediction dictionaries</ins>.\n",
        "\n",
        "<font color=\"purple\">A bunch of steps, but nothing we can't handle!\n",
        "\n",
        "Let's do it.</font>"
      ],
      "metadata": {
        "id": "gOqT6e7-dzVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"對照7.2 predict(img)\"\"\"\n",
        "import pathlib\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from timeit import default_timer as timer # https://docs.python.org/3/library/timeit.html#timeit.default_timer \n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict\n",
        "\n",
        "# 1. Create a function that takes a list of paths and a trained PyTorch and a series of transforms a list of target class names and a target device.\n",
        "def pred_and_store(paths: List[pathlib.Path],\n",
        "           model: torch.nn.Module,\n",
        "           transform: torchvision.transforms,\n",
        "           class_names: List[str],\n",
        "           device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n",
        "  \"\"\"\n",
        "  paths: 要預測的圖檔路徑\n",
        "  model: 用來預測的模型\n",
        "  transform: 轉換\n",
        "  class_names: 可預測類別名稱\n",
        "  device: 裝置\n",
        "  \"\"\"\n",
        "  \n",
        "  # 2. Create an empty list (can return a full list of all predictions later).\n",
        "  pred_list = []\n",
        "\n",
        "  # 3. Loop through the target input paths (the rest of the steps will take place inside the loop).\n",
        "  for path in tqdm(paths):\n",
        "\n",
        "    # 4. Create an empty dictionary for each sample (prediction statistics will go in here). \n",
        "    pred_dict = {}\n",
        "\n",
        "    # 5. Get the sample path and ground truth class from the filepath.\n",
        "    pred_dict[\"image_path\"] = path\n",
        "    class_name = path.parent.stem\n",
        "    pred_dict[\"class_name\"] = class_name\n",
        "\n",
        "    # 6. Start the prediction timer.\n",
        "    start_time = timer()\n",
        "\n",
        "    # 7. Open the image using Image.open(path)\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # 8. Transform the image to be usable with a given model (also add a batch dimension and send to target device)\n",
        "    transformed_image = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # 9. Prepare the model model for inference by sending to the target device and turning on eval() mode.\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 10. Turn on `torch.inference_mode()` and pass the target transformed image to the model and perform forward pass + calculate pred prob + pred class.\n",
        "    with torch.inference_mode():\n",
        "      pred_logit = model(transformed_image)\n",
        "      pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into predicition probabilities\n",
        "      pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probability into prediction label\n",
        "      pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU (Python variables live on CPU)\n",
        "  \n",
        "      # 11. Add the pred prob + pred class to empty dictionary from step 4. \n",
        "      pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
        "      pred_dict[\"pred_class\"] = pred_class\n",
        "\n",
        "      # 12. End the prediction timer started in step 6 and add the time to the prediction dictionary.\n",
        "      end_time = timer()\n",
        "      pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n",
        "\n",
        "    # 13. See if the predicted class matches the ground truth class. \n",
        "    pred_dict[\"correct\"] = class_name == pred_class\n",
        "\n",
        "    # 14. Append the updated prediction dictionary to the empty list of predictions we created in step 2. \n",
        "    pred_list.append(pred_dict) \n",
        "\n",
        "  # 15. Return the list of prediction dictionaries.\n",
        "  return pred_list"
      ],
      "metadata": {
        "id": "06yutpY0mlCm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Ho, ho! \n",
        "\n",
        "What a good looking function!</font>\n",
        "\n",
        "<font color=\"purple\">And you know what, since our <font color=\"PEAR\">**`pred_and_store()`**</font> is a pretty good utility function for making and storing predictions, it could be stored to [`going_modular.going_modular.predictions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/predictions.py) for later use. That might be an extension you'd like to try, check out [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/) for ideas."
      ],
      "metadata": {
        "id": "cN0HdSQvekVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Making and timing predictions with EffNetB2\n",
        "\n",
        "Let's test our `pred_and_store()` function.\n",
        "\n",
        "Two things to note:\n",
        "1. Device - we're going to hardcode our predictions to happen on CPU (because you won't always be sure of having a GPU when you deploy your model).\n",
        "2. Transforms - we want to make sure each of the models are predicting on images that have been prepared with the appropriate transforms (e.g. EffNetB2 with `effnetb2_transforms`) \n"
      ],
      "metadata": {
        "id": "NVUy5XTRms5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Let's start by using it to make predictions across the test dataset with our EffNetB2 model, paying attention to two details:\n",
        "1. **Device** - We'll hard code the `device` parameter to use `\"cpu\"` because when we deploy our model, we won't always have access to a `\"cuda\"` (GPU) device.\n",
        "    * <font color=\"purple\">Making the predictions on CPU will be a good indicator of speed of inference too because generally predictions on CPU devices are slower than GPU devices.\n",
        "2. **Transforms** - We'll also be sure to set the `transform` parameter to `effnetb2_transforms` to make sure the images are opened and transformed in the same way our `effnetb2` model has been trained on. "
      ],
      "metadata": {
        "id": "xVdBlwDfitoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions test dataset with EffNetB2 (Make predictions across test dataset with EffNetB2)\n",
        "\"\"\"(一)\"\"\"\n",
        "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                        model=effnetb2,\n",
        "                        transform=effnetb2_transforms,\n",
        "                        class_names=class_names,\n",
        "                        device=\"cpu\") # hardcode predictions to happen on CPU (make predictions on CPU)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0d0b2fbe5b904839b575c60f007c9977",
            "a5b08b956f794e99bf76fc5cc9679a2d",
            "144570d550b64d11958b2e497696b7cd",
            "a8b2cee8020a448a8b568a471148d981",
            "96017667844f4a67a6e670c66c2141ab",
            "9a906e7dd2714589bd586e08565ff95b",
            "8df61ce127634d848c0054b2e0b72974",
            "35ba32b372f14557bdec85747f349318",
            "e4bd9761438f4734b674ebcc5e71fa08",
            "d134831ffa9e446b85d36ec9ea0b2105",
            "cf83b2528f8146b0a1b5b34e4f2f53f3"
          ]
        },
        "id": "sduFwTaomzTH",
        "outputId": "b4b43263-f1ea-4523-ec0c-0698f5d18246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/150 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d0b2fbe5b904839b575c60f007c9977"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Nice! Look at those predictions fly!\n",
        "\n",
        "Let's inspect the first couple and see what they look like.</font>"
      ],
      "metadata": {
        "id": "j9clthPYjGwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the first 2 prediction dictionaries\n",
        "\"\"\"(二)\"\"\"\n",
        "effnetb2_test_pred_dicts[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMgD-dmdsHXQ",
        "outputId": "d08a607a-c980-4931-9758-6576eb4b587f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/sushi/1203702.jpg'),\n",
              "  'class_name': 'sushi',\n",
              "  'pred_prob': 0.8812,\n",
              "  'pred_class': 'sushi',\n",
              "  'time_for_pred': 0.1742,\n",
              "  'correct': True},\n",
              " {'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/sushi/46797.jpg'),\n",
              "  'class_name': 'sushi',\n",
              "  'pred_prob': 0.9184,\n",
              "  'pred_class': 'sushi',\n",
              "  'time_for_pred': 0.1266,\n",
              "  'correct': True}]"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Woohoo!\n",
        "\n",
        "It looks like our <font color=\"PEAR\">**`pred_and_store()`**</font> function worked nicely.</font>\n",
        "\n",
        "<font color=\"purple\">Thanks to our list of dictionaries data structure, we've got plenty of useful information we can further inspect.\n",
        "\n",
        "To do so, let's turn our list of dictionaries into a pandas DataFrame.</font>"
      ],
      "metadata": {
        "id": "1AykXbQnjV8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the test_pred_dicts into a DataFrame\n",
        "\"\"\"(三)\"\"\"\n",
        "import pandas as pd\n",
        "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
        "effnetb2_test_pred_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6KoSTMaNsN8_",
        "outputId": "8ffa9058-5cc7-4aaa-b3c4-386d4ec64150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          image_path class_name  pred_prob  \\\n",
              "0  data/pizza_steak_sushi_20_percent/test/sushi/1...      sushi     0.8812   \n",
              "1  data/pizza_steak_sushi_20_percent/test/sushi/4...      sushi     0.9184   \n",
              "2  data/pizza_steak_sushi_20_percent/test/sushi/3...      sushi     0.6810   \n",
              "3  data/pizza_steak_sushi_20_percent/test/sushi/3...      sushi     0.8659   \n",
              "4  data/pizza_steak_sushi_20_percent/test/sushi/1...      sushi     0.8830   \n",
              "\n",
              "  pred_class  time_for_pred  correct  \n",
              "0      sushi         0.1742     True  \n",
              "1      sushi         0.1266     True  \n",
              "2      sushi         0.1276     True  \n",
              "3      sushi         0.1258     True  \n",
              "4      sushi         0.1247     True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2fb8c555-7130-4f1b-a83b-feb81bc531a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>class_name</th>\n",
              "      <th>pred_prob</th>\n",
              "      <th>pred_class</th>\n",
              "      <th>time_for_pred</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/sushi/1...</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.8812</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.1742</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/sushi/4...</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.9184</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.1266</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/sushi/3...</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.6810</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/sushi/3...</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.8659</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.1258</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/sushi/1...</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.8830</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.1247</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2fb8c555-7130-4f1b-a83b-feb81bc531a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2fb8c555-7130-4f1b-a83b-feb81bc531a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2fb8c555-7130-4f1b-a83b-feb81bc531a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Beautiful!\n",
        "\n",
        "<font color=\"purple\">Look how easily those prediction dictionaries turn into a structured format we can perform analysis on.\n",
        "\n",
        "Such as finding how many predictions our <font color=\"red\">**EffNetB2** model</font> got wrong...</font>"
      ],
      "metadata": {
        "id": "QTUevFxBjkBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of correct predictions\n",
        "\"\"\"(四)\"\"\"\n",
        "effnetb2_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djw2PSQ7sjB8",
        "outputId": "fc742e7d-102a-45a9-97aa-279584187cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     145\n",
              "False      5\n",
              "Name: correct, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Five wrong predictions out of 150 total, not bad!\n",
        "\n",
        "And how about the average prediction time?</font>"
      ],
      "metadata": {
        "id": "gm135GTmj0F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the average time per prediction \n",
        "\"\"\"(五)\"\"\"\n",
        "effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxxgCNKFs2e2",
        "outputId": "c3ba39ec-28a2-450e-8c32-9aa943033151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EffNetB2 average time per prediction: 0.1244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** Prediction times will vary (much like training times) depending on the hardware you're using... so generally the faster your compute (e.g. CPU or GPU), the faster the predictions will happen."
      ],
      "metadata": {
        "id": "vHT8FLadtOda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Hmm, how does that average prediction time live up to our criteria of our model performing at real-time (~30FPS or 0.03 seconds per prediction)?\n",
        "> <font color=\"purple\">**Note:** Prediction times will be different across different hardware types (e.g. a local Intel i9 vs Google Colab CPU). The better and faster the hardware, generally, the faster the prediction. For example, on my local deep learning PC with an Intel i9 chip, my average prediction time with <font color=\"red\">**EffNetB2**</font> is around 0.031 seconds (just under real-time). However, on Google Colab (I'm not sure what CPU hardware Colab uses but it looks like it might be an [Intel(R) Xeon(R)](https://stackoverflow.com/questions/47805170/whats-the-hardware-spec-for-google-colaboratory)), my average prediction time with <font color=\"red\">**EffNetB2**</font> is about 0.1396 seconds (3-4x slower).\n",
        "\n",
        "Let's add our <font color=\"red\">**EffNetB2**</font> average time per prediction to our <font color=\"PEAR\">**`effnetb2_stats`**</font> dictionary.</font>"
      ],
      "metadata": {
        "id": "n43u5KdzkGf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add time per pred to EffNetB2 stats dictionary (Add EffNetB2 average prediction time to stats dictionary)\n",
        "\"\"\"(六)\"\"\"\n",
        "effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred \n",
        "effnetb2_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3drhFrfwAkF",
        "outputId": "bb40c859-6a91-4838-a6c0-a5ab1bc55a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.28128677904605864,\n",
              " 'test_acc': 0.96875,\n",
              " 'number_of_parameters': 7705221,\n",
              " 'model_size (MB)': 29.824288368225098,\n",
              " 'time_per_pred_cpu': 0.1244}"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Making and timing predictions with ViT "
      ],
      "metadata": {
        "id": "S7O84_p3tKPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've made predictions with our <font color=\"red\">**EffNetB2** model</font>, now let's do the same for our <font color=\"red\">**ViT** model</font>.\n",
        "\n",
        "<font color=\"purple\">To do so, we can use the <font color=\"PEAR\">**`pred_and_store()`**</font> function we created above except this time we'll pass in our <font color=\"red\">**`vit`** model</font> as well as the <font color=\"PEAR\">**`vit_transforms`**</font>.\n",
        "\n",
        "And we'll keep the predictions on the CPU via `device=\"cpu\"` (a natural extension here would be to test the prediction times on CPU and on GPU).</font>"
      ],
      "metadata": {
        "id": "49C1xI-2sZfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make list of prediction dictionaries with ViT feature extractor model on test images \n",
        "\"\"\"(一)\"\"\"\n",
        "vit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                    model=vit,\n",
        "                    transform=vit_transforms,\n",
        "                    class_names=class_names,\n",
        "                    device=\"cpu\") # hardcode device to CPU because not sure if GPU available when we deploy "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "dae44612c46644c8b496a7f9bfa6c3c2",
            "48486a66fd844a9290ec49866c134b3c",
            "3983c92304d04553a213755aac582367",
            "4b95cf3fb5f34ef98a83ef1fe14d9db6",
            "65db50a782564d31b1d0f81224adfd56",
            "aa5ad051ad4b4d1299e417bbfd15581b",
            "e5023f3772b14e1ca6f31a683cc9849e",
            "55b52e781a074d27b054eefe594e06c8",
            "aa902acee91d41219628f85319b4902e",
            "95bfc6e17e634d2c831d79aef7ec545d",
            "b3ba57b373e140988c34219c07bed764"
          ]
        },
        "id": "1eWeigxfumoO",
        "outputId": "46a25224-ebb4-4fdf-881d-facaf2754e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/150 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dae44612c46644c8b496a7f9bfa6c3c2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Predictions made!\n",
        "\n",
        "Now let's check out the first couple.</font>"
      ],
      "metadata": {
        "id": "Iq2wkow2srPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first couple of ViT predictions (Check the first couple of ViT predictions on the test dataset)\n",
        "\"\"\"(二)\"\"\"\n",
        "vit_test_pred_dicts[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTjpX207u6hQ",
        "outputId": "260e0702-14f0-4749-800e-c2010a811bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/sushi/1203702.jpg'),\n",
              "  'class_name': 'sushi',\n",
              "  'pred_prob': 0.9589,\n",
              "  'pred_class': 'sushi',\n",
              "  'time_for_pred': 0.7028,\n",
              "  'correct': True},\n",
              " {'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/sushi/46797.jpg'),\n",
              "  'class_name': 'sushi',\n",
              "  'pred_prob': 0.9927,\n",
              "  'pred_class': 'sushi',\n",
              "  'time_for_pred': 0.5721,\n",
              "  'correct': True}]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Wonderful!\n",
        "\n",
        "And just like before, since our <font color=\"red\">**ViT** model's</font> predictions are in the form of a list of dictionaries, we can easily turn them into a pandas DataFrame for further inspection.</font> "
      ],
      "metadata": {
        "id": "9DF8JPHks6tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn vit_test_pred_dicts (Turn vit_test_pred_dicts into a DataFrame)\n",
        "\"\"\"(三)\"\"\"\n",
        "import pandas as pd\n",
        "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
        "vit_test_pred_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ivd1lYSHvHvz",
        "outputId": "b83690e8-5c22-4d06-9e2f-a070370bfef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          image_path class_name  pred_prob  \\\n",
              "0  data/pizza_steak_sushi_20_percent/test/sushi/1...      sushi     0.9589   \n",
              "1  data/pizza_steak_sushi_20_percent/test/sushi/4...      sushi     0.9927   \n",
              "2  data/pizza_steak_sushi_20_percent/test/sushi/3...      sushi     0.9908   \n",
              "3  data/pizza_steak_sushi_20_percent/test/sushi/3...      sushi     0.4956   \n",
              "4  data/pizza_steak_sushi_20_percent/test/sushi/1...      sushi     0.9870   \n",
              "\n",
              "  pred_class  time_for_pred  correct  \n",
              "0      sushi         0.7028     True  \n",
              "1      sushi         0.5721     True  \n",
              "2      sushi         0.5463     True  \n",
              "3      pizza         0.5606    False  \n",
              "4      sushi         0.5535     True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a734f163-cb4a-46b2-9e73-fbc646487af7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>class_name</th>\n",
              "      <th>pred_prob</th>\n",
              "      <th>pred_class</th>\n",
              "      <th>time_for_pred</th>\n",
              "      <th>correct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/sushi/1...</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.9589</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.7028</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/sushi/4...</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.9927</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.5721</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/sushi/3...</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.9908</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.5463</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/sushi/3...</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>pizza</td>\n",
              "      <td>0.5606</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/pizza_steak_sushi_20_percent/test/sushi/1...</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.9870</td>\n",
              "      <td>sushi</td>\n",
              "      <td>0.5535</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a734f163-cb4a-46b2-9e73-fbc646487af7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a734f163-cb4a-46b2-9e73-fbc646487af7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a734f163-cb4a-46b2-9e73-fbc646487af7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">How many predictions did our <font color=\"red\">**ViT** model</font> get correct?"
      ],
      "metadata": {
        "id": "2AZp2DOrtNAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# See how many correct (Count the number of correct predictions)\n",
        "\"\"\"(四)\"\"\"\n",
        "vit_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbOWvg2NvQLc",
        "outputId": "dc26274f-3895-49d9-8b80-df1ee5f4728c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     148\n",
              "False      2\n",
              "Name: correct, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Woah!\n",
        "\n",
        "Our <font color=\"red\">**ViT** model</font> did a little better than our <font color=\"red\">**EffNetB2** model</font> in terms of correct predictions, only two samples wrong across the whole test dataset.</font>\n",
        "\n",
        "<font color=\"purple\">As an extension you might want to visualize the <font color=\"red\">**ViT** model's</font> wrong predictions and see if there's any reason why it might've got them wrong.\n",
        "\n",
        "How about we calculate how long the <font color=\"red\">**ViT** model</font> took per prediction?</font>"
      ],
      "metadata": {
        "id": "jG6d1V16tX2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average time per prediction for ViT model \n",
        "\"\"\"(五)\"\"\"\n",
        "vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"ViT average time per prediction: {vit_average_time_per_pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSa1HzC2vcRl",
        "outputId": "41f6f50e-b5f1-491d-8ad9-1c7e830df4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViT average time per prediction: 0.5554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Well, that looks a little slower than our <font color=\"red\">**EffNetB2** model's</font> average time per prediction but how does it look in terms of our second criteria: speed?\n",
        "\n",
        "For now, let's add the value to our <font color=\"red\">**`vit_stats`** dictionary</font> so we can compare it to our <font color=\"red\">**EffNetB2** model's</font> stats.</font>\n",
        "\n",
        "> <font color=\"purple\">**Note:** The average time per prediction values will be highly dependent on the hardware you make them on. For example, for the ViT model, my average time per prediction (on the CPU) was 0.0693-0.0777 seconds on my local deep learning PC with an Intel i9 CPU. Where as on Google Colab, my average time per prediction with the ViT model was 0.6766-0.7113 seconds."
      ],
      "metadata": {
        "id": "jG2iGO-htnDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add average time per prediction to ViT stats (Add average prediction time for ViT model on CPU)\n",
        "\"\"\"(六)\"\"\"\n",
        "vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\n",
        "vit_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgfzAKT1vxFe",
        "outputId": "9c815d86-32c4-443a-dd84-279df269dbd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.06418212698772549,\n",
              " 'test_acc': 0.984659090909091,\n",
              " 'number_of_parameters': 85800963,\n",
              " 'model_size (MB)': 327.36128330230713,\n",
              " 'time_per_pred_cpu': 0.5554}"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# effnetb2_stats={'test_loss': 0.28128677904605864,\n",
        "#  'test_acc': 0.96875,\n",
        "#  'number_of_parameters': 7705221,\n",
        "#  'model_size (MB)': 29.824288368225098,\n",
        "#  'time_per_pred_cpu': 0.1244\n",
        "#  }"
      ],
      "metadata": {
        "id": "-1Gcpjkj_3JL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vit_stats = {'test_loss': 0.06418212698772549,\n",
        "#  'test_acc': 0.984659090909091,\n",
        "#  'number_of_parameters': 85800963,\n",
        "#  'model_size (MB)': 327.36128330230713,\n",
        "#  'time_per_pred_cpu': 0.5554\n",
        "#  }"
      ],
      "metadata": {
        "id": "MCa9P2Ka_fD-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
        "# df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "IyNuvu05F0gH",
        "outputId": "0e2cdb09-b254-40b6-e97e-4b38b64d04c3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   test_loss  test_acc  number_of_parameters  model_size (MB)  \\\n",
              "0   0.281287  0.968750               7705221        29.824288   \n",
              "1   0.064182  0.984659              85800963       327.361283   \n",
              "\n",
              "   time_per_pred_cpu  \n",
              "0             0.1244  \n",
              "1             0.5554  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07ce4fe5-115f-438d-8fe4-2f4cf90f29e9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_acc</th>\n",
              "      <th>number_of_parameters</th>\n",
              "      <th>model_size (MB)</th>\n",
              "      <th>time_per_pred_cpu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.281287</td>\n",
              "      <td>0.968750</td>\n",
              "      <td>7705221</td>\n",
              "      <td>29.824288</td>\n",
              "      <td>0.1244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.064182</td>\n",
              "      <td>0.984659</td>\n",
              "      <td>85800963</td>\n",
              "      <td>327.361283</td>\n",
              "      <td>0.5554</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07ce4fe5-115f-438d-8fe4-2f4cf90f29e9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-07ce4fe5-115f-438d-8fe4-2f4cf90f29e9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-07ce4fe5-115f-438d-8fe4-2f4cf90f29e9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df[0:1]   # 取第一個row\n",
        "# df.iloc[0] # 取第一個row\n",
        "# df[0]    #錯誤寫法"
      ],
      "metadata": {
        "id": "emzbTgdIJCXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Comparing model results, prediction times and size"
      ],
      "metadata": {
        "id": "Kofmdazwv-JT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Our two best model contenders have been trained and evaluated.\n",
        "\n",
        "Now let's put them head to head and compare across their different statistics.</font>\n",
        "\n",
        "<font color=\"purple\">To do so, let's turn our <font color=\"PEAR\">**`effnetb2_stats`**</font> and <font color=\"PEAR\">**`vit_stats`**</font> ***dictionaries*** into a pandas DataFrame.\n",
        "\n",
        "<font color=\"purple\">We'll add a column to view the model names as well as the convert the test accuracy to a whole percentage rather than decimal."
      ],
      "metadata": {
        "id": "_XVjM7ZBt49f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn stat dictionaries into DataFrame \n",
        "\"\"\"用effnetb2_stats與vit_stats兩個dictionaries合併成一個DataFrame\"\"\"\n",
        "# import pandas as pd\n",
        "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
        "\n",
        "# Add column for model names\n",
        "df[\"model\"] = [\"EffNetB2\", \"ViT\"] \n",
        "\n",
        "# Convert accuracy to percentages\n",
        "df[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2) \n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "TOtprDnSwvD8",
        "outputId": "6fda6b74-2f36-415c-84eb-7de973bed037"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   test_loss  test_acc  number_of_parameters  model_size (MB)  \\\n",
              "0   0.281287     96.88               7705221        29.824288   \n",
              "1   0.064182     98.47              85800963       327.361283   \n",
              "\n",
              "   time_per_pred_cpu     model  \n",
              "0             0.1244  EffNetB2  \n",
              "1             0.5554       ViT  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab574ab6-b939-420b-9916-af9f2ed01562\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_acc</th>\n",
              "      <th>number_of_parameters</th>\n",
              "      <th>model_size (MB)</th>\n",
              "      <th>time_per_pred_cpu</th>\n",
              "      <th>model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.281287</td>\n",
              "      <td>96.88</td>\n",
              "      <td>7705221</td>\n",
              "      <td>29.824288</td>\n",
              "      <td>0.1244</td>\n",
              "      <td>EffNetB2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.064182</td>\n",
              "      <td>98.47</td>\n",
              "      <td>85800963</td>\n",
              "      <td>327.361283</td>\n",
              "      <td>0.5554</td>\n",
              "      <td>ViT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab574ab6-b939-420b-9916-af9f2ed01562')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ab574ab6-b939-420b-9916-af9f2ed01562 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ab574ab6-b939-420b-9916-af9f2ed01562');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which model is better?\n",
        "* `test_loss` (lower is better) - ViT\n",
        "* `test_acc` (higher is better) - ViT\n",
        "* `number_of_parameters` (generally lower is better*) - EffNetB2 (if a model has more parameters, it generally takes longer to compute)\n",
        "  * *sometimes models with higher parameters can still perform fast\n",
        "* `model_size (MB)` - EffNetB2 (for our use case of deploying to a mobile device, generally lower is better)\n",
        "* `time_per_pred_cpu` (lower is better, will be highly dependent on the hardware you're running on) - EffNetB2 \n",
        "\n",
        "Both models fail to achieve our goal of 30+FPS... however we could always just try and use EffNetB2 and see how it goes."
      ],
      "metadata": {
        "id": "IJXzZlkQxQyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Wonderful!\n",
        "\n",
        "It seems our models are quite close in terms of overall test accuracy but how do they look across the other fields?</font>\n",
        "\n",
        "<font color=\"purple\">One way to find out would be to divide the <font color=\"red\">**ViT** model</font> statistics by the <font color=\"red\">**EffNetB2** model</font> statistics to find out the different ratios between the models.\n",
        "\n",
        "Let's create another DataFrame to do so.</font>"
      ],
      "metadata": {
        "id": "a8vyEFkEubPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare ViT to EffNetB2 across different characteristics\n",
        "\"\"\"將之前的DataFrame做適當處理\"\"\"\n",
        "pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]),\n",
        "          columns=[\"ViT to EffNetB2 ratios\"]).T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "OJzwJqjLw841",
        "outputId": "fe57d28b-11fa-43bb-da20-f274819019ae"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        test_loss  test_acc  number_of_parameters  \\\n",
              "ViT to EffNetB2 ratios   0.228173  1.016412             11.135432   \n",
              "\n",
              "                        model_size (MB)  time_per_pred_cpu  \n",
              "ViT to EffNetB2 ratios        10.976332            4.46463  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a6ec27e6-3780-436e-a89a-7b6bb1179ce2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_acc</th>\n",
              "      <th>number_of_parameters</th>\n",
              "      <th>model_size (MB)</th>\n",
              "      <th>time_per_pred_cpu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ViT to EffNetB2 ratios</th>\n",
              "      <td>0.228173</td>\n",
              "      <td>1.016412</td>\n",
              "      <td>11.135432</td>\n",
              "      <td>10.976332</td>\n",
              "      <td>4.46463</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6ec27e6-3780-436e-a89a-7b6bb1179ce2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a6ec27e6-3780-436e-a89a-7b6bb1179ce2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a6ec27e6-3780-436e-a89a-7b6bb1179ce2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">It seems our <font color=\"red\">**ViT** model</font> outperforms the <font color=\"red\">**EffNetB2** model</font> across the performance metrics (test loss, where lower is better and test accuracy, where higher is better) but at the expense of having:\n",
        "* 11x+ the number of parameters.\n",
        "* 11x+ the model size. \n",
        "* 2.5x+ the prediction time per image.\n",
        "\n",
        "<font color=\"purple\">Are these tradeoffs worth it?\n",
        "\n",
        "<font color=\"purple\">Perhaps if we had unlimited compute power but for our use case of deploying the FoodVision Mini model to a smaller device (e.g. a mobile phone), we'd likely start out with the <font color=\"red\">**EffNetB2** model</font> for faster predictions at a slightly reduced performance but dramatically smaller "
      ],
      "metadata": {
        "id": "OeRDnKIcur9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Visualizing the speed vs. performance tradeoff\n",
        "\n",
        "So we've compared our EffNetB2 and ViT feature extractor models, now let's visualize the comparison with a speed vs. performance plot.\n",
        "\n",
        "We can do so with matplotlib: \n",
        "1. Create a scatter plot from the comparison DataFrame to compare EffNetB2 and ViT across test accuracy and prediction time.\n",
        "2. Add titles and labels to make our plot look nice.\n",
        "3. Annotate the samples on the scatter plot so we know what's going on. \n",
        "4. Create a legend based on the model sizes (`model_size (MB)`)."
      ],
      "metadata": {
        "id": "6KiF2pRiyxVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've seen that our <font color=\"red\">**ViT** model</font> outperforms our <font color=\"red\">**EffNetB2** mode</font>l in terms of performance metrics such as test loss and test accuracy.\n",
        "\n",
        "However, our <font color=\"red\">**EffNetB2** model</font> makes performs predictions faster and has a much small model size.\n",
        "\n",
        "> <font color=\"purple\">**Note:** Performance or inference time is also often referred to as \"latency\".\n",
        "\n",
        "How about we make this fact visual?</font>\n",
        "\n",
        "<font color=\"purple\">We can do so by creating a plot with matplotlib:\n",
        "1. Create a scatter plot from the comparison DataFrame to compare <font color=\"red\">**EffNetB2**</font> and <font color=\"red\">**ViT**</font> `time_per_pred_cpu` and `test_acc` values.\n",
        "2. Add titles and labels respective of the data and customize the fontsize for aesthetics.\n",
        "3. Annotate the samples on the scatter plot from step 1 with their appropriate labels (the model names).\n",
        "4. Create a legend based on the model sizes (`model_size (MB)`). "
      ],
      "metadata": {
        "id": "yT-e7OoUu2Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "t-MOghSy0hLD",
        "outputId": "7c30e7c6-db5c-48e8-9c17-b2c236a77e43"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   test_loss  test_acc  number_of_parameters  model_size (MB)     model\n",
              "0   0.281287     96.88               7705221        29.824288  EffNetB2\n",
              "1   0.064182     98.47              85800963       327.361283       ViT"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5203b182-b585-4e1e-8440-6d6c6263e26c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test_loss</th>\n",
              "      <th>test_acc</th>\n",
              "      <th>number_of_parameters</th>\n",
              "      <th>model_size (MB)</th>\n",
              "      <th>model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.281287</td>\n",
              "      <td>96.88</td>\n",
              "      <td>7705221</td>\n",
              "      <td>29.824288</td>\n",
              "      <td>EffNetB2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.064182</td>\n",
              "      <td>98.47</td>\n",
              "      <td>85800963</td>\n",
              "      <td>327.361283</td>\n",
              "      <td>ViT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5203b182-b585-4e1e-8440-6d6c6263e26c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5203b182-b585-4e1e-8440-6d6c6263e26c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5203b182-b585-4e1e-8440-6d6c6263e26c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nLzxvWaA14wA",
        "outputId": "3ba1a8e7-8cc1-4482-b8a1-b9f170ac68d3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.7.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a plot from model comparison DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(12, 8)) \n",
        "\"\"\"返回一個figure圖像和子圖ax的array列表\"\"\"\n",
        "\n",
        "scatter = ax.scatter(data=df,\n",
        "            x=\"time_per_pred_cpu\", # 第一組數據 ( x 軸 )\n",
        "            \n",
        "            y=\"test_acc\",     # 第二組數據 ( y 軸 )\n",
        "\n",
        "            c=[\"blue\", \"orange\"], # (what colours to use?) 資料點的顏色\n",
        "\n",
        "            s=\"model_size (MB)\")  # (size the dots by the model sizes) 資料點的尺寸\n",
        "\n",
        "# 2. Add titles and labels to make our plot look good\n",
        "\"\"\"Add titles, labels and customize fontsize for aesthetics\"\"\"\n",
        "ax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18)\n",
        "\"\"\"設定title\"\"\"\n",
        "\n",
        "ax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14)\n",
        "\"\"\"設定x軸標籤\"\"\"\n",
        "\n",
        "ax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\n",
        "\"\"\"設定y軸標籤\"\"\"\n",
        "\n",
        "ax.tick_params(axis=\"both\", labelsize=12)\n",
        "\"\"\"設定軸刻度大小\"\"\"\n",
        "\n",
        "ax.grid(True)\n",
        "\"\"\"設定網格\"\"\"\n",
        "\n",
        "# 3. Annotate the samples on the scatter plot so we know what's going on.\n",
        "\"\"\"Annotate with model names \"\"\"\n",
        "for index, row in df.iterrows(): \n",
        "  \"\"\"df中的兩個rows被一一取出\"\"\"\n",
        "  ax.annotate(text=row[\"model\"], # note: in some versions of Matplotlib, this may need to be \"text\" rather than \"s\" (取出model名稱)\n",
        "        # note: depending on your version of Matplotlib, you may need to use \"s=...\" or \"text=...\", see: https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270       \n",
        "        xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03), # (註釋的x軸與y軸的位置)\n",
        "        size=12)                             # (註釋的字體大小)\n",
        "\n",
        "# 4. Create a legend based on the model sizes (model_size (MB)).\n",
        "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\n",
        "\"\"\"handles是需要顯示在圖例中的繪圖對象句柄(指針)列表,labels是對應的名稱\"\"\"\n",
        "model_size_legend = ax.legend(handles,\n",
        "                 labels,\n",
        "                 loc=\"lower right\",\n",
        "                 title=\"Model size (MB)\",\n",
        "                 fontsize=12)\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig(\"09-foodvision-mini-inference-speed-vs-performance.png\")\n",
        "\n",
        "# # Show the figure\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "tnjz2F77z3In",
        "outputId": "24b88353-b371-4b85-8845-77cdb2167db0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Woah!\n",
        "\n",
        "<font color=\"purple\">The plot really visualizes the **speed vs. performance tradeoff**, in other words, when you have a larger, better performing deep model (like our <font color=\"red\">**ViT** model</font>), it *generally* takes longer to perform inference (higher latency).\n",
        "\n",
        "<font color=\"purple\">There are exceptions to the rule and new research is being published all the time to help make larger models perform faster.\n",
        "\n",
        "And it can be tempting to just deploy the *best* performing model but it's also good to take into considersation where the model is going to be performing.</font>\n",
        "\n",
        "<font color=\"purple\">In our case, the differences between our model's performance levels (on the test loss and test accuracy) aren't too extreme.\n",
        "\n",
        "<font color=\"purple\">But since we'd like to put an emphasis on speed to begin with, we're going to stick with deploying <font color=\"red\">**EffNetB2**</font> since it's faster and has a much smaller footprint.\n",
        "\n",
        "> <font color=\"purple\">**Note:** Prediction times will be different across different hardware types (e.g. Intel i9 vs Google Colab CPU vs GPU) so it's important to think about and test where your model is going to end up. Asking questions like \"where is the model going to be run?\" or \"what is the ideal scenario for running the model?\" and then running experiments to try and provide answers on your way to deployment is very helpful. "
      ],
      "metadata": {
        "id": "Q1tijkh5wzrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Bringing FoodVision Mini to life by creating a Gradio demo\n",
        "\n",
        "We've chosen to deploy EffNetB2 as it fulfils our criteria the best.\n",
        "\n",
        "What is Gradio? \n",
        "\n",
        "> Gradio is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere! https://gradio.app/ \n",
        "\n",
        "For FoodVision Mini, we're going to be working towards building something like this: https://huggingface.co/spaces/mrdbourke/foodvision_mini \n",
        "\n"
      ],
      "metadata": {
        "id": "j_dqjX900zdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've decided we'd like to deploy the <font color=\"red\">**EffNetB2** model</font> (to begin with, this could always be changed later).\n",
        "\n",
        "So how can we do that?</font>\n",
        "\n",
        "<font color=\"purple\">There are several ways to deploy a machine learning model each with specific use cases (as discussed above).\n",
        "\n",
        "We're going to be focused on perhaps the quickest and certainly one of the most fun ways to get a model deployed to the internet.</font>\n",
        "\n",
        "<font color=\"purple\">And that's by using [Gradio](https://gradio.app/).\n",
        "\n",
        "What's Gradio?</font>\n",
        "\n",
        "<font color=\"purple\">The homepage describes it beautifully: \n",
        "\n",
        "> Gradio is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere!</font>\n",
        "\n",
        "<font color=\"purple\">Why create a demo of your models?\n",
        "\n",
        "Because metrics on the test set look nice but you never really know how you're model performs until you use it in the wild.</font>\n",
        "\n",
        "<font color=\"purple\">So let's get deploying!\n",
        "\n",
        "We'll start by importing Gradio with the common alias `gr` and if it's not present, we'll install it.</font>"
      ],
      "metadata": {
        "id": "xIy5w9A6xxJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import/install Gradio \n",
        "try:\n",
        "  import gradio as gr\n",
        "except: \n",
        "  !pip -q install gradio\n",
        "  import gradio as gr\n",
        "    \n",
        "print(f\"Gradio version: {gr.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGMmZINs1GbD",
        "outputId": "91356a41-9309-487b-e4e1-36d90ed2a0db"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.6/136.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Gradio version: 3.32.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Gradio ready!\n",
        "\n",
        "Let's turn FoodVision Mini into a demo application.</font>"
      ],
      "metadata": {
        "id": "Pze_d4b4yG0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Gradio overview\n",
        "\n",
        "Gradio helps you create machine learning demos.\n",
        "\n",
        "Why create a demo? \n",
        "\n",
        "So other people can try our models and we can test them in the real-world.\n",
        "\n",
        "Deployment is as important as training.\n",
        "\n",
        "The overall premise of Gradio is to map inputs -> function/model -> outputs."
      ],
      "metadata": {
        "id": "s5WOjvlY1FGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">The overall premise of Gradio is very similar to what we've been repeating throughout the course.\n",
        "\n",
        "What are our **inputs** and **outputs**?</font>\n",
        "\n",
        "<font color=\"purple\">And how should we get there?\n",
        "\n",
        "Well that's what our machine learning model does.</font>\n",
        "\n",
        "```\n",
        "inputs -> ML model -> outputs\n",
        "```\n",
        "\n",
        "<font color=\"purple\">In our case, for FoodVision Mini, our inputs are images of food, our ML model is EffNetB2 and our outputs are classes of food (pizza, steak or sushi).\n",
        "\n",
        "```\n",
        "images of food -> EffNetB2 -> outputs\n",
        "```\n",
        "\n",
        "Though the concepts of inputs and outputs can be bridged to almost any other kind of ML problem.</font>\n",
        "\n",
        "<font color=\"purple\">Your inputs and outputs might be any combination of the following:\n",
        "* Images\n",
        "* Text\n",
        "* Video\n",
        "* Tabular data\n",
        "* Audio\n",
        "* Numbers\n",
        "* & more</font>\n",
        "\n",
        "<font color=\"purple\">And the ML model you build will depend on your inputs and outputs.\n",
        "\n",
        "Gradio emulates this paradigm by creating an interface ([`gradio.Interface()`](https://gradio.app/docs/#interface-header)) to from inputs to outputs.</font>\n",
        "\n",
        "```\n",
        "gradio.Interface(fn, inputs, outputs)\n",
        "```\n",
        "\n",
        "<font color=\"purple\">Where, `fn` is a Python function to map the `inputs` to the `outputs`.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-gradio-workflow.png\" alt=\"gradio workflow of inputs flowing into some kind of model or function and then producing outputs\" width=900/>\n",
        "\n",
        "*Gradio provides a very helpful `Interface` class to easily create an inputs -> model/function -> outputs workflow where the inputs and outputs could be almost anything you want. For example, you might input Tweets (text) to see if they're about machine learning or not or [input a text prompt to generate images](https://huggingface.co/blog/stable_diffusion).*</font>\n",
        "\n",
        "> <font color=\"purple\">**Note:** Gradio has a vast number of possible `inputs` and `outputs` options known as \"Components\" from images to text to numbers to audio to videos and more. You can see all of these in the [Gradio Components documentation](https://gradio.app/docs/#components).</font>"
      ],
      "metadata": {
        "id": "UlWIz9YpyP2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Creating a function to map our inputs and outputs"
      ],
      "metadata": {
        "id": "1KnalAXU28mL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">To create our FoodVision Mini demo with Gradio, we'll need a function to map our inputs to our outputs.\n",
        "\n",
        "<font color=\"purple\">We created a function earlier called `pred_and_store()` to make predictions with a given model across a list of target files and store them in a list of dictionaries.\n",
        "\n",
        "<font color=\"purple\">How about we create a similar function but this time focusing on a making a prediction on a single image with our <font color=\"red\">**EffNetB2** model</font>?\n",
        "\n",
        "<font color=\"purple\">More specifically, we want a function that takes an image as input, preprocesses (transforms) it, makes a prediction with <font color=\"red\">**EffNetB2**</font> and then returns the prediction (pred or pred label for short) as well as the prediction probability (pred prob).\n",
        "\n",
        "<font color=\"purple\">And while we're here, let's return the time it took to do so too:\n",
        "\n",
        "```\n",
        "input: image -> transform -> predict with EffNetB2 -> output: pred, pred prob, time taken\n",
        "```\n",
        "\n",
        "This will be our `fn` parameter for our Gradio interface.</font>\n",
        "\n",
        "<font color=\"purple\">First, let's make sure our <font color=\"red\">**EffNetB2** model</font> is on the CPU (since we're sticking with CPU-only predictions, however you could change this if you have access to a GPU).</font>"
      ],
      "metadata": {
        "id": "bxi5FtP6yuaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put our model on the CPU\n",
        "effnetb2 = effnetb2.to(\"cpu\")\n",
        "\n",
        "# Check the device \n",
        "next(iter(effnetb2.parameters())).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yYjOWA43UYX",
        "outputId": "97c53c77-ab49-490f-a5c7-854faa453453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a function called `predict()` to go from:\n",
        "\n",
        "```\n",
        "images of food -> ML model (EffNetB2) -> outputs (food class label, prediction time)\n",
        "```"
      ],
      "metadata": {
        "id": "JFRRK6AX3q_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">And now let's create a function called `predict()` to replicate the workflow above."
      ],
      "metadata": {
        "id": "vfSGLXKfzDMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"對照5.1 pred_and_store(paths: ,model: ,transform: ,class_names: ,device: )\"\"\"\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "  \"\"\"\n",
        "  Transforms and performs a prediction on img and returns prediction and time taken.\n",
        "  img : PIL.Image.Image 類型\n",
        "  \"\"\"\n",
        "  # Start a timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # Transform the input image for use with EffNetB2\n",
        "  \"\"\"Transform the target image and add a batch dimension\"\"\"\n",
        "  img = effnetb2_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on 0th index\n",
        "\n",
        "  # Put model into eval mode, make prediction\n",
        "  \"\"\"Put model into evaluation mode and turn on inference mode\"\"\"\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Pass transformed image through the model and turn the prediction logits into probaiblities\n",
        "    \"\"\"Pass the transformed image through the model and turn the prediction logits into prediction probabilities\"\"\"\n",
        "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "  # Create a prediction label and prediction probability dictionary\n",
        "  \"\"\"Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\"\"\"\n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "  # Calculate pred time\n",
        "  \"\"\"Calculate the prediction time\"\"\"\n",
        "  end_time = timer()\n",
        "  pred_time = round(end_time - start_time, 4)\n",
        "\n",
        "  # Return pred dict and pred time\n",
        "  \"\"\"Return the prediction dictionary and prediction time \"\"\"\n",
        "  return pred_labels_and_probs, pred_time\n"
      ],
      "metadata": {
        "id": "XP1g2Qmh3o9r"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beautiful! \n",
        "\n",
        "<font color=\"purple\">Now let's see our function in action by performing a prediction on a random image from the test dataset.\n",
        "\n",
        "We'll start by getting a list of all the image paths from the test directory and then randomly selecting one.</font>\n",
        "\n",
        "<font color=\"purple\">Then we'll open the randomly selected image with [`PIL.Image.open()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#functions).\n",
        "\n",
        "Finally, we'll pass the image to our `predict()` function.</font>"
      ],
      "metadata": {
        "id": "c2dhF2kG0pkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image \n",
        "from pathlib import Path\n",
        "\n",
        "# Get a list of all test image filepaths\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "print(f\"Example test data path: {test_data_paths[0]}\")\n",
        "\n",
        "# Randomly select a test image path \n",
        "random_image_path = random.sample(test_data_paths, k=1)[0] # random.sample會回傳一個list[]\n",
        "random_image_path\n",
        "\n",
        "# Open the target image\n",
        "image = Image.open(random_image_path)\n",
        "print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n",
        "\n",
        "# Predict on the target image and print out the outputs\n",
        "pred_dict, pred_time = predict(img=image)\n",
        "print(pred_dict)  # {'pizza': 0.6918998956680298, 'steak': 0.15962719917297363, 'sushi': 0.14847293496131897}\n",
        "print(pred_time)  # 0.1745"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Eu6e03t5vr_",
        "outputId": "2c1abc7b-3ce6-42ac-84cb-7dd32ad7d48b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example test data path: data/pizza_steak_sushi_20_percent/test/sushi/720302.jpg\n",
            "[INFO] Predicting on image at path: data/pizza_steak_sushi_20_percent/test/steak/86782.jpg\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Nice!\n",
        "\n",
        "Running the cell above a few times we can see different prediction probabilities for each label from our <font color=\"red\">**EffNetB2** model</font> as well as the time it took per prediction.</font>"
      ],
      "metadata": {
        "id": "ea4DRytZ12oC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Creating a list of example images\n",
        "\n",
        "The examples for Gradio can be created with the `examples` parameter, see here: https://gradio.app/docs/#building-demos "
      ],
      "metadata": {
        "id": "WQbfWo2W5wWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Our <font color=\"PEAR\">**`predict()`**</font> function enables us to go from inputs -> transform -> ML model -> outputs.\n",
        "\n",
        "Which is exactly what we need for our Graido demo.</font>\n",
        "\n",
        "<font color=\"purple\">But before we create the demo, let's create one more thing: a list of examples.\n",
        "\n",
        "Gradio's [`Interface`](https://gradio.app/docs/#interface) class takes a list of `examples` of as an optional parameter (`gradio.Interface(examples=List[Any])`).</font>\n",
        "\n",
        "<font color=\"purple\">And the format for the `examples` parameter is a list of lists.\n",
        "\n",
        "So let's create a list of lists containing random filepaths to our test images.</font>\n",
        "\n",
        "<font color=\"purple\">Three examples should be enough."
      ],
      "metadata": {
        "id": "VJ8tDa7C2CTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of example inputs to our Gradio demo\n",
        "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)] \n",
        "\"\"\"type(filepath)是'pathlib.PosixPath'\"\"\"\n",
        "example_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "4HyaNFkd7_Jc",
        "outputId": "5a7f539c-5f3b-4e23-f6d1-8509efc3c03c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-369d258400e8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create list of example inputs to our Gradio demo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexample_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\"\"\"type(filepath)是'pathlib.PosixPath'\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mexample_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font color=\"purple\">Perfect!\n",
        "\n",
        "Our Gradio demo will showcase these as example inputs to our demo so people can try it out and see what it does without uploading any of their own data.</font> "
      ],
      "metadata": {
        "id": "PRmZun0M2S23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4 Building a Gradio Interface\n",
        "\n",
        "Let's use `gr.Interface()` to go from:\n",
        "\n",
        "```\n",
        "input: image -> transform -> predict with EffNetB2 -> output: pred, prob prob, time\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "phwDZEfW8mI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Time to put everything together and bring our FoodVision Mini demo to life!\n",
        "\n",
        "Let's create a Gradio interface to replicate the workflow:</font>\n",
        "\n",
        "```\n",
        "input: image -> transform -> predict with EffNetB2 -> output: pred, pred prob, time taken\n",
        "```\n",
        "\n",
        "<font color=\"purple\">We can do with the [`gradio.Interface()`](https://gradio.app/docs/#interface) class with the following parameters:\n",
        "* <font color=\"#63C5DA\">**`fn`**</font> - a Python function to map `inputs` to `outputs`, in our case, we'll use our <font color=\"PEAR\">**`predict()`**</font> function.\n",
        "* <font color=\"#63C5DA\">**`inputs`**</font> - the input to our interface, such as an image using [`gradio.Image()`](https://gradio.app/docs/#image) or `\"image\"`. \n",
        "* <font color=\"#63C5DA\">**`outputs`**</font> - the output of our interface once the `inputs` have gone through the <font color=\"#63C5DA\">**`fn`**</font>, such as a label using [`gradio.Label()`](https://gradio.app/docs/#label) (for our model's predicted labels) or number using [`gradio.Number()`](https://gradio.app/docs/#number) (for our model's prediction time).</font>\n",
        "    * <font color=\"purple\">**Note:** Gradio comes with many in-built <font color=\"#63C5DA\">**`inputs`**</font> and <font color=\"#63C5DA\">**`outputs`**</font> options known as [\"Components\"](https://gradio.app/docs/#components).\n",
        "* <font color=\"#63C5DA\">**`examples`**</font> - a list of examples to showcase for the demo.\n",
        "* <font color=\"#63C5DA\">**`title`**</font> - a string title of the demo.\n",
        "* <font color=\"#63C5DA\">**`description`**</font> - a string description of the demo.\n",
        "* <font color=\"#63C5DA\">**`article`**</font> - a reference note at the bottom of the demo.</font>\n",
        "\n",
        "<font color=\"purple\">Once we've created our demo instance of `gr.Interface()`, we can bring it to life using [`gradio.Interface().launch()`](https://gradio.app/docs/#launch-header) or <font color=\"#63C5DA\">**`demo.launch()`**</font> command. \n",
        "\n",
        "Easy!</font>"
      ],
      "metadata": {
        "id": "l2U4X1yj2fzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create title, description and article (strings)\n",
        "title = \"FoodVision Mini 🍕🥩🍣\"\n",
        "description = \"An [EfficientNetB2 feature extractor](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.efficientnet_b2) computer vision model to classify images as pizza, steak or sushi.\"\n",
        "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/#74-building-a-gradio-interface).\"\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=predict, # maps inputs to outputs\n",
        "            inputs=gr.Image(type=\"pil\"),                   #( what are the inputs? 類型是PIL.Image)\n",
        "            outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"),  #( what are the outputs? label= component name in interface)  \n",
        "                 gr.Number(label=\"Prediction time (s)\")],  #( our fn has two outputs, therefore we have two outputs)\n",
        "            examples=example_list,\n",
        "            title=title,\n",
        "            description=description,\n",
        "            article=article)\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch(debug=False, # print errors locally?\n",
        "      share=True) # generate a publically shareable URL "
      ],
      "metadata": {
        "id": "WbV3AzwAGRKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/09-gradio-running-in-google-colab-and-in-browser.gif\" alt=\"Gradio demo running in Google Colab and on the web\" width=750/>\n",
        "\n",
        "<font color=\"purple\">*FoodVision Mini Gradio demo running in Google Colab and in the browser (the link when running from Google Colab only lasts for 72 hours). You can see the [permanent live demo on Hugging Face Spaces](https://huggingface.co/spaces/mrdbourke/foodvision_mini).*\n",
        "\n",
        "Woohoo!!! What an epic demo!!!</font>\n",
        "\n",
        "<font color=\"purple\">FoodVision Mini has officially come to life in an interface someone could use and try out.\n",
        "\n",
        "If you set the parameter `share=True` in the `launch()` method, Gradio also provides you with a shareable link such as `https://123XYZ.gradio.app` (this link is an example only and likely expired) which is valid for 72-hours.</font>\n",
        "\n",
        "<font color=\"purple\">The link provides a proxy back to the Gradio interface you launched.\n",
        "\n",
        "For more permanent hosting, you can upload your Gradio app to [Hugging Face Spaces](https://huggingface.co/spaces) or anywhere that runs Python code.</font>"
      ],
      "metadata": {
        "id": "AxCQs1Lq3plB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Turning our FoodVision Mini Gradio Demo into a deployable app\n",
        "\n",
        "Our Gradio demos from Google Colab are fantastic but they expire within 72 hours.\n",
        "\n",
        "To fix this, we're going to prepare our app files so we can host them on Hugging Face Spaces: https://huggingface.co/docs/hub/spaces"
      ],
      "metadata": {
        "id": "gVpShnWV_KVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've seen our FoodVision Mini model come to life through a Gradio demo.\n",
        "\n",
        "But what if we wanted to share it with our friends?</font>\n",
        "\n",
        "<font color=\"purple\">Well, we could use the provided Gradio link, however, the shared link only lasts for 72-hours.\n",
        "\n",
        "To make our FoodVision Mini demo more permanent, we can package it into an app and upload it to [Hugging Face Spaces](https://huggingface.co/spaces/launch).</font>"
      ],
      "metadata": {
        "id": "akhycu4k37d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 What is Hugging Face Spaces? \n",
        "\n",
        "> Hugging Face Spaces offer a simple way to host ML demo apps directly on your profile or your organization’s profile. This allows you to create your ML portfolio, showcase your projects at conferences or to stakeholders, and work collaboratively with other people in the ML ecosystem.\n",
        "\n",
        "If GitHub is a place to show your coding ability, Hugging Face Spaces is a place to show your machine learning ability (through sharing ML demos that you've built).\n",
        "\n"
      ],
      "metadata": {
        "id": "cByikjGwlnSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Hugging Face Spaces is a resource that allows you to host and share machine learning apps.\n",
        "\n",
        "Building a demo is one of the best ways to showcase and test what you've done.</font>\n",
        "\n",
        "<font color=\"purple\">And Spaces allows you to do just that.\n",
        "\n",
        "You can think of Hugging Face as the GitHub of machine learning.</font>\n",
        "\n",
        "<font color=\"purple\">If having a good GitHub portfolio showcases your coding abilities, having a good Hugging Face portfolio can showcase your machine learning abilities.\n",
        "\n",
        "> <font color=\"purple\">**Note:** There are many other places we could upload and host our Gradio app such as, Google Cloud, AWS (Amazon Web Services) or other cloud vendors, however, we're going to use <font color=\"red\">Hugging Face Spaces</font> due to the ease of use and wide adoption by the machine learning community."
      ],
      "metadata": {
        "id": "6rouFVoG4JqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Deployed Gradio app structure\n",
        "\n",
        "Let's start to put all of our app files into a single directory:\n",
        "\n",
        "```\n",
        "Colab -> folder with all Gradio files -> upload app files to Hugging Face Spaces -> deploy\n",
        "```\n",
        "\n",
        "By the end our file structure will look like this: \n",
        "\n",
        "```\n",
        "demos/\n",
        "└── foodvision_mini/\n",
        "    ├── 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
        "    ├── app.py\n",
        "    ├── examples/\n",
        "    │   ├── example_1.jpg\n",
        "    │   ├── example_2.jpg\n",
        "    │   └── example_3.jpg\n",
        "    ├── model.py\n",
        "    └── requirements.txt\n",
        "```\n",
        "\n",
        "Why use this structure?\n",
        "\n",
        "Because it's one of the simplest we could start with.\n",
        "\n",
        "You can see this in action:\n",
        "* Deployed app - https://huggingface.co/spaces/mrdbourke/foodvision_mini\n",
        "* See the example file structure - https://huggingface.co/spaces/mrdbourke/foodvision_mini/tree/main "
      ],
      "metadata": {
        "id": "N7Jatztnlp7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">To upload our demo Gradio app, we'll want to put everything relating to it into a single directory.\n",
        "\n",
        "<font color=\"purple\">For example, our demo might live at the path `demos/foodvision_mini/` with the file structure:\n",
        "\n",
        "```\n",
        "demos/\n",
        "└── foodvision_mini/\n",
        "    ├── 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
        "    ├── app.py\n",
        "    ├── examples/\n",
        "    │   ├── example_1.jpg\n",
        "    │   ├── example_2.jpg\n",
        "    │   └── example_3.jpg\n",
        "    ├── model.py\n",
        "    └── requirements.txt\n",
        "```\n",
        "\n",
        "<font color=\"purple\">Where:\n",
        "* <font color=\"purple\">`09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth` is our trained PyTorch model file.\n",
        "* `app.py` contains our Gradio app (similar to the code that launched the app).\n",
        "    * <font color=\"purple\">**Note:** `app.py` is the default filename used for <font color=\"coral\">Hugging Face Spaces</font>, if you deploy your app there, Spaces will by default look for a file called `app.py` to run. This is changable in settings.\n",
        "* `examples/` contains example images to use with our Gradio app.\n",
        "* `model.py` contains the model defintion as well as any transforms assosciated with the model.\n",
        "* `requirements.txt` contains the dependencies to run our app such as `torch`, `torchvision` and `gradio`.</font>\n",
        "\n",
        "<font color=\"purple\">Why this way?\n",
        "\n",
        "Because it's one of the simplest layouts we could begin with.</font> \n",
        "\n",
        "<font color=\"purple\">Our focus is: *experiment, experiment, experiment!* \n",
        "\n",
        "The quicker we can run smaller experiments, the better our bigger ones will be.</font>\n",
        "\n",
        "<font color=\"purple\">We're going to work towards recreating the structure above but you can see a live demo app running on <font color=\"coral\">Hugging Face Spaces</font> as well as the file structure:\n",
        "* [Live Gradio demo of FoodVision Mini 🍕🥩🍣](https://huggingface.co/spaces/mrdbourke/foodvision_mini).\n",
        "* [FoodVision Mini file structure on Hugging Face Spaces](https://huggingface.co/spaces/mrdbourke/foodvision_mini/tree/main)."
      ],
      "metadata": {
        "id": "qtjVtx7d4-ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Creating a `demos` folder to store our FoodVision app files"
      ],
      "metadata": {
        "id": "zdJRIkLymrao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">To begin, let's first create a `demos/` directory to store all of our FoodVision Mini app files.\n",
        "\n",
        "We can do with Python's [`pathlib.Path(\"path_to_dir\")`](https://docs.python.org/3/library/pathlib.html#basic-use) to establish the directory path and [`pathlib.Path(\"path_to_dir\").mkdir()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir) to create it.</font> "
      ],
      "metadata": {
        "id": "mNEg3EvY5dzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create FoodVision mini demo path\n",
        "foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n",
        "\n",
        "# Remove files that might exist and create a new directory\n",
        "\"\"\"Remove files that might already exist there and create new directory\"\"\"\n",
        "if foodvision_mini_demo_path.exists():\n",
        "  shutil.rmtree(foodvision_mini_demo_path)\n",
        "  foodvision_mini_demo_path.mkdir(parents=True,  # (make the parent folders?)\n",
        "                    exist_ok=True) # (create it even if it already exists?)\n",
        "else:\n",
        "  \"\"\"If the file doesn't exist, create it anyway\"\"\"\n",
        "  foodvision_mini_demo_path.mkdir(parents=True,\n",
        "                    exist_ok=True)\n",
        "\n",
        "# (Check what's in the folder)\n",
        "!ls demos/foodvision_mini/ # 初次建立，應該是空的"
      ],
      "metadata": {
        "id": "EwtkCXuSo6Ac"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4 Creating a folder of example images to use with our FoodVision Mini demo\n",
        "\n",
        "What we want:\n",
        "* 3 images in an `examples/` directory \n",
        "* Images should be from the test set"
      ],
      "metadata": {
        "id": "geu6Ge7PpiJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Now we've got a directory to store our FoodVision Mini demo files, let's add some examples to it.\n",
        "\n",
        "Three example images from the test dataset should be enough.</font>\n",
        "\n",
        "<font color=\"purple\">To do so we'll:\n",
        "1. Create an `examples/` directory within the `demos/foodvision_mini` directory.\n",
        "2. Choose three random images from the test dataset and collect their filepaths in a list.\n",
        "3. Copy the three random images from the test dataset to the `demos/foodvision_mini/examples/` directory.</font>"
      ],
      "metadata": {
        "id": "-SoaLvT36QpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Create an examples directory\n",
        "\"\"\"\n",
        "foodvision_mini_demo_path   是 demos/foodvision_mini/\n",
        "foodvision_mini_examples_path 是 demos/foodvision_mini/examples\n",
        "\"\"\"\n",
        "foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\" \n",
        "foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Collect three random test dataset image paths\n",
        "foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n",
        "                Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n",
        "                Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n",
        " \n",
        "# Copy the three images to the examples directory \n",
        "for example in foodvision_mini_examples:\n",
        "  \"\"\"example是Path物件\"\"\"\n",
        "  destination = foodvision_mini_examples_path / example.name\n",
        "  \"\"\"destination是Path物件, example.name是Path物件內檔案路徑的 檔案名  \"\"\"\n",
        "  print(f\"[INFO] Copying {example} to {destination}\")\n",
        "  shutil.copy2(src=example, dst=destination)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcq5yGKdqGwx",
        "outputId": "f5090602-4cfb-4ffa-ce29-f8de36b2ebf9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Copying data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg to demos/foodvision_mini/examples/592799.jpg\n",
            "[INFO] Copying data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg to demos/foodvision_mini/examples/3622237.jpg\n",
            "[INFO] Copying data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg to demos/foodvision_mini/examples/2582289.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now verify that we can get a list of lists from our `examples/` directory."
      ],
      "metadata": {
        "id": "ew_bFP2Krh87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Now to verify our examples are present, let's list the contents of our `demos/foodvision_mini/examples/` directory with [`os.listdir()`](https://docs.python.org/3/library/os.html#os.listdir) and then format the filepaths into a list of lists (so it's compatible with Gradio's [`gradio.Interface()`](https://gradio.app/docs/#interface) `example` parameter)."
      ],
      "metadata": {
        "id": "TL26XoEU-gIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Get example filepaths in a list of lists\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\n",
        "\"\"\"type(example)是str\"\"\"\n",
        "example_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VxEbm-bqaCa",
        "outputId": "8fcaa7e3-20ba-469e-ca72-51c82ec0dd49"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['examples/2582289.jpg'], ['examples/3622237.jpg'], ['examples/592799.jpg']]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5 Moving our trained EffNetB2 model to our FoodVision Mini demo directory "
      ],
      "metadata": {
        "id": "y3Ujy4l9rmFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We previously (3.5) saved our FoodVision Mini <font color=\"red\">**EffNetB2** feature extractor model</font> under `models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth`.\n",
        "\n",
        "<font color=\"purple\">And rather double up on saved model files, let's move our model to our `demos/foodvision_mini` directory.\n",
        "\n",
        "<font color=\"purple\">We can do so using Python's [`shutil.move()`](https://docs.python.org/3/library/shutil.html#shutil.move) method and passing in `src` (the source path of the target file) and `dst` (the destination path of the target file to be moved to) parameters. "
      ],
      "metadata": {
        "id": "G9YOxjNL-qDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
        "foodvision_mini_demo_path, effnetb2_foodvision_mini_model_path.split(\"/\")[1]"
      ],
      "metadata": {
        "id": "XYOGQzr4j8GA",
        "outputId": "b9c8c56b-75c4-4b19-f35c-176cc484c8e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('demos/foodvision_mini'),\n",
              " '09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create a source path for our target model \n",
        "effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n",
        "\n",
        "# Create a destination path for our target model\n",
        "effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n",
        "\"\"\"\n",
        "foodvision_mini_demo_path              是 demos/foodvision_mini\n",
        "effnetb2_foodvision_mini_model_pathh.split(\"/\")[1] 是 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
        "\"\"\"\n",
        "\n",
        "# Try to move the model file\n",
        "try:\n",
        "  print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n",
        "\n",
        "  # Move the movel\n",
        "  shutil.move(src=effnetb2_foodvision_mini_model_path,\n",
        "        dst=effnetb2_foodvision_mini_model_destination)\n",
        "  \n",
        "  print(f\"[INFO] Model move complete.\")\n",
        "# If the model has already been moved, check if it exists\n",
        "except:\n",
        "  print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n",
        "  print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot32u9VAsZea",
        "outputId": "d096f381-bb38-418f-ea39-4b573de1cefe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Attempting to move models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth to demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n",
            "[INFO] No model found at models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth, perhaps its already been moved?\n",
            "[INFO] Model exists at demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.6 Turning off EffNetB2 model into a Python script (`model.py`)\n",
        "\n",
        "We have a saved `.pth` model `state_dict` and want to load it into a model instance.\n",
        "\n",
        "Let's move our `create_effnetb2_model()` function to a script so we can reuse it."
      ],
      "metadata": {
        "id": "Cm-Qg7QFtO32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Our current model's <font color=\"PEAR\">**`state_dict`**</font> is saved to <font color=\"PEAR\">`demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth`</font>.\n",
        "\n",
        "<font color=\"purple\">To load it in we can use <font color=\"PEAR\">**`model.load_state_dict()`**</font> along with <font color=\"PEAR\">**`torch.load()`**</font>.\n",
        "\n",
        "> <font color=\"purple\">**Note:** For a refresh on saving and loading a model (or a model's <font color=\"PEAR\">**`state_dict`**</font> in PyTorch, see [01. PyTorch Workflow Fundamentals section 5: Saving and loading a PyTorch model](https://www.learnpytorch.io/01_pytorch_workflow/#5-saving-and-loading-a-pytorch-model) or see the PyTorch recipe for [What is a `state_dict` in PyTorch?](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html)\n",
        "\n",
        "<font color=\"purple\">But before we can do this, we first need a way to <font color=\"brown\">instantiate a **`model`**</font>.\n",
        "\n",
        "<font color=\"purple\">To do this in a modular fashion we'll create a script called `model.py` which contains our <font color=\"PEAR\">**`create_effnetb2_model()`**</font> function we created in [section 3.1: *Creating a function to make an EffNetB2 feature extractor*](https://www.learnpytorch.io/09_pytorch_model_deployment/#31-creating-a-function-to-make-an-effnetb2-feature-extractor).\n",
        "\n",
        "<font color=\"purple\">That way we can import the function in *another* script (see `app.py` below) and then use it to create our <font color=\"red\">**EffNetB2** `model`</font> instance as well as get its appropriate transforms.\n",
        "\n",
        "<font color=\"purple\">Just like in [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/), we'll use the `%%writefile path/to/file` magic command to turn a cell of code into a file.</font>"
      ],
      "metadata": {
        "id": "uP3ze-CN_Gge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ins>把3.1節的create_effnetb2_model()複製到model.py</ins>"
      ],
      "metadata": {
        "id": "VWkWehflxmGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/model.py\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "def create_effnetb2_model(num_classes:int=3, # default output classes = 3 (pizza, steak, sushi)\n",
        "              seed:int=42):\n",
        "  \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "      num_classes (int, optional): number of classes in the classifier head. \n",
        "          Defaults to 3.\n",
        "      seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "      model (torch.nn.Module): EffNetB2 feature extractor model. \n",
        "      transforms (torchvision.transforms): EffNetB2 image transforms.\n",
        "  \"\"\"\n",
        "  # 1, 2, 3 Create EffNetB2 pretrained weights, transforms and model\n",
        "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "  # 4. Freeze all layers in the base model\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # 5. Change classifier head with random seed for reproducibility\n",
        "  torch.manual_seed(seed)\n",
        "  model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.3, inplace=True),\n",
        "    nn.Linear(in_features=1408, out_features=num_classes)\n",
        "  )\n",
        "\n",
        "  return model, transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-kPD8bjukR7",
        "outputId": "1f9c9b3c-8d4a-4317-d133-674e2895b94a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_mini/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiukwXZzvvuZ",
        "outputId": "5df04bf7-3771-4caa-8852-b6ec52f7e57d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pizza', 'steak', 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.7 Turning our FoodVision Mini Gradio app into a Python script (`app.py`)\n",
        "\n",
        "The `app.py` file will have four major parts:\n",
        "1. Imports and class names setup\n",
        "2. Model and transforms preparation\n",
        "3. Predict function (`predict()`) \n",
        "4. Gradio app - our Gradio interface + launch command"
      ],
      "metadata": {
        "id": "y1xii5QDu_vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've now got a `model.py` script as well as a path to a saved model <font color=\"PEAR\">**`state_dict`**</font> that we can load in.\n",
        "\n",
        "Time to construct `app.py`.</font>\n",
        "\n",
        "<font color=\"purple\">We call it `app.py` because by default when you create a <font color=\"coral\">**HuggingFace Space**</font>, it looks for a file called `app.py` to run and host (though you can change this in settings).\n",
        "\n",
        "<font color=\"purple\">Our `app.py` script will put together all of the pieces of the puzzle to create our Gradio demo and will have four main parts: \n",
        "\n",
        "1. <font color=\"purple\">(步驟1)**Imports and class names setup** - Here we'll **import** the various dependencies for our demo including the <font color=\"PEAR\">**`create_effnetb2_model()`**</font> function from `model.py` as well as **setup** the different class names for our FoodVision Mini app. \n",
        "2. (步驟2)**Model and transforms preparation** - Here we'll create an <font color=\"red\">**EffNetB2** model</font> instance along with the transforms to go with it and then we'll load in the saved model weights/`state_dict`. When we load the model we'll also set `map_location=torch.device(\"cpu\")` in [`torch.load()`](https://pytorch.org/docs/stable/generated/torch.load.html) so our model gets loaded onto the CPU regardless of the device it trained on (we do this because we won't necessarily have a GPU when we deploy and we'll get an error if our model is trained on GPU but we try to deploy it to CPU without explicitly saying so).\n",
        "3. (步驟3)**Predict function** - Gradio's `gradio.Interface()` takes a `fn` parameter to map inputs to outputs, our `predict()` function will be the same as the one we defined above in [section 7.2: *Creating a function to map our inputs and outputs*](https://www.learnpytorch.io/09_pytorch_model_deployment/#72-creating-a-function-to-map-our-inputs-and-outputs), it will take in an image and then use the loaded transforms to preprocess it before using the loaded model to make a prediction on it.\n",
        "    * <font color=\"purple\">**Note:** We'll have to create the example list on the fly via the `examples` parameter. We can do so by creating a list of the files inside the `examples/` directory with: `[[\"examples/\" + example] for example in os.listdir(\"examples\")]`.\n",
        "4. (步驟4)**Gradio app** - This is where the main logic of our demo will live, we'll create a <font color=\"PEAR\">**`gradio.Interface()`**</font> instance called <font color=\"PEAR\">**`demo`**</font> to put together our inputs, `predict()` function and outputs. And we'll finish the script by calling <font color=\"PEAR\">**`demo.launch()`**</font> to launch our FoodVision Mini demo!"
      ],
      "metadata": {
        "id": "6xSgcSGAEseJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/app.py\n",
        "### 1. Imports and class names setup (步驟1) ###\n",
        "import gradio as gr\n",
        "import os \n",
        "import torch\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Setup class names\n",
        "class_names = ['pizza', 'steak', 'sushi']\n",
        "\n",
        "### 2. Model and transforms perparation (步驟2) ###\n",
        "\"\"\"Create EffNetB2 model: 獲得模型定義與變換\"\"\"\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
        "    num_classes=3) # (len(class_names) would also work)\n",
        "\n",
        "# Load save weights\n",
        "\"\"\"加載權重到模型\"\"\"\n",
        "effnetb2.load_state_dict(\n",
        "  torch.load(\n",
        "    f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n",
        "    map_location=torch.device(\"cpu\") # load the model to the CPU\n",
        "  )\n",
        ")\n",
        "\n",
        "### 3. Predict function (步驟3) ### \n",
        "\"\"\"Create predict function: 建立預測函數 (from 7.2)\"\"\"\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "  # Start a timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # Transform the input image for use with EffNetB2\n",
        "  \"\"\"Transform the target image and add a batch dimension\"\"\"\n",
        "  img = effnetb2_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on 0th index\n",
        "\n",
        "  # Put model into eval mode, make prediction (Put model into evaluation mode and turn on inference mode)\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Pass transformed image through the model and turn the prediction logits into probaiblities\n",
        "    \"\"\"Pass the transformed image through the model and turn the prediction logits into prediction probabilities\"\"\"\n",
        "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "  # Create a prediction label and prediction probability dictionary (for each prediction class (this is the required format for Gradio's output parameter))\n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "  # Calculate pred time (prediction time)\n",
        "  end_time = timer()\n",
        "  pred_time = round(end_time - start_time, 4)\n",
        "\n",
        "  # Return pred dict and pred time (the prediction dictionary and prediction time)\n",
        "  return pred_labels_and_probs, pred_time\n",
        "\n",
        "### 4. Gradio app (步驟4) ### \n",
        "\"\"\"(from 7.4)\"\"\"\n",
        "# Create title, description and article (strings)\n",
        "title = \"FoodVision Mini 🍕🥩🍣\"\n",
        "description = \"An [EfficientNetB2 feature extractor](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.efficientnet_b2) computer vision model to classify images as pizza, steak or sushi.\"\n",
        "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/#74-building-a-gradio-interface).\"\n",
        "\n",
        "# Create example list (from \"examples/\" directory)\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=predict, # maps inputs to outputs #( mapping function from input to output)\n",
        "            inputs=gr.Image(type=\"pil\"), #( what are the inputs?)\n",
        "            outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), #( what are the outputs?)\n",
        "                      gr.Number(label=\"Prediction time (s)\")], #( our fn has two outputs, therefore we have two outputs)\n",
        "            # (Create examples list from \"examples/\" directory)\n",
        "            examples=example_list,\n",
        "            title=title,\n",
        "            description=description,\n",
        "            article=article)\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x8ZsZ-uvoyZ",
        "outputId": "447cf92d-b153-42fc-a022-6bd2839a1d4c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_mini/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">上面app.py的第一行如果被mask後</font>\n",
        "```\n",
        "# %%writefile demos/foodvision_mini/app.py\n",
        "```\n",
        "<font color=\"red\">執行時會出錯於</font>\n",
        "```\n",
        "from model import create_effnetb2_model\n",
        "```\n",
        "<font color=\"red\">此時意味著在root之處,尋找model.py,當然會找不到,因為model.py是在demos/foodvision_mini/</font>  \n",
        "\n"
      ],
      "metadata": {
        "id": "DFydNCCCerZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.8 Creating a requirements file for FoodVision Mini (`requirements.txt`)\n",
        "\n",
        "The requirements file will tell our Hugging Face Space what software dependencies our app requires.\n",
        "\n",
        "The three main ones are:\n",
        "* `torch`\n",
        "* `torchvision`\n",
        "* `gradio` "
      ],
      "metadata": {
        "id": "1if1WABux1R6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">The last file we need to create for our FoodVision Mini app is a [`requirements.txt` file](https://learnpython.com/blog/python-requirements-file/).\n",
        "\n",
        "This will be a text file containing all of the required dependencies for our demo.</font>\n",
        "\n",
        "<font color=\"purple\">When we deploy our demo app to <font color=\"coral\">Hugging Face Spaces</font>, it will search through this file and install the dependencies we define so our app can run.\n",
        "\n",
        "The good news is, there's only three!</font>\n",
        "\n",
        "1. `torch==1.12.0`\n",
        "2. `torchvision==0.13.0`\n",
        "3. `gradio==3.1.4`\n",
        "\n",
        "<font color=\"purple\">The \"`==1.12.0`\" states the version number to install.\n",
        "\n",
        "Defining the version number is not 100% required but we will for now so if any breaking updates occur in future releases, our app still runs (PS if you find any errors, feel free to post on the course [GitHub Issues](https://github.com/mrdbourke/pytorch-deep-learning/issues)).</font>"
      ],
      "metadata": {
        "id": "ZFq04TSfMKy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_mini/requirements.txt\n",
        "torch==1.12.0\n",
        "torchvision==0.13.0\n",
        "gradio==3.1.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1yzFu1hydaK",
        "outputId": "e3e8a9d0-97d9-4405-f804-4ae3f6c98dee"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_mini/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Nice!\n",
        "\n",
        "We've officially got all the files we need to deploy our FoodVision Mini demo!</font>"
      ],
      "metadata": {
        "id": "cuQzUD5YMd22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Deploying our FoodVision Mini app <font color=\"coral\">HuggingFace Spaces</font>\n",
        "\n",
        "There are two main options for uploading to a Hugging Face Space (also called a Hugging Face Repository, similar to a git repository):\n",
        "\n",
        "* Uploading via the Hugging Face Web interface (easiest).\n",
        "* Uploading via the command line or terminal.\n",
        "  * Bonus: You can also use the huggingface_hub library to interact with Hugging Face, this  would be a good extension to the above two options."
      ],
      "metadata": {
        "id": "jilb9gjszIA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've got a file containing our FoodVision Mini demo, now how do we get it to run on <font color=\"coral\">Hugging Face Spaces</font>?\n",
        "\n",
        "<font color=\"purple\">There are two main options for uploading to a <font color=\"coral\">Hugging Face Space</font> (also called a [Hugging Face Repository](https://huggingface.co/docs/hub/repositories-getting-started#getting-started-with-repositories), similar to a git repository): \n",
        "1. [Uploading via the Hugging Face Web interface (easiest)](https://huggingface.co/docs/hub/repositories-getting-started#adding-files-to-a-repository-web-ui).\n",
        "2. [Uploading via the command line or terminal](https://huggingface.co/docs/hub/repositories-getting-started#terminal).\n",
        "    * **Bonus:** You can also use the [`huggingface_hub` library](https://huggingface.co/docs/huggingface_hub/index) to interact with <font color=\"coral\">Hugging Face</font>, this would be a good extension to the above two options.</font>\n",
        "\n",
        "<font color=\"purple\">Feel free to read the documentation on both options but we're going to go with option two.\n",
        "\n",
        "> **Note:** To host anything on Hugging Face, you will to [sign up for a free Hugging Face account](https://huggingface.co/join).</font> "
      ],
      "metadata": {
        "id": "zG-JDJBkMoZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1 Downloading our FoodVision Mini app files\n",
        "\n",
        "We want to download our `foodvision_mini` demo app so we can upload it to Hugging Face Spaces."
      ],
      "metadata": {
        "id": "LNadlbJs0sIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Let's check out the demo files we've got inside `demos/foodvision_mini`.\n",
        "\n",
        "<font color=\"purple\">To do so we can use the `!ls` command followed by the target filepath.\n",
        "\n",
        "`ls` stands for \"list\" and the `!` means we want to execute the command at the shell level.</font>"
      ],
      "metadata": {
        "id": "gfJwjgRANDCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls demos/foodvision_mini/examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKjcf3120OvF",
        "outputId": "adefff37-21ec-4d2e-e0d0-73d03167fe39"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2582289.jpg  3622237.jpg  592799.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">These are all files that we've created!\n",
        "\n",
        "To begin uploading our files to Hugging Face, let's now download them from Google Colab (or wherever you're running this notebook).</font>\n",
        "\n",
        "<font color=\"purple\">To do so, we'll first compress the files into a single zip folder via the command: \n",
        "\n",
        "```\n",
        "zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
        "```\n",
        "\n",
        "Where: \n",
        "* <font color=\"#63C5DA\">**`zip`**</font> stands for \"zip\" as in \"please zip together the files in the following directory\". \n",
        "* <font color=\"#63C5DA\">**`-r`**</font> stands for \"recursive\" as in, \"go through all of the files in the target directory\".\n",
        "* <font color=\"#63C5DA\">**`../foodvision_mini.zip`**</font> is the target directory we'd like our files to be zipped to.\n",
        "* <font color=\"#63C5DA\">**`*`**</font> stands for \"all the files in the current directory\".\n",
        "* <font color=\"#63C5DA\">**`-x`**</font> stands for \"exclude these files\".</font> \n",
        "\n",
        "<font color=\"purple\">We can download our zip file from Google Colab using [`google.colab.files.download(\"demos/foodvision_mini.zip\")`](https://colab.research.google.com/notebooks/io.ipynb) (we'll put this inside a `try` and `except` block just in case we're not running the code inside Google Colab, and if so we'll print a message saying to manually download the files).\n",
        "\n",
        "Let's try it out!</font>"
      ],
      "metadata": {
        "id": "mRAv0ctAOOKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change into the foodvision_mini directory and then zip it from the inside\n",
        "!cd demos/foodvision_mini && zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCNPI3so06_E",
        "outputId": "b483036a-7466-497b-bea8-41d2c1529fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth (deflated 8%)\n",
            "updating: app.py (deflated 54%)\n",
            "updating: examples/ (stored 0%)\n",
            "updating: examples/592799.jpg (deflated 1%)\n",
            "updating: examples/3622237.jpg (deflated 0%)\n",
            "updating: examples/2582289.jpg (deflated 17%)\n",
            "updating: model.py (deflated 46%)\n",
            "updating: requirements.txt (deflated 4%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download\n",
        "\"\"\"Download the zipped FoodVision Mini app (if running in Google Colab)\"\"\"\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download(\"demos/foodvision_mini.zip\")\n",
        "except:\n",
        "  print(f\"Not running in Google Colab, can't use google.colab.files.download(), please download foodvision_mini.zip manually.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "sZthL_n01dBj",
        "outputId": "ccb02622-9844-406a-de18-d71a17550730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4115c36f-3db2-4eed-a1d3-985b969bee91\", \"foodvision_mini.zip\", 28972299)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Woohoo!\n",
        "\n",
        "Looks like our <font color=\"#63C5DA\">**`zip`**</font> command was successful.</font>\n",
        "\n",
        "<font color=\"purple\">If you're running this notebook in Google Colab, you should see a file start to download in your browser.\n",
        "\n",
        "Otherwise, you can see the `foodvision_mini.zip` folder (and more) on the [course GitHub under the `demos/` directory](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/demos).</font> "
      ],
      "metadata": {
        "id": "Eldbm2W6OzYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2 Running our Gradio demo app locally <font color=\"red\">(略過)\n",
        "\n",
        "\n",
        "Running the app locally - https://www.learnpytorch.io/09_pytorch_model_deployment/#92-running-our-foodvision-mini-demo-locally \n",
        "\n"
      ],
      "metadata": {
        "id": "96nmMyNm1d23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">If you download the `foodvision_mini.zip` file, you can test it locally by:\n",
        "1. Unzipping the file.\n",
        "2. Opening terminal or a command line prompt.\n",
        "3. Changing into the `foodvision_mini` directory (<font color=\"#63C5DA\">**`cd foodvision_mini`**</font>).\n",
        "4. Creating an environment (<font color=\"#63C5DA\">**`python3 -m venv env`**</font>).\n",
        "5. Activating the environment (<font color=\"#63C5DA\">**`source env/bin/activate`**</font>).\n",
        "5. Installing the requirements (<font color=\"#63C5DA\">**`pip install -r requirements.txt`**</font>, the \"<font color=\"#63C5DA\">**`-r`**</font>\" is for recursive).\n",
        "    * <font color=\"purple\">**Note:** This step may take 5-10 minutes depending on your internet connection. And if you're facing errors, you may need to upgrade <font color=\"#63C5DA\">**`pip`**</font> first: <font color=\"#63C5DA\">**`pip install --upgrade pip`**</font>.\n",
        "6. Run the app (<font color=\"#63C5DA\">**`python3 app.py`**</font>).</font>\n",
        "\n",
        "<font color=\"purple\">This should result in a Gradio demo just like the one we built above running locally on your machine at a URL such as `http://127.0.0.1:7860/`.\n",
        "\n",
        "> <font color=\"purple\">**Note:** If you run the app locally and you notice a `flagged/` directory appear, it contains samples that have been \"flagged\". \n",
        ">\n",
        "> <font color=\"purple\">For example, if someone tries the demo and the model produces an incorrect result, the sample can be \"flagged\" and reviewed for later.\n",
        "> \n",
        "> For more on flagging in Gradio, see the [flagging documentation](https://gradio.app/docs/#flagging).</font>"
      ],
      "metadata": {
        "id": "xXrygndbPTxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3 Uploading our FoodVision Mini Gradio demo to <font color=\"coral\">Hugging Face Spaces</font>\n",
        "\n",
        "See the steps here - https://www.learnpytorch.io/09_pytorch_model_deployment/#93-uploading-to-hugging-face \n",
        "\n",
        "See the live app deployed here - https://huggingface.co/spaces/mrdbourke/foodvision_mini_video "
      ],
      "metadata": {
        "id": "p1ALcikm45es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've verfied our FoodVision Mini app works locally, however, the fun of creating a machine learning demo is to show it to other people and allow them to use it.\n",
        "\n",
        "<font color=\"purple\">To do so, we're going to upload our FoodVision Mini demo to <font color=\"coral\">Hugging Face</font>.\n",
        "\n",
        "> <font color=\"purple\">**Note:** The following series of steps uses a Git (a file tracking system) workflow. For more on how Git works, I'd recommend going through the [Git and GitHub for Beginners tutorial](https://youtu.be/RGOj5yH7evk) on freeCodeCamp.\n",
        "\n",
        "1. [Sign up](https://huggingface.co/join) for a <font color=\"coral\">Hugging Face</font> account. \n",
        "2. Start a new <font color=\"coral\">Hugging Face Space</font> by going to your profile and then [clicking \"New Space\"](https://huggingface.co/new-space).\n",
        "    * <font color=\"purple\">**Note:** A <font color=\"coral\">Space in Hugging Face</font> is also known as a \"code repository\" (a place to store your code/files) or \"repo\" for short.\n",
        "3. Give the Space a name, for example, mine is called `mrdbourke/foodvision_mini`, you can see it here: https://huggingface.co/spaces/mrdbourke/foodvision_mini\n",
        "4. Select a license (I used [MIT](https://opensource.org/licenses/MIT)).\n",
        "5. Select Gradio as the Space SDK (software development kit). \n",
        "   * <font color=\"purple\">**Note:** You can use other options such as Streamlit but since our app is built with Gradio, we'll stick with that.\n",
        "6. Choose whether your Space is it's public or private (I selected public since I'd like my Space to be available to others).\n",
        "7. Click \"Create Space\".\n",
        "8. Clone the repo locally by running something like: <font color=\"#63C5DA\">**`git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]`**</font> in terminal or command prompt.\n",
        "    * <font color=\"purple\">**Note:** You can also add files via uploading them under the \"Files and versions\" tab.\n",
        "9. Copy/move the contents of the downloaded `foodvision_mini` folder to the cloned repo folder.\n",
        "10. To upload and track larger files (e.g. files over 10MB or in our case, our PyTorch model file) you'll need to [install Git LFS](https://git-lfs.github.com/) (which stands for \"git large file storage\").\n",
        "11. After you've installed <font color=\"red\">**Git LFS**</font>, you can activate it by running <font color=\"#63C5DA\">**`git lfs install`**</font>.\n",
        "12. In the `foodvision_mini` directory, track the files over 10MB with <font color=\"red\">**Git LFS**</font> with <font color=\"#63C5DA\">**`git lfs track \"*.file_extension\"`**</font>.\n",
        "    * <font color=\"purple\">Track <font color=\"red\">**EffNetB2** PyTorch model file</font> with <font color=\"#63C5DA\">**`git lfs track \"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"`**</font>.\n",
        "13. Track `.gitattributes` (automatically created when cloning from <font color=\"coral\">HuggingFace</font>, this file will help ensure our larger files are tracked with <font color=\"red\">**Git LFS**</font>). You can see an example `.gitattributes` file on the [FoodVision Mini Hugging Face Space](https://huggingface.co/spaces/mrdbourke/foodvision_mini/blob/main/.gitattributes).\n",
        "    * <font color=\"#63C5DA\">**`git add .gitattributes`**</font>\n",
        "14. Add the rest of the `foodvision_mini` app files and commit them with: \n",
        "    * <font color=\"#63C5DA\">**`git add *`**</font>\n",
        "    * <font color=\"#63C5DA\">**`git commit -m \"first commit\"`**</font>\n",
        "15. Push (upload) the files to Hugging Face:\n",
        "    * <font color=\"#63C5DA\">**`git push`**</font>\n",
        "16. Wait 3-5 minutes for the build to happen (future builds are faster) and your app to become live!\n",
        "\n",
        "<font color=\"purple\">If everything worked, you should see a live running example of our FoodVision Mini Gradio demo like the one here: https://huggingface.co/spaces/mrdbourke/foodvision_mini \n",
        "\n",
        "And we can even embed our FoodVision Mini Gradio demo into our notebook as an [iframe](https://gradio.app/sharing_your_app/#embedding-with-iframes) with [`IPython.display.IFrame`](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.IFrame) and a link to our space in the format `https://hf.space/embed/[YOUR_USERNAME]/[YOUR_SPACE_NAME]/+`.</font>"
      ],
      "metadata": {
        "id": "EGZq1iBiQOrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also share our app by embedding it: https://gradio.app/sharing_your_app/#embedding-hosted-spaces"
      ],
      "metadata": {
        "id": "Jv7eB8Sm-B81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IPython is a library to help make Python interactive\n",
        "from IPython.display import IFrame\n",
        "\n",
        "# Embed FoodVision Mini Gradio demo\n",
        "IFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_mini_video/+\", width=900, height=750)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "oq1ozup75H1T",
        "outputId": "69c9bc16-0cf1-45ea-9ec4-6d24d823e2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f0a592721d0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"750\"\n",
              "            src=\"https://hf.space/embed/mrdbourke/foodvision_mini_video/+\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Creating FoodVision Big!!!\n",
        "\n",
        "FoodVision Mini works well with 3 classes (pizza, steak, sushi). \n",
        "\n",
        "So all of experimenting is paying off...\n",
        "\n",
        "Let's step things up a notch and make FoodVision BIG!!! using all of the Food101 classes. "
      ],
      "metadata": {
        "id": "m41X7TCd6_cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've spent the past few sections and chapters working on bringing FoodVision Mini to life.\n",
        "\n",
        "And now we've seen it working in a live demo, how about we step things up a notch?</font>\n",
        "\n",
        "<font color=\"purple\">How?\n",
        "\n",
        "FoodVision Big!</font>\n",
        "\n",
        "<font color=\"purple\">Since FoodVision Mini is trained on pizza, steak and sushi images from the [Food101 dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html) (101 classes of food x 1000 images each), how about we make FoodVision Big by training a model on all 101 classes!\n",
        "\n",
        "We'll go from three classes to 101!</font>\n",
        "\n",
        "<font color=\"purple\">From pizza, steak, sushi to pizza, steak, sushi, hot dog, apple pie, carrot cake, chocolate cake, french fires, garlic bread, ramen, nachos, tacos and more!\n",
        "\n",
        "How?</font>\n",
        "\n",
        "<font color=\"purple\">Well, we've got all the steps in place, all we have to do is alter our <font color=\"red\">**EffNetB2** model</font> slightly as well as prepare a different dataset.\n",
        "\n",
        "To finish Milestone Project 3, let's recreate a Gradio demo similar to FoodVision Mini (three classes) but for FoodVision Big (101 classes).</font>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-model-deployment-foodvision-mini-to-foodvision-big.png\" alt=\"foodvision mini model on three classes: pizza, steak, sushi and foodvision big on all of the 101 classes in the food101 dataset\" width=900/>\n",
        "\n",
        "<font color=\"purple\">*FoodVision Mini works with three food classes: pizza, steak and sushi. And FoodVision Big steps it up a notch to work across 101 food classes: all of the [classes in the Food101 dataset](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).*</font>"
      ],
      "metadata": {
        "id": "_g49XqN3RGUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.1 Creating a model for FoodVision Big + transforms"
      ],
      "metadata": {
        "id": "dWAgWBXr812g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">When creating FoodVision Mini we saw that the <font color=\"red\">**EffNetB2** model</font> was a good tradeoff between speed and performance (it performed well with a fast speed).\n",
        "\n",
        "<font color=\"purple\">So we'll continue using the same model for FoodVision Big.\n",
        "\n",
        "<font color=\"purple\">We can create an <font color=\"red\">**EffNetB2** feature extractor</font> for Food101 by using our <font color=\"PEAR\">**`create_effnetb2_model()`**</font> function we created above, in [section 3.1](https://www.learnpytorch.io/09_pytorch_model_deployment/#31-creating-a-function-to-make-an-effnetb2-feature-extractor), and passing it the parameter `num_classes=101` (since Food101 has 101 classes).</font>"
      ],
      "metadata": {
        "id": "BMAtyqt1X0UK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Food101 model and transforms\n",
        "\"\"\"調用create_effnetb2_model建立 effnetb2_food101模型 與 effnetb2_transforms變換 (來自3.1)\"\"\"\n",
        "effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)"
      ],
      "metadata": {
        "id": "zNvhScO59IR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Beautiful!\n",
        "\n",
        "Let's now get a summary of our model.</font>"
      ],
      "metadata": {
        "id": "3Kk9tzKGYKcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Print EffNetB2 model summary (uncomment for full output) \n",
        "summary(effnetb2_food101, \n",
        "    input_size=(1, 3, 224, 224),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_vqDRO69seT",
        "outputId": "f0a1d543-65ca-442c-a920-2f74b09dfcfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 101]             --                   Partial\n",
              "├─Sequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    (864)                False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    (64)                 False\n",
              "│    │    └─SiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
              "│    └─Sequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    (1,448)              False\n",
              "│    │    └─MBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    (612)                False\n",
              "│    └─Sequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      (6,004)              False\n",
              "│    │    └─MBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    │    └─MBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    └─Sequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      (16,518)             False\n",
              "│    │    └─MBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    │    └─MBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    └─Sequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      (50,300)             False\n",
              "│    │    └─MBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    └─Sequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     (149,158)            False\n",
              "│    │    └─MBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    └─Sequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       (301,406)            False\n",
              "│    │    └─MBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    └─Sequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       (846,900)            False\n",
              "│    │    └─MBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       (1,888,920)          False\n",
              "│    └─Conv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      (495,616)            False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      (2,816)              False\n",
              "│    │    └─SiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
              "├─Sequential (classifier)                                    [1, 1408]            [1, 101]             --                   True\n",
              "│    └─Dropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
              "│    └─Linear (1)                                            [1, 1408]            [1, 101]             142,309              True\n",
              "============================================================================================================================================\n",
              "Total params: 7,843,303\n",
              "Trainable params: 142,309\n",
              "Non-trainable params: 7,700,994\n",
              "Total mult-adds (M): 657.78\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 156.80\n",
              "Params size (MB): 31.37\n",
              "Estimated Total Size (MB): 188.77\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/09-effnetb2-feature-extractor-101-classes.png\" width=900 alt=\"effnetb2 feature extractor with 100 output classes model summary\"/>\n",
        "                                                                                                                                                      \n",
        "<font color=\"purple\">Nice!\n",
        "\n",
        "See how just like our <font color=\"red\">**EffNetB2** model</font> for FoodVision Mini the base layers are frozen (these are pretrained on ImageNet) and the outer layers (the `classifier` layers) are trainble with an ouput shape of `[batch_size, 101]` (`101` for 101 classes in Food101).</font> \n",
        "\n",
        "<font color=\"purple\">Now since we're going to be dealing with a fair bit more data than usual, how about we add a little data augmentation to our transforms (`effnetb2_transforms`) to augment the training data.\n",
        "\n",
        "> <font color=\"purple\">**Note:** Data augmentation is a technique used to alter the appearance of an input training sample (e.g. rotating an image or slightly skewing it) to artificially increase the diversity of a training dataset to hopefully prevent overfitting. You can see more on data augmentation in [04. PyTorch Custom Datasets section 6](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation).\n",
        "\n",
        "<font color=\"purple\">Let's compose a `torchvision.transforms` pipeline to use [`torchvision.transforms.TrivialAugmentWide()`](https://pytorch.org/vision/main/generated/torchvision.transforms.TrivialAugmentWide.html) (the same data augmentation used by the PyTorch team in their [computer vision recipes](https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/#break-down-of-key-accuracy-improvements)) as well as the `effnetb2_transforms` to transform our training images. "
      ],
      "metadata": {
        "id": "SR53HPd0YdnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're working with a larger dataset, we may want to introduce some data augmentation techniques: \n",
        "* This is because with larger datasets and larger models, overfitting becomes more of a problem. \n",
        "* Because we're working with a large number of classes, let's use TrivialAugment as our data augmentation technique.\n",
        "\n",
        "For a list of state-of-the-art computer vision recipes: https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/ "
      ],
      "metadata": {
        "id": "VPWD94rM-wfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training data transforms\n",
        "\"\"\"Create Food101 training data transforms (only perform data augmentation on the training images)\"\"\"\n",
        "food101_train_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.TrivialAugmentWide(),\n",
        "    effnetb2_transforms])    \n",
        "\n",
        "food101_train_transforms                          "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "ZE7_bIz599tF",
        "outputId": "bbfbf944-bd91-49d3-8c42-2a6a6daa7bb1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0aa0670eb9b7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create training data transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\"\"\"Create Food101 training data transforms (only perform data augmentation on the training images)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m food101_train_transforms = torchvision.transforms.Compose([\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrivialAugmentWide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     effnetb2_transforms])    \n",
            "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Epic!\n",
        "\n",
        "Now let's compare `food101_train_transforms` (for the training data) and `effnetb2_transforms` (for the testing/inference data).</font> "
      ],
      "metadata": {
        "id": "_TBbU-bEY8Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing data transform\n",
        "effnetb2_transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNsAgHj6-EOC",
        "outputId": "24b775e8-5861-4d2b-e291-6bf31e9b9096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[288]\n",
              "    resize_size=[288]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BICUBIC\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training transforms:\\n{food101_train_transforms}\\n\") \n",
        "print(f\"Testing transforms:\\n{effnetb2_transforms}\")"
      ],
      "metadata": {
        "id": "dqAnX8ZmbQ6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2 Getting data for FoodVision Big\n",
        "\n",
        "Get Food101 dataset - https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html "
      ],
      "metadata": {
        "id": "_g5Ctywd_VpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">For FoodVision Mini, we made our own [custom data splits](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb) of the entire Food101 dataset.\n",
        "\n",
        "<font color=\"purple\">To get the whole Food101 dataset, we can use [`torchvision.datasets.Food101()`](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html).\n",
        "\n",
        "<font color=\"purple\">We'll first setup a path to directory `data/` to store the images. \n",
        "\n",
        "<font color=\"purple\">Then we'll download and transform the training and testing dataset splits using `food101_train_transforms` and `effnetb2_transforms` to transform each dataset respectively. \n",
        "\n",
        "> <font color=\"purple\">**Note:** If you're using Google Colab, the cell below will take ~3-5 minutes to fully run and download the Food101 images from PyTorch. \n",
        ">\n",
        "> <font color=\"purple\">This is because there is over 100,000 images being downloaded (101 classes x 1000 images per class). If you restart your Google Colab runtime and come back to this cell, the images will have to redownload. Alternatively, if you're running this notebook locally, the images will be cached and stored in the directory specified by the `root` parameter of `torchvision.datasets.Food101()`."
      ],
      "metadata": {
        "id": "fv-KRdiEbWDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "# Setup data directory\n",
        "from pathlib import Path\n",
        "data_dir = Path(\"data\")\n",
        "\n",
        "# Get the training data (~750 images x 101 classes)\n",
        "train_data = datasets.Food101(root=data_dir, # (path to download data to)\n",
        "                split=\"train\",  # (dataset split to get)\n",
        "                transform=food101_train_transforms, # apply data augmentation to training data ( perform data augmentation on training data)\n",
        "                download=True)  # (want to download?)\n",
        "\n",
        "# Get the testing data (~250 images x 101 classes)\n",
        "test_data = datasets.Food101(root=data_dir,\n",
        "                split=\"test\",\n",
        "                transform=effnetb2_transforms, # don't perform data augmentation on the test data ( perform normal EffNetB2 transforms on test data)\n",
        "                download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "T0zcPt91_p1p",
        "outputId": "2064b521-943b-4168-8f9f-ddaccc5fdc39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-96041e42f833>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m train_data = datasets.Food101(root=data_dir, # (path to download data to)\n\u001b[1;32m      9\u001b[0m                 \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# (dataset split to get)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfood101_train_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# apply data augmentation to training data ( perform data augmentation on training data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 download=True)  # (want to download?)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'food101_train_transforms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Data downloaded!\n",
        "\n",
        "Now we can get a list of all the class names using `train_data.classes`.</font>"
      ],
      "metadata": {
        "id": "IRPO37T_iyYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "750 * 101, 250 * 101"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxsMRAGBAMGH",
        "outputId": "f4335d78-84fc-4f9a-fdeb-9ea43cf706dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75750, 25250)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Food101 class names \n",
        "food101_class_names = train_data.classes\n",
        "\n",
        "# View the first 10\n",
        "food101_class_names[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdUvebbJAM6T",
        "outputId": "746e92e8-ea20-49c5-e82f-6ee939128bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apple_pie',\n",
              " 'baby_back_ribs',\n",
              " 'baklava',\n",
              " 'beef_carpaccio',\n",
              " 'beef_tartare',\n",
              " 'beet_salad',\n",
              " 'beignets',\n",
              " 'bibimbap',\n",
              " 'bread_pudding',\n",
              " 'breakfast_burrito']"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Ho ho! Those are some delicious sounding foods (although I've never heard of \"beignets\"... update: after a quick Google search, beignets also look delicious). \n",
        "                                                \n",
        "<font color=\"purple\">You can see a full list of the Food101 class names on the course GitHub under [`extras/food101_class_names.txt`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt)."
      ],
      "metadata": {
        "id": "CTjfqhZki4NH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3 Creating a subset of the Food101 dataset for faster experimenting\n",
        "\n",
        "Why create a subset?\n",
        "\n",
        "We want our first few experiments to run as quick as possible.\n",
        "\n",
        "We know FoodVision Mini works pretty well but this the is first time we've upgraded to 101 classes.\n",
        "\n",
        "To do so, let's make a subset of 20% of the data from the Food101 dataset (training and test).\n",
        "\n",
        "Our short-term goal: to beat the original Food101 paper result of 56.40% accuracy on the test dataset (see the paper: https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf)\n",
        "\n",
        "We want to beat this result using modern deep learning techniques and only 20% of the data. "
      ],
      "metadata": {
        "id": "6WiVb1a3C10F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">This is optional.\n",
        "\n",
        "We don't *need* to create another subset of the Food101 dataset, we could train and evaluate a model across the whole 101,000 images.</font>\n",
        "\n",
        "<font color=\"purple\">But to keep training fast, let's create a 20% split of the training and test datasets.\n",
        "\n",
        "Our goal will be to see if we can beat the original [Food101 paper's](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/) best results with only 20% of the data.</font>\n",
        "\n",
        "<font color=\"purple\">To breakdown the datasets we've used/will use:\n",
        "\n",
        "| **Notebook(s)** | **Project name** | **Dataset** | **Number of classes** | **Training images** | **Testing images** | \n",
        "| ----- | ----- | ----- | ----- | ----- | ----- |\n",
        "| 04, 05, 06, 07, 08 | FoodVision Mini (10% data) | Food101 custom split | 3 (pizza, steak, sushi) | 225 | 75 | \n",
        "| 07, 08, 09 | FoodVision Mini (20% data) | Food101 custom split | 3 (pizza, steak, sushi) | 450 | 150 |\n",
        "| **09 (this one)** | FoodVision Big (20% data) | Food101 custom split | 101 (all Food101 classes) | 15150 | 5050 | \n",
        "| Extension | FoodVision Big | Food101 all data | 101 | 75750 | 25250 | \n",
        "\n",
        "<font color=\"purple\">Can you see the trend? \n",
        "\n",
        "Just like our model size slowly increased overtime, so has the size of the dataset we've been using for experiments.</font>\n",
        "\n",
        "> <font color=\"purple\">**Note:** To truly beat the original Food101 paper's results with 20% of the data, we'd have to train a model on 20% of the training data and then evaluate our model on the *whole* test set rather than the split we created. I'll leave this as an extension exercise for you to try. I'd also encourage you to try training a model on the entire Food101 training dataset.\n",
        "\n",
        "To make our FoodVision Big (20% data) split, let's create a function called `split_dataset()` to split a given dataset into certain proportions.</font>\n",
        "\n",
        "<font color=\"purple\">We can use [`torch.utils.data.random_split()`](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) to create splits of given sizes using the `lengths` parameter. \n",
        "\n",
        "The `lengths` parameter accepts a list of desired split lengths where the total of the list must equal the overall length of the dataset.</font>\n",
        "\n",
        "<font color=\"purple\">For example, with a dataset of size 100, you could pass in `lengths=[20, 80]` to receive a 20% and 80% split.\n",
        "\n",
        "We'll want our function to return two splits, one with the target length (e.g. 20% of the training data) and the other with the remaining length (e.g. the remaining 80% of the training data).</font>\n",
        "\n",
        "<font color=\"purple\">Finally, we'll set `generator` parameter to a `torch.manual_seed()` value for reproducibility."
      ],
      "metadata": {
        "id": "V0W70o5Pjl8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<font color=\"red\">掛載google drive</font>\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```\n",
        "\n",
        "<font color=\"red\">卸載google drive</font>\n",
        "```\n",
        "drive.flush_and_unmount()\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hoMEmZP5qrgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data) * 0.2, len(test_data) * 0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXmjLXeuFIzS",
        "outputId": "a597987c-f9e7-49fb-d9bf-87e44b7e012a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15150.0, 5050.0)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split # https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split \n",
        "\n",
        "def split_dataset(dataset:torchvision.datasets,\n",
        "          split_size:float=0.2,\n",
        "          seed:int=42):\n",
        "  \"\"\"Randomly splits a given dataset into two proportions based on split_size and seed.\n",
        "\n",
        "    Args:\n",
        "      dataset (torchvision.datasets): A PyTorch Dataset, typically one from torchvision.datasets.\n",
        "      split_size (float, optional): How much of the dataset should be split? \n",
        "          E.g. split_size=0.2 means there will be a 20% split and an 80% split. Defaults to 0.2.\n",
        "      seed (int, optional): Seed for random generator. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "      tuple: (random_split_1, random_split_2) where random_split_1 is of size split_size*len(dataset) and \n",
        "          random_split_2 is of size (1-split_size)*len(dataset).\n",
        "  \"\"\"\n",
        "  # Create split lengths based on original dataset length\n",
        "  length_1 = int(len(dataset) * split_size) # defaults to 20% data split (# desired length)\n",
        "  length_2 = len(dataset) - length_1 # remaining length (# remaining length)\n",
        "\n",
        "  # Print out info\n",
        "  print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} and {length_2}\")\n",
        "  \n",
        "  # Create splits with given random seed\n",
        "  random_split_1, random_split_2 = torch.utils.data.random_split(dataset,\n",
        "                                     lengths=[length_1, length_2],\n",
        "                                     generator=torch.manual_seed(seed)) # (set the random seed for reproducible splits)\n",
        "  \n",
        "  return random_split_1, random_split_2"
      ],
      "metadata": {
        "id": "DZbT-mGSEEGG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "774ef071-df08-434c-e801-704118073679"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f3f07289bcee>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_split\u001b[0m \u001b[0;31m# https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m def split_dataset(dataset:torchvision.datasets,\n\u001b[0m\u001b[1;32m      4\u001b[0m           \u001b[0msplit_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           seed:int=42):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Dataset split function created!\n",
        "\n",
        "Now let's test it out by creating a 20% training and testing dataset split of Food101.</font>"
      ],
      "metadata": {
        "id": "D_o6Cknykhu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training 20% split Food101 (Create training 20% split of Food101)\n",
        "train_data_food101_20_percent, _ = split_dataset(dataset=train_data,\n",
        "                            split_size=0.2)\n",
        "\n",
        "# Create testing 20% split Food101 (Create testing 20% split of Food101)\n",
        "test_data_food101_20_percent, _ = split_dataset(dataset=test_data,\n",
        "                            split_size=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rnr-VGlNF03o",
        "outputId": "f27e4529-a22f-4b56-ea2d-1a09dc93f5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Splitting dataset of length 75750 into splits of size: 15150 and 60600\n",
            "[INFO] Splitting dataset of length 25250 into splits of size: 5050 and 20200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data_food101_20_percent), len(test_data_food101_20_percent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmIIuYj2GN2i",
        "outputId": "71adf2ef-f603-4ae9-ba1b-7908a7aa861a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15150, 5050)"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Excellent!"
      ],
      "metadata": {
        "id": "UU7fCWlQk49h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.4 Turning our Food101 datasets into `DataLoader`s"
      ],
      "metadata": {
        "id": "Jl7899MqGSFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Now let's turn our Food101 20% dataset splits into `DataLoader`'s using `torch.utils.data.DataLoader()`.\n",
        "\n",
        "<font color=\"purple\">We'll set `shuffle=True` for the training data only and the batch size to `32` for both datasets.\n",
        "\n",
        "<font color=\"purple\">And we'll set `num_workers` to `4` if the CPU count is available or `2` if it's not (though the value of `num_workers` is very experimental and will depend on the hardware you're using, there's an [active discussion thread about this on the PyTorch forums](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813))."
      ],
      "metadata": {
        "id": "OqVUep32lAs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.cpu_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shvR7ZPlHa-c",
        "outputId": "10d81b7c-36d6-43f0-d270-d22d5af3639d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "NUM_WORKERS = 2 # this value is very experimental and the best value will differ depeneding on the hardware you're using, search \"pytorch num workers setting for more\"\n",
        "BATCH_SIZE = 32\n",
        "# NUM_WORKERS = 2 if os.cpu_count() <= 4 else 4 # this value is very experimental and will depend on the hardware you have available, Google Colab generally provides 2x CPUs\n",
        "\n",
        "\n",
        "# Create Food101 20% training DataLoader\n",
        "train_dataloader_food101_20_percent = torch.utils.data.DataLoader(dataset=train_data_food101_20_percent,\n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    shuffle=True,\n",
        "                                    num_workers=NUM_WORKERS)\n",
        "\n",
        "# Create Food101 20% testing DataLoader\n",
        "test_dataloader_food101_20_percent = torch.utils.data.DataLoader(dataset=test_data_food101_20_percent,\n",
        "                                    batch_size=BATCH_SIZE,\n",
        "                                    shuffle=False,\n",
        "                                    num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "0I5ReLXgGbdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader_food101_20_percent), len(test_dataloader_food101_20_percent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJcjlCKiIQp6",
        "outputId": "d0786505-96c5-491f-ac4e-4f35e86cf680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(474, 158)"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5 Training FoodVision Big!!!!\n",
        "\n",
        "Things for training:\n",
        "* 5 epochs\n",
        "* Optimizer: `torch.optim.Adam(lr=1e-3)`\n",
        "* Loss function: `torch.nn.CrossEntropyLoss(label_smoothing=0.1)`\n",
        "\n",
        "Why use label smoothing? \n",
        "\n",
        "Label smoothing helps to prevent overfitting (it's a regularization technique).\n",
        "\n",
        "Without label smoothing and 5 classes: \n",
        "\n",
        "```\n",
        "[0.00, 0.00, 0.99, 0.01, 0.00]\n",
        "```\n",
        "\n",
        "With label smoothing and 5 classes:\n",
        "\n",
        "```\n",
        "[0.01, 0.01, 0.96, 0.01, 0.01]\n",
        "```\n",
        "\n",
        "> **Note:** Depending on your hardware, running the following cell may take 15-20 minutes (takes about 17 minutes on a NVIDIA Tesla P100 GPU)."
      ],
      "metadata": {
        "id": "L6_bOhjTIT5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">FoodVision Big model and `DataLoader`s ready!\n",
        "\n",
        "<font color=\"purple\">Time for training.\n",
        "\n",
        "<font color=\"purple\">We'll create an optimizer using `torch.optim.Adam()` and a learning rate of `1e-3`.\n",
        "\n",
        "<font color=\"purple\">And because we've got so many classes, we'll also setup a loss function using `torch.nn.CrossEntropyLoss()` with `label_smoothing=0.1`, inline with [`torchvision`'s state-of-the-art training recipe](https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/#label-smoothing).\n",
        "\n",
        "<font color=\"purple\">What's [**label smoothing**](https://paperswithcode.com/method/label-smoothing)? \n",
        "\n",
        "<font color=\"purple\">**Label smoothing** is a regularization technique (regularization is another word to describe the process of [preventing overfitting](https://www.learnpytorch.io/04_pytorch_custom_datasets/#81-how-to-deal-with-overfitting)) that reduces the value a model gives to anyone label and spreads it across the other labels.\n",
        "\n",
        "<font color=\"purple\">In essence, rather than a model getting *too confident* on a single label, label smoothing gives a non-zero value to other labels to help aid in generalization.\n",
        "\n",
        "For example, if a model *without* label smoothing had the following outputs for 5 classes:</font>\n",
        "\n",
        "```\n",
        "[0, 0, 0.99, 0.01, 0]\n",
        "```\n",
        "\n",
        "<font color=\"purple\">A model *with* label smoothing may have the following outputs:\n",
        "\n",
        "```\n",
        "[0.01, 0.01, 0.96, 0.01, 0.01]\n",
        "```\n",
        "\n",
        "The model is still confident on its prediction of class 3 but giving small values to the other labels forces the model to at least consider other options.</font>\n",
        "\n",
        "<font color=\"purple\">Finally, to keep things quick, we'll train our model for five epochs using the `engine.train()` function we created in [05. PyTorch Going Modular section 4](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them) with the goal of beating the original Food101 paper's result of 56.4% accuracy on the test set.\n",
        "\n",
        "Let's train our biggest model yet!</font>\n",
        "\n",
        "> <font color=\"purple\">**Note:** Running the cell below will take ~15-20 minutes to run on Google Colab. This is because it's training the biggest model with the largest amount of data we've used so far (15,150 training images, 5050 testing images). And it's a reason we decided to split 20% of the full Food101 dataset off before (so training didn't take over an hour). "
      ],
      "metadata": {
        "id": "NSj0TEwGpR_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),\n",
        "                             lr=1e-3)\n",
        "\n",
        "# Setup loss\n",
        "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # (throw in a little label smoothing because so many classes)\n",
        "\n",
        "# Want to beat the original Food101 paper's result of 56.4% accuracy on the test dataset with 20% of the data\n",
        "set_seeds()\n",
        "effnetb2_food101_results = engine.train(model=effnetb2_food101,\n",
        "                      train_dataloader=train_dataloader_food101_20_percent,\n",
        "                      test_dataloader=test_dataloader_food101_20_percent,\n",
        "                      optimizer=optimizer,\n",
        "                      loss_fn=loss_fn,\n",
        "                      epochs=5,\n",
        "                      device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "5a41e92879364ffdbe4a4b84b2a815b4",
            "f7267b534abd43ab84c70c74b644a06b",
            "f11aeabb239a4e5aa8c1847d7437b60f",
            "572281cc888f48c7afd2461426b0321e",
            "245ab0553fb445ec8d7961a218c97a39",
            "2b5471c1519b461aa31736b9d24d43d2",
            "c98453092a49405780dec8a65b79e6fb",
            "bb8a6f3b1d7443b28be1571e43755493",
            "cc9109d69c8544e18a9dac3fa8f4da22",
            "0aabb802874547beaa385aa0ee024fbd",
            "bf88ce31ca9041e9910921e1c6fe84cf"
          ]
        },
        "id": "c7a3wwHZIbii",
        "outputId": "7d9cbc37-3eb6-4204-cd1a-7f15dfebb007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a41e92879364ffdbe4a4b84b2a815b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 3.6411 | train_acc: 0.2814 | test_loss: 2.7810 | test_acc: 0.4947\n",
            "Epoch: 2 | train_loss: 2.8608 | train_acc: 0.4421 | test_loss: 2.4720 | test_acc: 0.5355\n",
            "Epoch: 3 | train_loss: 2.6546 | train_acc: 0.4862 | test_loss: 2.3634 | test_acc: 0.5612\n",
            "Epoch: 4 | train_loss: 2.5434 | train_acc: 0.5125 | test_loss: 2.3020 | test_acc: 0.5765\n",
            "Epoch: 5 | train_loss: 2.4951 | train_acc: 0.5236 | test_loss: 2.2794 | test_acc: 0.5796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've just done something in ~18 minutes that wasn't possible 10 years ago..."
      ],
      "metadata": {
        "id": "m58oNCziUI55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Woohoo!!!!\n",
        "\n",
        "<font color=\"purple\">Looks like we beat the original Food101 paper's results of 56.4% accuracy with only 20% of the training data (though we only evaluated on 20% of the testing data too, to fully replicate the results, we could evaluate on 100% of the testing data). \n",
        "\n",
        "<font color=\"purple\">That's the power of transfer learning!"
      ],
      "metadata": {
        "id": "YvzoSCyKqRCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.6 Inspecting loss curves of FoodVision Big model"
      ],
      "metadata": {
        "id": "vvLG-Ng8NHaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Let's make our FoodVision Big loss curves visual.\n",
        "\n",
        "<font color=\"purple\">We can do so with the `plot_loss_curves()` function from `helper_functions.py`."
      ],
      "metadata": {
        "id": "FDdy3XsyqeMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "# (Check out the loss curves for FoodVision Big)\n",
        "plot_loss_curves(effnetb2_food101_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "rjR0ojWvNZUf",
        "outputId": "4a35982b-8f0f-4612-8562-9a9678988906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Nice!!!\n",
        "\n",
        "<font color=\"purple\">It looks like our regularization techniques (data augmentation and label smoothing) helped prevent our model from overfitting (the training loss is still higher than the test loss) this indicates our model has a bit more capacity to learn and could improve with further training. "
      ],
      "metadata": {
        "id": "JjfrTxGCqyfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.7 Save and load FoodVision Big model"
      ],
      "metadata": {
        "id": "xl7SV-rvNfkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Now we've trained our biggest model yet, let's save it so we can load it back in later."
      ],
      "metadata": {
        "id": "CAiJAJHsq-Ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import utils\n",
        "\n",
        "# Create a model path\n",
        "effnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"\n",
        "\n",
        "# Save FoodVision Big model\n",
        "utils.save_model(model=effnetb2_food101,\n",
        "         target_dir=\"models/\",\n",
        "         model_name=effnetb2_food101_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMsSEOoMNicH",
        "outputId": "7696b526-589b-4ae9-99b1-5dc99a1a3807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving model to: models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Model saved!\n",
        "\n",
        "<font color=\"purple\">Before we move on, let's make sure we can load it back in.\n",
        "\n",
        "<font color=\"purple\">We'll do so by creating a model instance first with `create_effnetb2_model(num_classes=101)` (101 classes for all Food101 classes).\n",
        "\n",
        "<font color=\"purple\">And then loading the saved `state_dict()` with [`torch.nn.Module.load_state_dict()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict) and [`torch.load()`](https://pytorch.org/docs/stable/generated/torch.load.html). "
      ],
      "metadata": {
        "id": "gcYqOXtUrOtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Food101 compatible EffNetB2 instance\n",
        "\"\"\"調用create_effnetb2_model建立 loaded_effnetb2_food101模型 與 effnetb2_transforms變換\"\"\"\n",
        "loaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n",
        "\n",
        "# Load the saved model's state_dict()\n",
        "loaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfw3uDURN2b1",
        "outputId": "50a0b47e-43a3-468a-9e3d-0ec34f6e680f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.8 Checking FoodVision Big model size"
      ],
      "metadata": {
        "id": "YzliGRsUOPVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Our FoodVision Big model is capable of classifying 101 classes versus FoodVision Mini's 3 classes, a 33.6x increase!\n",
        "\n",
        "<font color=\"purple\">How does this effect the model size?\n",
        "\n",
        "<font color=\"purple\">Let's find out."
      ],
      "metadata": {
        "id": "QwnbBZ8urcNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \n",
        "print(f\"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyIZ_3DSOT6R",
        "outputId": "99988ce0-d6ab-4c21-8b25-82b004ff8ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained EffNetB2 feature extractor Food101 model size: 30 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Hmm, it looks like the model size stayed largely the same (30 MB for FoodVision Big and 29 MB for FoodVision Mini) despite the large increase in the number of classes.\n",
        "\n",
        "<font color=\"purple\">This is because all the extra parameters for FoodVision Big are *only* in the last layer (the classifier head). \n",
        "\n",
        "<font color=\"purple\">All of the base layers are the same between FoodVision Big and FoodVision Mini.\n",
        "\n",
        "<font color=\"purple\">Going back up and comparing the model summaries will give more details.\n",
        "\n",
        "| **Model** | **Output shape (num classes)** | **Trainable parameters** | **Total parameters** | **Model size (MB)** |\n",
        "| ----- | ----- | ----- | ----- | ----- |\n",
        "| FoodVision Mini (EffNetB2 feature extractor) | 3 | 4,227 | 7,705,221 |  29 |\n",
        "| FoodVision Big (EffNetB2 feature extractor) | 101 | 142,309 | 7,843,303 | 30 |"
      ],
      "metadata": {
        "id": "ifCUPLyVrqMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Turning our FoodVision Big model into a deployable app \n",
        "\n",
        "Why deploy a model?\n",
        "\n",
        "Deploying a model allows you to see how your model goes in the real-world (the ultimate test set).\n",
        "\n",
        "Let's create an outline for our FoodVision Big app: \n",
        "\n",
        "```\n",
        "demos/\n",
        "  foodvision_big/\n",
        "    09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n",
        "    app.py\n",
        "    class_names.txt\n",
        "    examples/\n",
        "      example_1.jpg\n",
        "    model.py\n",
        "    requirements.txt\n",
        "```"
      ],
      "metadata": {
        "id": "b2gXaj5DU7H5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've got a trained and saved <font color=\"red\">**EffNetB2** model</font> on 20% of the Food101 dataset.\n",
        "\n",
        "<font color=\"purple\">And instead of letting our model live in a folder all its life, let's deploy it!\n",
        "\n",
        "<font color=\"purple\">We'll deploy our FoodVision Big model in the same way we deployed our FoodVision Mini model, as a Gradio demo on <font color=\"coral\">Hugging Face Spaces</font>.\n",
        "\n",
        "<font color=\"purple\">To begin, let's create a `demos/foodvision_big/` directory to store our FoodVision Big demo files as well as a `demos/foodvision_big/examples` directory to hold an example image to test the demo with.\n",
        "\n",
        "<font color=\"purple\">When we're finished we'll have the following file structure:\n",
        "\n",
        "```\n",
        "demos/\n",
        "  foodvision_big/\n",
        "    09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n",
        "    app.py\n",
        "    class_names.txt\n",
        "    examples/\n",
        "      example_1.jpg\n",
        "    model.py\n",
        "    requirements.txt\n",
        "```\n",
        "\n",
        "<font color=\"purple\">Where(共六個檔案):\n",
        "* `09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth` is our trained PyTorch model file.\n",
        "* `app.py` contains our FoodVision Big Gradio app.\n",
        "* `class_names.txt` contains all of the class names for FoodVision Big.\n",
        "* `examples/` contains example images to use with our Gradio app.\n",
        "* `model.py` contains the model defintion as well as any transforms assosciated with the model.\n",
        "* `requirements.txt` contains the dependencies to run our app such as `torch`, `torchvision` and `gradio`.</font>"
      ],
      "metadata": {
        "id": "0l9perHkr8sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create FoodVision Big demo path\n",
        "foodvision_big_demo_path = Path(\"demos/foodvision_big/\")\n",
        "\n",
        "# Make FoodVision Big demo directory\n",
        "foodvision_big_demo_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Make FoodVision Big demo examples directory\n",
        "(foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "zlIU3bK9WTTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls demos/foodvision_big/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itszfiShXDGV",
        "outputId": "259ed63d-ed6c-4052-e87f-e83a91917812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.1 Downloading an example image and moving it to the `examples` directory"
      ],
      "metadata": {
        "id": "EVYwRk_vXFIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">For our example image, we're going to use the faithful [`pizza-dad` image](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/04-pizza-dad.jpeg) (a photo of my dad eating pizza).\n",
        "\n",
        "<font color=\"purple\">So let's download it from the course GitHub via the `!wget` command and then we can move it to `demos/foodvision_big/examples` with the `!mv` command (short for \"move\").\n",
        "\n",
        "<font color=\"purple\">While we're here we'll move our trained Food101 EffNetB2 model from `models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth` to `demos/foodvision_big` as well."
      ],
      "metadata": {
        "id": "QsaQEoeCwQ3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and move example image (and move an example image)\n",
        "!wget https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/04-pizza-dad.jpeg \n",
        "!mv 04-pizza-dad.jpeg demos/foodvision_big/examples/04-pizza-dad.jpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwuV22JaXvZX",
        "outputId": "072534e6-2666-4696-c95f-bbacda84492a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-30 01:31:42--  https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/04-pizza-dad.jpeg\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg [following]\n",
            "--2022-08-30 01:31:43--  https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2874848 (2.7M) [image/jpeg]\n",
            "Saving to: ‘04-pizza-dad.jpeg’\n",
            "\n",
            "04-pizza-dad.jpeg   100%[===================>]   2.74M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-08-30 01:31:43 (64.7 MB/s) - ‘04-pizza-dad.jpeg’ saved [2874848/2874848]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Move trained model to FoodVision Big demo folder (will error if model is already moved))\n",
        "!mv models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth demos/foodvision_big/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3223kIODX09M",
        "outputId": "5ab451d1-2eed-474c-b090-2c8525fda9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.2 Saving Food101 class names to file (`class_names.txt`)\n",
        "\n",
        "Let's save all of the Food101 class names to a .txt file so we can import them and use them in our app."
      ],
      "metadata": {
        "id": "R8svkoEWYNxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Because there are so many classes in the Food101 dataset, instead of storing them as a list in our `app.py` file, let's saved them to a `.txt` file and read them in when necessary instead.\n",
        "\n",
        "<font color=\"purple\">We'll just remind ourselves what they look like first by checking out `food101_class_names`."
      ],
      "metadata": {
        "id": "9b5zi6XRw-um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the first 10 Food101 class names\n",
        "food101_class_names[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiIzS9SuYX4b",
        "outputId": "73e8aa2d-fdde-4d8d-a0c7-5d1b2166ed11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apple_pie',\n",
              " 'baby_back_ribs',\n",
              " 'baklava',\n",
              " 'beef_carpaccio',\n",
              " 'beef_tartare',\n",
              " 'beet_salad',\n",
              " 'beignets',\n",
              " 'bibimbap',\n",
              " 'bread_pudding',\n",
              " 'breakfast_burrito']"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Wonderful, now we can write these to a text file by first creating a path to `demos/foodvision_big/class_names.txt` and then opening a file with Python's `open()` and then writing to it leaving a new line for each class.\n",
        "\n",
        "<font color=\"purple\">Ideally, we want our class names to be saved like:\n",
        "\n",
        "```\n",
        "apple_pie\n",
        "baby_back_ribs\n",
        "baklava\n",
        "beef_carpaccio\n",
        "beef_tartare\n",
        "...\n",
        "```"
      ],
      "metadata": {
        "id": "4W0Qtqas5XE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create path to Food101 class names\n",
        "foodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n",
        "foodvision_big_class_names_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfsNWfGyZCv4",
        "outputId": "0de3b408-6d5a-416f-8857-fa89013c3100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('demos/foodvision_big/class_names.txt')"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Food101 class names to text file (class names list to file)\n",
        "with open(foodvision_big_class_names_path, \"w\") as f:\n",
        "  print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")\n",
        "  f.write(\"\\n\".join(food101_class_names)) # new line per class name (# leave a new line between each class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-QAU7HyZOr1",
        "outputId": "792ff87c-d115-47b9-a773-07f998ef8820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Saving Food101 class names to demos/foodvision_big/class_names.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Excellent, now let's make sure we can read them in.\n",
        "\n",
        "<font color=\"purple\">To do so we'll use Python's [`open()`](https://www.w3schools.com/python/ref_func_open.asp) in read mode (`\"r\"`) and then use the [`readlines()`](https://www.w3schools.com/python/ref_file_readlines.asp) method to read each line of our `class_names.txt` file.\n",
        "\n",
        "<font color=\"purple\">And we can save the class names to a list by stripping the newline value of each of them with a list comprehension and [`strip()`](https://www.w3schools.com/python/ref_string_strip.asp). "
      ],
      "metadata": {
        "id": "uM8vOUkk6VVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open Food101 class names file and read each line into a list\n",
        "with open(foodvision_big_class_names_path, \"r\") as f:\n",
        "  food101_class_names_loaded = [food.strip() for food in f.readlines()]\n",
        "\n",
        "# (View the first 5 class names loaded back in)\n",
        "food101_class_names_loaded[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFDijaFJZi_l",
        "outputId": "b168ced1-cba0-48cf-e25b-25f77bffa442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare']"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.3 Turning our FoodVision Big model into a Python script (`model.py`)"
      ],
      "metadata": {
        "id": "ZCXNzl6taDu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Just like the FoodVision Mini demo, let's create a script that's capable of instantiating an EffNetB2 feature extractor model along with its necessary transforms."
      ],
      "metadata": {
        "id": "xCD3gHCS6k1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_big/model.py\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "def create_effnetb2_model(num_classes:int=3, # default output classes = 3 (pizza, steak, sushi)\n",
        "              seed:int=42):\n",
        "  \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n",
        "\n",
        "    Args:\n",
        "      num_classes (int, optional): number of classes in the classifier head. \n",
        "          Defaults to 3.\n",
        "      seed (int, optional): random seed value. Defaults to 42.\n",
        "\n",
        "    Returns:\n",
        "      model (torch.nn.Module): EffNetB2 feature extractor model. \n",
        "      transforms (torchvision.transforms): EffNetB2 image transforms.\n",
        "  \"\"\"\n",
        "\n",
        "  # 1, 2, 3 Create EffNetB2 pretrained weights, transforms and model\n",
        "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "  # 4. Freeze all layers in the base model\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # 5. Change classifier head with random seed for reproducibility\n",
        "  torch.manual_seed(seed)\n",
        "  model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.3, inplace=True),\n",
        "    nn.Linear(in_features=1408, out_features=num_classes)\n",
        "  )\n",
        "\n",
        "  return model, transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPBlo8Poan44",
        "outputId": "09654c8f-fbba-460f-c2e6-393757be6eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_big/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.4 Turning our FoodVision Big Gradio app into a Python script (`app.py`)\n",
        "\n",
        "The `app.py` file will have four major parts:\n",
        "1. Imports and class names setup - for class names, we'll need to import from `class_names.txt` rather than with a Python list \n",
        "2. Model and transforms preparation - we'll need to make sure our model is suitable for FoodVision Big\n",
        "3. Predict function (`predict()`) - this can stay the same as the original `predict()`\n",
        "4. Gradio app - our Gradio interface + launch command - this will change slightly from FoodVision Mini to reflect the FoodVision Big updates "
      ],
      "metadata": {
        "id": "m9FsAU6paxUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've got a FoodVision Big `model.py` script, now let's create a FoodVision Big `app.py` script.\n",
        "\n",
        "<font color=\"purple\">This will again mostly be the same as the FoodVision Mini `app.py` script except we'll change:\n",
        "\n",
        "1. <font color=\"purple\">**Imports and class names setup** - The `class_names` variable will be a list for all of the Food101 classes rather than pizza, steak, sushi. We can access these via `demos/foodvision_big/class_names.txt`.\n",
        "2. **Model and transforms preparation** - The `model` will have `num_classes=101` rather than `num_classes=3`. We'll also be sure to load the weights from `\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"` (our FoodVision Big model path).\n",
        "3. **Predict function** - This will stay the same as FoodVision Mini's `app.py`.\n",
        "4. **Gradio app** - The Gradio interace will have different `title`, `description` and `article` parameters to reflect the details of FoodVision Big.\n",
        "\n",
        "<font color=\"purple\">We'll also make sure to save it to `demos/foodvision_big/app.py` using the `%%writefile` magic command."
      ],
      "metadata": {
        "id": "Al2Sc5W67y5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_big/app.py\n",
        "### 1. Imports and class names setup ###\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model import create_effnetb2_model\n",
        "from timeit import default_timer as timer\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "# Setup class names\n",
        "with open(\"class_names.txt\", \"r\") as f: # (reading them in from class_names.txt)\n",
        "  class_names = [food_name.strip() for food_name in f.readlines()]\n",
        "\n",
        "### 2. Model and transforms preparation ### \n",
        "# Create model and transforms\n",
        "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
        "    num_classes=101 # (could also use len(class_names))\n",
        ")\n",
        "\n",
        "# Load saved weights\n",
        "effnetb2.load_state_dict(\n",
        "    torch.load(f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",\n",
        "    map_location=torch.device(\"cpu\")) # load to CPU\n",
        ")\n",
        "\n",
        "### 3. Predict function ###\n",
        "\"\"\"Create predict function\"\"\"\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "  \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n",
        "  \"\"\"\n",
        "  # Start a timer\n",
        "  start_time = timer()\n",
        "\n",
        "  # Transform the input image for use with EffNetB2\n",
        "  \"\"\"Transform the target image and add a batch dimension\"\"\"\n",
        "  img = effnetb2_transforms(img).unsqueeze(0) # unsqueeze = add batch dimension on 0th index\n",
        "\n",
        "  # Put model into eval mode, make prediction\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "    # Pass transformed image through the model and turn the prediction logits into probaiblities\n",
        "    \"\"\"Pass the transformed image through the model and turn the prediction logits into prediction probabilities\"\"\"\n",
        "    pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "  # Create a prediction label and prediction probability dictionary \n",
        "  \"\"\"for each prediction class (this is the required format for Gradio's output parameter)\"\"\"\n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "  # Calculate pred time (prediction time)\n",
        "  end_time = timer()\n",
        "  pred_time = round(end_time - start_time, 4)\n",
        "\n",
        "  # Return pred dict and pred time\n",
        "  return pred_labels_and_probs, pred_time\n",
        "\n",
        "### 4. Gradio app ###\n",
        "\n",
        "# Create title, description and article (strings)\n",
        "title = \"FoodVision BIG 🍔👁💪\"\n",
        "description = \"An [EfficientNetB2 feature extractor](https://pytorch.org/vision/stable/models/generated/torchvision.models.efficientnet_b2.html#torchvision.models.efficientnet_b2) computer vision model to classify images [101 classes of food from the Food101 dataset](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).\"\n",
        "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/#11-turning-our-foodvision-big-model-into-a-deployable-app).\"\n",
        "\n",
        "# Create example list(from \"examples/\" directory)\n",
        "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
        "\n",
        "# Create the Gradio demo\n",
        "demo = gr.Interface(fn=predict, # maps inputs to outputs\n",
        "            inputs=gr.Image(type=\"pil\"),\n",
        "            outputs=[gr.Label(num_top_classes=5, label=\"Predictions\"),\n",
        "                      gr.Number(label=\"Prediction time (s)\")],\n",
        "            examples=example_list,\n",
        "            title=title,\n",
        "            description=description,\n",
        "            article=article)\n",
        "\n",
        "# Launch the demo!\n",
        "demo.launch() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV5AMfeSa8Kh",
        "outputId": "4038910f-d8b0-428b-a573-1f145a622303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_big/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.5 Creating a requirements file for FoodVision Big (`requirements.txt`) "
      ],
      "metadata": {
        "id": "-X7rUdifeUT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">Now all we need is a `requirements.txt` file to tell our Hugging Face Space what dependencies our FoodVision Big app requires."
      ],
      "metadata": {
        "id": "M2v2hcno-LKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile demos/foodvision_big/requirements.txt\n",
        "torch==1.12.0\n",
        "torchvision==0.13.0\n",
        "gradio==3.1.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CVhzrUZdlA_",
        "outputId": "9812c007-69fc-4d90-a410-6c063cdcaed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demos/foodvision_big/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.6 Downloading our FoodVision Big app files"
      ],
      "metadata": {
        "id": "hvaDFJZwePry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">We've got all the files we need to deploy our FoodVision Big app on Hugging Face, let's now zip them together and download them. \n",
        "\n",
        "<font color=\"purple\">We'll use the same process we used for the FoodVision Mini app above in [section 9.1: *Downloading our Foodvision Mini app files*](https://www.learnpytorch.io/09_pytorch_model_deployment/#91-downloading-our-foodvision-mini-app-files)."
      ],
      "metadata": {
        "id": "y3AGufRE-WOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change into the foodvision_big directory and then zip it from the inside\n",
        "\"\"\"Zip foodvision_big folder but exclude certain files\"\"\"\n",
        "!cd demos/foodvision_big && zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejIkVTl5ebVB",
        "outputId": "188d7a9b-0916-4887-e3db-815ada252a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: 09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth (deflated 8%)\n",
            "  adding: app.py (deflated 54%)\n",
            "  adding: class_names.txt (deflated 48%)\n",
            "  adding: examples/ (stored 0%)\n",
            "  adding: examples/04-pizza-dad.jpeg (deflated 0%)\n",
            "  adding: model.py (deflated 46%)\n",
            "  adding: requirements.txt (deflated 4%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download\n",
        "\"\"\"Download the zipped FoodVision Big app (if running in Google Colab)\"\"\"\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download(\"demos/foodvision_big.zip\")\n",
        "except:\n",
        "  print(f\"Not running in Google Colab, can't use google.colab.files.download(), please download foodvision_big.zip manually.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7ffyQOhJemOL",
        "outputId": "4ea3bb6a-b577-48d0-a96a-2c2ba1655d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_948085f5-4cd1-4408-ba53-6bbd224ce6ff\", \"foodvision_big.zip\", 32183738)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.7 Deploying our FoodVision Big model app to Hugging Faces Spaces\n",
        "\n",
        "Let's bring FoodVision Big to life by deploying it to the world!!!\n",
        "\n",
        "See steps here: https://www.learnpytorch.io/09_pytorch_model_deployment/#117-deploying-our-foodvision-big-app-to-huggingface-spaces \n",
        "\n",
        "See our deployed app here: https://huggingface.co/spaces/mrdbourke/foodvision_big_video "
      ],
      "metadata": {
        "id": "KkOrng7Sewfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B, E, A, Utiful! \n",
        "\n",
        "Time to bring our biggest model of the whole course to life!\n",
        "\n",
        "Let's deploy our FoodVision Big Gradio demo to Hugging Face Spaces so we can test it interactively and let others experience the magic of our machine learning efforts!\n",
        "\n",
        "> **Note:** There are [several ways to upload files to Hugging Face Spaces](https://huggingface.co/docs/hub/repositories-getting-started#getting-started-with-repositories). The following steps treat Hugging Face as a git repository to track files. However, you can also upload directly to Hugging Face Spaces via the [web interface](https://huggingface.co/docs/hub/repositories-getting-started#adding-files-to-a-repository-web-ui) or by the [`huggingface_hub` library](https://huggingface.co/docs/huggingface_hub/index). \n",
        "\n",
        "The good news is, we've already done the steps to do so with FoodVision Mini, so now all we have to do is customize them to suit FoodVision Big:\n",
        "\n",
        "1. [Sign up](https://huggingface.co/join) for a Hugging Face account. \n",
        "2. Start a new Hugging Face Space by going to your profile and then [clicking \"New Space\"](https://huggingface.co/new-space).\n",
        "    * **Note:** A Space in Hugging Face is also known as a \"code repository\" (a place to store your code/files) or \"repo\" for short.\n",
        "3. Give the Space a name, for example, mine is called `mrdbourke/foodvision_big`, you can see it here: https://huggingface.co/spaces/mrdbourke/foodvision_big\n",
        "4. Select a license (I used [MIT](https://opensource.org/licenses/MIT)).\n",
        "5. Select Gradio as the Space SDK (software development kit). \n",
        "   * **Note:** You can use other options such as Streamlit but since our app is built with Gradio, we'll stick with that.\n",
        "6. Choose whether your Space is it's public or private (I selected public since I'd like my Space to be available to others).\n",
        "7. Click \"Create Space\".\n",
        "8. Clone the repo locally by running: `git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]` in terminal or command prompt.\n",
        "    * **Note:** You can also add files via uploading them under the \"Files and versions\" tab.\n",
        "9. Copy/move the contents of the downloaded `foodvision_big` folder to the cloned repo folder.\n",
        "10. To upload and track larger files (e.g. files over 10MB or in our case, our PyTorch model file) you'll need to [install Git LFS](https://git-lfs.github.com/) (which stands for \"git large file storage\").\n",
        "11. After you've installed Git LFS, you can activate it by running `git lfs install`.\n",
        "12. In the `foodvision_big` directory, track the files over 10MB with Git LFS with `git lfs track \"*.file_extension\"`.\n",
        "    * Track EffNetB2 PyTorch model file with `git lfs track \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"`.\n",
        "    * **Note:** If you get any errors uploading images, you may have to track them with `git lfs` too, for example `git lfs track \"examples/04-pizza-dad.jpg\"`\n",
        "13. Track `.gitattributes` (automatically created when cloning from HuggingFace, this file will help ensure our larger files are tracked with Git LFS). You can see an example `.gitattributes` file on the [FoodVision Big Hugging Face Space](https://huggingface.co/spaces/mrdbourke/foodvision_big/blob/main/.gitattributes).\n",
        "    * `git add .gitattributes`\n",
        "14. Add the rest of the `foodvision_big` app files and commit them with: \n",
        "    * `git add *`\n",
        "    * `git commit -m \"first commit\"`\n",
        "15. Push (upload) the files to Hugging Face:\n",
        "    * `git push`\n",
        "16. Wait 3-5 minutes for the build to happen (future builds are faster) and your app to become live!\n",
        "\n",
        "If everything worked correctly, our FoodVision Big Gradio demo should be ready to classify!\n",
        "\n",
        "You can see my version here: https://huggingface.co/spaces/mrdbourke/foodvision_big/\n",
        "\n",
        "Or we can even embed our FoodVision Big Gradio demo right within our notebook as an [iframe](https://gradio.app/sharing_your_app/#embedding-with-iframes) with [`IPython.display.IFrame`](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.IFrame) and a link to our space in the format `https://hf.space/embed/[YOUR_USERNAME]/[YOUR_SPACE_NAME]/+`."
      ],
      "metadata": {
        "id": "v05obwa0-ye0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (IPython is a library to help work with Python iteractively )\n",
        "from IPython.display import IFrame\n",
        "\n",
        "# (Embed FoodVision Big Gradio demo as an iFrame)\n",
        "IFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_big/+\", width=900, height=750)"
      ],
      "metadata": {
        "id": "fFAbOiGE-7sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">How cool is that!?!\n",
        "\n",
        "<font color=\"purple\">We've come a long way from building PyTorch models to predict a straight line... now we're building computer vision models accessible to people all around the world!"
      ],
      "metadata": {
        "id": "nXkurFyl_CDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main takeaways, exercises and extra-curriculum\n",
        "\n",
        "* Main takeaways: https://www.learnpytorch.io/09_pytorch_model_deployment/#main-takeaways \n",
        "* Exercises: https://www.learnpytorch.io/09_pytorch_model_deployment/#exercises\n",
        "* Extra-curriculum: https://www.learnpytorch.io/09_pytorch_model_deployment/#extra-curriculum "
      ],
      "metadata": {
        "id": "wk_u0VmsfyuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"purple\">**Deployment is as important as training.** Once you’ve got a good working model, your first question should be: how can I deploy this and make it accessible to others? Deployment allows you to test your model in the real world rather than on private training and test sets.\n",
        "* <font color=\"purple\">**Three questions for machine learning model deployment:**\n",
        "    1. What’s the most ideal use case for the model (how well and how fast does it perform)? <font color=\"red\">怎樣的模型：多快？多精確?</font>\n",
        "    2. Where’s the model going to go (is it on-device or on the cloud)? <font color=\"red\">模型放置位置：本機上？雲端上？</font>\n",
        "    3. How’s the model going to function (are predictions online or offline)? <font color=\"red\">模型如何運作：online? offline?</font>\n",
        "* **Deployment options are a plenty.** But best to start simple. One of the best current ways (I say current because these things are always changing) is to use Gradio to create a demo and host it on Hugging Face Spaces. Start simple and scale up when needed.\n",
        "* **Never stop experimenting.** Your machine learning model needs will likely change overtime so deploying a single model is not the last step. You might find the dataset changes, so you’ll have to update your model. Or new research gets released and there’s a better architecture to use.\n",
        "    * <font color=\"purple\">So deploying one model is an excellent step, but you'll likely want to update it over time. \n",
        "* **Machine learning model deployment is part of the engineering practice of MLOps (machine learning operations).** MLOps is an extension of DevOps (development operations) and involves all the engineering parts around training a model: data collection and storage, data preprocessing, model deployment, model monitoring, versioning and more. It’s a rapidly evolving field but there are some solid resources out there to learn more, many of which are in [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/#resources-for-machine-learning-and-deep-learning-engineering)."
      ],
      "metadata": {
        "id": "hFLt76Ti_NWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises 參見lecture file"
      ],
      "metadata": {
        "id": "TaCA347A_i1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra-curriculum"
      ],
      "metadata": {
        "id": "XDPiukmf_rHq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a4AK4P2Glajh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}